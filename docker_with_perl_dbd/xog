
class OutOfBandError(RemoteError):
    """Exception raised when a remote repo reports failure"""

    def __init__(self, message=None, hint=None):
        # type: (Optional[bytes], Optional[bytes]) -> None
        from .i18n import _

        if message:
            # Abort.format() adds a trailing newline
            message = _(b"remote error:\n%s") % message.rstrip(b'\n')
        else:
            message = _(b"remote error")
        super(OutOfBandError, self).__init__(message, hint=hint)


class ParseError(Abort):
    """Raised when parsing config files and {rev,file}sets (msg[, pos])"""

    detailed_exit_code = 10

    def __init__(self, message, location=None, hint=None):
        # type: (bytes, Optional[Union[bytes, int]], Optional[bytes]) -> None
        super(ParseError, self).__init__(message, hint=hint)
        self.location = location

    def format(self):
        # type: () -> bytes
        from .i18n import _

        if self.location is not None:
            message = _(b"hg: parse error at %s: %s\n") % (
                pycompat.bytestr(self.location),
                self.message,
            )
        else:
            message = _(b"hg: parse error: %s\n") % self.message
        if self.hint:
            message += _(b"(%s)\n") % self.hint
        return message


class PatchError(Exception):
    __bytes__ = _tobytes


class PatchParseError(PatchError):
    __bytes__ = _tobytes


class PatchApplicationError(PatchError):
    __bytes__ = _tobytes


def getsimilar(symbols, value):
    # type: (Iterable[bytes], bytes) -> List[bytes]
    sim = lambda x: difflib.SequenceMatcher(None, value, x).ratio()
    # The cutoff for similarity here is pretty arbitrary. It should
    # probably be investigated and tweaked.
    return [s for s in symbols if sim(s) > 0.6]


def similarity_hint(similar):
    # type: (List[bytes]) -> Optional[bytes]
    from .i18n import _

    if len(similar) == 1:
        return _(b"did you mean %s?") % similar[0]
    elif similar:
        ss = b", ".join(sorted(similar))
        return _(b"did you mean one of %s?") % ss
    else:
        return None


class UnknownIdentifier(ParseError):
    """Exception raised when a {rev,file}set references an unknown identifier"""

    def __init__(self, function, symbols):
        # type: (bytes, Iterable[bytes]) -> None
        from .i18n import _

        similar = getsimilar(symbols, function)
        hint = similarity_hint(similar)

        ParseError.__init__(
            self, _(b"unknown identifier: %s") % function, hint=hint
        )


class RepoError(Hint, Exception):
    __bytes__ = _tobytes


class RepoLookupError(RepoError):
    pass


class FilteredRepoLookupError(RepoLookupError):
    pass


class CapabilityError(RepoError):
    pass


class RequirementError(RepoError):
    """Exception raised if .hg/requires has an unknown entry."""


class StdioError(IOError):
    """Raised if I/O to stdout or stderr fails"""

    def __init__(self, err):
        # type: (IOError) -> None
        IOError.__init__(self, err.errno, err.strerror)

    # no __bytes__() because error message is derived from the standard IOError


class UnsupportedMergeRecords(Abort):
    def __init__(self, recordtypes):
        # type: (Iterable[bytes]) -> None
        from .i18n import _

        self.recordtypes = sorted(recordtypes)
        s = b' '.join(self.recordtypes)
        Abort.__init__(
            self,
            _(b'unsupported merge state records: %s') % s,
            hint=_(
                b'see https://mercurial-scm.org/wiki/MergeStateRecords for '
                b'more information'
            ),
        )


class UnknownVersion(Abort):
    """generic exception for aborting from an encounter with an unknown version"""

    def __init__(self, msg, hint=None, version=None):
        # type: (bytes, Optional[bytes], Optional[bytes]) -> None
        self.version = version
        super(UnknownVersion, self).__init__(msg, hint=hint)


class LockError(IOError):
    def __init__(self, errno, strerror, filename, desc):
        # TODO: figure out if this should be bytes or str
        # _type: (int, str, str, bytes) -> None
        IOError.__init__(self, errno, strerror, filename)
        self.desc = desc

    # no __bytes__() because error message is derived from the standard IOError


class LockHeld(LockError):
    def __init__(self, errno, filename, desc, locker):
        LockError.__init__(self, errno, b'Lock held', filename, desc)
        self.locker = locker


class LockUnavailable(LockError):
    pass


# LockError is for errors while acquiring the lock -- this is unrelated
class LockInheritanceContractViolation(RuntimeError):
    __bytes__ = _tobytes


class ResponseError(Exception):
    """Raised to print an error with part of output and exit."""

    __bytes__ = _tobytes


# derived from KeyboardInterrupt to simplify some breakout code
class SignalInterrupt(KeyboardInterrupt):
    """Exception raised on SIGTERM and SIGHUP."""


class SignatureError(Exception):
    __bytes__ = _tobytes


class PushRaced(RuntimeError):
    """An exception raised during unbundling that indicate a push race"""

    __bytes__ = _tobytes


class ProgrammingError(Hint, RuntimeError):
    """Raised if a mercurial (core or extension) developer made a mistake"""

    def __init__(self, msg, *args, **kwargs):
        # type: (AnyStr, Any, Any) -> None
        # On Python 3, turn the message back into a string since this is
        # an internal-only error that won't be printed except in a
        # stack traces.
        msg = pycompat.sysstr(msg)
        super(ProgrammingError, self).__init__(msg, *args, **kwargs)

    __bytes__ = _tobytes


class WdirUnsupported(Exception):
    """An exception which is raised when 'wdir()' is not supported"""

    __bytes__ = _tobytes


# bundle2 related errors
class BundleValueError(ValueError):
    """error raised when bundle2 cannot be processed"""

    __bytes__ = _tobytes


class BundleUnknownFeatureError(BundleValueError):
    def __init__(self, parttype=None, params=(), values=()):
        self.parttype = parttype
        self.params = params
        self.values = values
        if self.parttype is None:
            msg = b'Stream Parameter'
        else:
            msg = parttype
        entries = self.params
        if self.params and self.values:
            assert len(self.params) == len(self.values)
            entries = []
            for idx, par in enumerate(self.params):
                val = self.values[idx]
                if val is None:
                    entries.append(val)
                else:
                    entries.append(b"%s=%r" % (par, pycompat.maybebytestr(val)))
        if entries:
            msg = b'%s - %s' % (msg, b', '.join(entries))
        ValueError.__init__(self, msg)  # TODO: convert to str?


class ReadOnlyPartError(RuntimeError):
    """error raised when code tries to alter a part being generated"""

    __bytes__ = _tobytes


class PushkeyFailed(Abort):
    """error raised when a pushkey part failed to update a value"""

    def __init__(
        self, partid, namespace=None, key=None, new=None, old=None, ret=None
    ):
        self.partid = partid
        self.namespace = namespace
        self.key = key
        self.new = new
        self.old = old
        self.ret = ret
        # no i18n expected to be processed into a better message
        Abort.__init__(
            self, b'failed to update value for "%s/%s"' % (namespace, key)
        )


class CensoredNodeError(StorageError):
    """error raised when content verification fails on a censored node

    Also contains the tombstone data substituted for the uncensored data.
    """

    def __init__(self, filename, node, tombstone):
        # type: (bytes, bytes, bytes) -> None
        from .node import short

        StorageError.__init__(self, b'%s:%s' % (filename, short(node)))
        self.tombstone = tombstone


class CensoredBaseError(StorageError):
    """error raised when a delta is rejected because its base is censored

    A delta based on a censored revision must be formed as single patch
    operation which replaces the entire base with new content. This ensures
    the delta may be applied by clones which have not censored the base.
    """


class InvalidBundleSpecification(Exception):
    """error raised when a bundle specification is invalid.

    This is used for syntax errors as opposed to support errors.
    """

    __bytes__ = _tobytes


class UnsupportedBundleSpecification(Exception):
    """error raised when a bundle specification is not supported."""

    __bytes__ = _tobytes


class CorruptedState(Exception):
    """error raised when a command is not able to read its state from file"""

    __bytes__ = _tobytes


class PeerTransportError(Abort):
    """Transport-level I/O error when communicating with a peer repo."""


class InMemoryMergeConflictsError(Exception):
    """Exception raised when merge conflicts arose during an in-memory merge."""

    __bytes__ = _tobytes


class WireprotoCommandError(Exception):
    """Represents an error during execution of a wire protocol command.

    Should only be thrown by wire protocol version 2 commands.

    The error is a formatter string and an optional iterable of arguments.
    """

    def __init__(self, message, args=None):
        # type: (bytes, Optional[Sequence[bytes]]) -> None
        self.message = message
        self.messageargs = args
                                                                                                                                                                                                                     usr/lib/python3/dist-packages/mercurial/exchange.py                                                 0000644 0000000 0000000 00000276316 14355257011 021107  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # exchange.py - utility to exchange data between repos.
#
# Copyright 2005-2007 Olivia Mackall <olivia@selenic.com>
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.


import collections
import weakref

from .i18n import _
from .node import (
    hex,
    nullrev,
)
from . import (
    bookmarks as bookmod,
    bundle2,
    bundlecaches,
    changegroup,
    discovery,
    error,
    lock as lockmod,
    logexchange,
    narrowspec,
    obsolete,
    obsutil,
    phases,
    pushkey,
    pycompat,
    requirements,
    scmutil,
    streamclone,
    url as urlmod,
    util,
    wireprototypes,
)
from .utils import (
    hashutil,
    stringutil,
    urlutil,
)
from .interfaces import repository

urlerr = util.urlerr
urlreq = util.urlreq

_NARROWACL_SECTION = b'narrowacl'


def readbundle(ui, fh, fname, vfs=None):
    header = changegroup.readexactly(fh, 4)

    alg = None
    if not fname:
        fname = b"stream"
        if not header.startswith(b'HG') and header.startswith(b'\0'):
            fh = changegroup.headerlessfixup(fh, header)
            header = b"HG10"
            alg = b'UN'
    elif vfs:
        fname = vfs.join(fname)

    magic, version = header[0:2], header[2:4]

    if magic != b'HG':
        raise error.Abort(_(b'%s: not a Mercurial bundle') % fname)
    if version == b'10':
        if alg is None:
            alg = changegroup.readexactly(fh, 2)
        return changegroup.cg1unpacker(fh, alg)
    elif version.startswith(b'2'):
        return bundle2.getunbundler(ui, fh, magicstring=magic + version)
    elif version == b'S1':
        return streamclone.streamcloneapplier(fh)
    else:
        raise error.Abort(
            _(b'%s: unknown bundle version %s') % (fname, version)
        )


def _format_params(params):
    parts = []
    for key, value in sorted(params.items()):
        value = urlreq.quote(value)
        parts.append(b"%s=%s" % (key, value))
    return b';'.join(parts)


def getbundlespec(ui, fh):
    """Infer the bundlespec from a bundle file handle.

    The input file handle is seeked and the original seek position is not
    restored.
    """

    def speccompression(alg):
        try:
            return util.compengines.forbundletype(alg).bundletype()[0]
        except KeyError:
            return None

    params = {}

    b = readbundle(ui, fh, None)
    if isinstance(b, changegroup.cg1unpacker):
        alg = b._type
        if alg == b'_truncatedBZ':
            alg = b'BZ'
        comp = speccompression(alg)
        if not comp:
            raise error.Abort(_(b'unknown compression algorithm: %s') % alg)
        return b'%s-v1' % comp
    elif isinstance(b, bundle2.unbundle20):
        if b'Compression' in b.params:
            comp = speccompression(b.params[b'Compression'])
            if not comp:
                raise error.Abort(
                    _(b'unknown compression algorithm: %s') % comp
                )
        else:
            comp = b'none'

        version = None
        for part in b.iterparts():
            if part.type == b'changegroup':
                cgversion = part.params[b'version']
                if cgversion in (b'01', b'02'):
                    version = b'v2'
                elif cgversion in (b'03',):
                    version = b'v2'
                    params[b'cg.version'] = cgversion
                else:
                    raise error.Abort(
                        _(
                            b'changegroup version %s does not have '
                            b'a known bundlespec'
                        )
                        % version,
                        hint=_(b'try upgrading your Mercurial client'),
                    )
            elif part.type == b'stream2' and version is None:
                # A stream2 part requires to be part of a v2 bundle
                requirements = urlreq.unquote(part.params[b'requirements'])
                splitted = requirements.split()
                params = bundle2._formatrequirementsparams(splitted)
                return b'none-v2;stream=v2;%s' % params
            elif part.type == b'obsmarkers':
                params[b'obsolescence'] = b'yes'
                if not part.mandatory:
                    params[b'obsolescence-mandatory'] = b'no'

        if not version:
            raise error.Abort(
                _(b'could not identify changegroup version in bundle')
            )
        spec = b'%s-%s' % (comp, version)
        if params:
            spec += b';'
            spec += _format_params(params)
        return spec

    elif isinstance(b, streamclone.streamcloneapplier):
        requirements = streamclone.readbundle1header(fh)[2]
        formatted = bundle2._formatrequirementsparams(requirements)
        return b'none-packed1;%s' % formatted
    else:
        raise error.Abort(_(b'unknown bundle type: %s') % b)


def _computeoutgoing(repo, heads, common):
    """Computes which revs are outgoing given a set of common
    and a set of heads.

    This is a separate function so extensions can have access to
    the logic.

    Returns a discovery.outgoing object.
    """
    cl = repo.changelog
    if common:
        hasnode = cl.hasnode
        common = [n for n in common if hasnode(n)]
    else:
        common = [repo.nullid]
    if not heads:
        heads = cl.heads()
    return discovery.outgoing(repo, common, heads)


def _checkpublish(pushop):
    repo = pushop.repo
    ui = repo.ui
    behavior = ui.config(b'experimental', b'auto-publish')
    if pushop.publish or behavior not in (b'warn', b'confirm', b'abort'):
        return
    remotephases = listkeys(pushop.remote, b'phases')
    if not remotephases.get(b'publishing', False):
        return

    if pushop.revs is None:
        published = repo.filtered(b'served').revs(b'not public()')
    else:
        published = repo.revs(b'::%ln - public()', pushop.revs)
        # we want to use pushop.revs in the revset even if they themselves are
        # secret, but we don't want to have anything that the server won't see
        # in the result of this expression
        published &= repo.filtered(b'served')
    if published:
        if behavior == b'warn':
            ui.warn(
                _(b'%i changesets about to be published\n') % len(published)
            )
        elif behavior == b'confirm':
            if ui.promptchoice(
                _(b'push and publish %i changesets (yn)?$$ &Yes $$ &No')
                % len(published)
            ):
                raise error.CanceledError(_(b'user quit'))
        elif behavior == b'abort':
            msg = _(b'push would publish %i changesets') % len(published)
            hint = _(
                b"use --publish or adjust 'experimental.auto-publish'"
                b" config"
            )
            raise error.Abort(msg, hint=hint)


def _forcebundle1(op):
    """return true if a pull/push must use bundle1

    This function is used to allow testing of the older bundle version"""
    ui = op.repo.ui
    # The goal is this config is to allow developer to choose the bundle
    # version used during exchanged. This is especially handy during test.
    # Value is a list of bundle version to be picked from, highest version
    # should be used.
    #
    # developer config: devel.legacy.exchange
    exchange = ui.configlist(b'devel', b'legacy.exchange')
    forcebundle1 = b'bundle2' not in exchange and b'bundle1' in exchange
    return forcebundle1 or not op.remote.capable(b'bundle2')


class pushoperation:
    """A object that represent a single push operation

    Its purpose is to carry push related state and very common operations.

    A new pushoperation should be created at the beginning of each push and
    discarded afterward.
    """

    def __init__(
        self,
        repo,
        remote,
        force=False,
        revs=None,
        newbranch=False,
        bookmarks=(),
        publish=False,
        pushvars=None,
    ):
        # repo we push from
        self.repo = repo
        self.ui = repo.ui
        # repo we push to
        self.remote = remote
        # force option provided
        self.force = force
        # revs to be pushed (None is "all")
        self.revs = revs
        # bookmark explicitly pushed
        self.bookmarks = bookmarks
        # allow push of new branch
        self.newbranch = newbranch
        # step already performed
        # (used to check what steps have been already performed through bundle2)
        self.stepsdone = set()
        # Integer version of the changegroup push result
        # - None means nothing to push
        # - 0 means HTTP error
        # - 1 means we pushed and remote head count is unchanged *or*
        #   we have outgoing changesets but refused to push
        # - other values as described by addchangegroup()
        self.cgresult = None
        # Boolean value for the bookmark push
        self.bkresult = None
        # discover.outgoing object (contains common and outgoing data)
        self.outgoing = None
        # all remote topological heads before the push
        self.remoteheads = None
        # Details of the remote branch pre and post push
        #
        # mapping: {'branch': ([remoteheads],
        #                      [newheads],
        #                      [unsyncedheads],
        #                      [discardedheads])}
        # - branch: the branch name
        # - remoteheads: the list of remote heads known locally
        #                None if the branch is new
        # - newheads: the new remote heads (known locally) with outgoing pushed
        # - unsyncedheads: the list of remote heads unknown locally.
        # - discardedheads: the list of remote heads made obsolete by the push
        self.pushbranchmap = None
        # testable as a boolean indicating if any nodes are missing locally.
        self.incoming = None
        # summary of the remote phase situation
        self.remotephases = None
        # phases changes that must be pushed along side the changesets
        self.outdatedphases = None
        # phases changes that must be pushed if changeset push fails
        self.fallbackoutdatedphases = None
        # outgoing obsmarkers
        self.outobsmarkers = set()
        # outgoing bookmarks, list of (bm, oldnode | '', newnode | '')
        self.outbookmarks = []
        # transaction manager
        self.trmanager = None
        # map { pushkey partid -> callback handling failure}
        # used to handle exception from mandatory pushkey part failure
        self.pkfailcb = {}
        # an iterable of pushvars or None
        self.pushvars = pushvars
        # publish pushed changesets
        self.publish = publish

    @util.propertycache
    def futureheads(self):
        """future remote heads if the changeset push succeeds"""
        return self.outgoing.ancestorsof

    @util.propertycache
    def fallbackheads(self):
        """future remote heads if the changeset push fails"""
        if self.revs is None:
            # not target to push, all common are relevant
            return self.outgoing.commonheads
        unfi = self.repo.unfiltered()
        # I want cheads = heads(::ancestorsof and ::commonheads)
        # (ancestorsof is revs with secret changeset filtered out)
        #
        # This can be expressed as:
        #     cheads = ( (ancestorsof and ::commonheads)
        #              + (commonheads and ::ancestorsof))"
        #              )
        #
        # while trying to push we already computed the following:
        #     common = (::commonheads)
        #     missing = ((commonheads::ancestorsof) - commonheads)
        #
        # We can pick:
        # * ancestorsof part of common (::commonheads)
        common = self.outgoing.common
        rev = self.repo.changelog.index.rev
        cheads = [node for node in self.revs if rev(node) in common]
        # and
        # * commonheads parents on missing
        revset = unfi.set(
            b'%ln and parents(roots(%ln))',
            self.outgoing.commonheads,
            self.outgoing.missing,
        )
        cheads.extend(c.node() for c in revset)
        return cheads

    @property
    def commonheads(self):
        """set of all common heads after changeset bundle push"""
        if self.cgresult:
            return self.futureheads
        else:
            return self.fallbackheads


# mapping of message used when pushing bookmark
bookmsgmap = {
    b'update': (
        _(b"updating bookmark %s\n"),
        _(b'updating bookmark %s failed\n'),
    ),
    b'export': (
        _(b"exporting bookmark %s\n"),
        _(b'exporting bookmark %s failed\n'),
    ),
    b'delete': (
        _(b"deleting remote bookmark %s\n"),
        _(b'deleting remote bookmark %s failed\n'),
    ),
}


def push(
    repo,
    remote,
    force=False,
    revs=None,
    newbranch=False,
    bookmarks=(),
    publish=False,
    opargs=None,
):
    """Push outgoing changesets (limited by revs) from a local
    repository to remote. Return an integer:
      - None means nothing to push
      - 0 means HTTP error
      - 1 means we pushed and remote head count is unchanged *or*
        we have outgoing changesets but refused to push
      - other values as described by addchangegroup()
    """
    if opargs is None:
        opargs = {}
    pushop = pushoperation(
        repo,
        remote,
        force,
        revs,
        newbranch,
        bookmarks,
        publish,
        **pycompat.strkwargs(opargs)
    )
    if pushop.remote.local():
        missing = (
            set(pushop.repo.requirements) - pushop.remote.local().supported
        )
        if missing:
            msg = _(
                b"required features are not"
                b" supported in the destination:"
                b" %s"
            ) % (b', '.join(sorted(missing)))
            raise error.Abort(msg)

    if not pushop.remote.canpush():
        raise error.Abort(_(b"destination does not support push"))

    if not pushop.remote.capable(b'unbundle'):
        raise error.Abort(
            _(
                b'cannot push: destination does not support the '
                b'unbundle wire protocol command'
            )
        )
    for category in sorted(bundle2.read_remote_wanted_sidedata(pushop.remote)):
        # Check that a computer is registered for that category for at least
        # one revlog kind.
        for kind, computers in repo._sidedata_computers.items():
            if computers.get(category):
                break
        else:
            raise error.Abort(
                _(
                    b'cannot push: required sidedata category not supported'
                    b" by this client: '%s'"
                )
                % pycompat.bytestr(category)
            )
    # get lock as we might write phase data
    wlock = lock = None
    try:
        # bundle2 push may receive a reply bundle touching bookmarks
        # requiring the wlock. Take it now to ensure proper ordering.
        maypushback = pushop.ui.configbool(b'experimental', b'bundle2.pushback')
        if (
            (not _forcebundle1(pushop))
            and maypushback
            and not bookmod.bookmarksinstore(repo)
        ):
            wlock = pushop.repo.wlock()
        lock = pushop.repo.lock()
        pushop.trmanager = transactionmanager(
            pushop.repo, b'push-response', pushop.remote.url()
        )
    except error.LockUnavailable as err:
        # source repo cannot be locked.
        # We do not abort the push, but just disable the local phase
        # synchronisation.
        msg = b'cannot lock source repository: %s\n' % stringutil.forcebytestr(
            err
        )
        pushop.ui.debug(msg)

    with wlock or util.nullcontextmanager():
        with lock or util.nullcontextmanager():
            with pushop.trmanager or util.nullcontextmanager():
                pushop.repo.checkpush(pushop)
                _checkpublish(pushop)
                _pushdiscovery(pushop)
                if not pushop.force:
                    _checksubrepostate(pushop)
                if not _forcebundle1(pushop):
                    _pushbundle2(pushop)
                _pushchangeset(pushop)
                _pushsyncphase(pushop)
                _pushobsolete(pushop)
                _pushbookmark(pushop)

    if repo.ui.configbool(b'experimental', b'remotenames'):
        logexchange.pullremotenames(repo, remote)

    return pushop


# list of steps to perform discovery before push
pushdiscoveryorder = []

# Mapping between step name and function
#
# This exists to help extensions wrap steps if necessary
pushdiscoverymapping = {}


def pushdiscovery(stepname):
    """decorator for function performing discovery before push

    The function is added to the step -> function mapping and appended to the
    list of steps.  Beware that decorated function will be added in order (this
    may matter).

    You can only use this decorator for a new step, if you want to wrap a step
    from an extension, change the pushdiscovery dictionary directly."""

    def dec(func):
        assert stepname not in pushdiscoverymapping
        pushdiscoverymapping[stepname] = func
        pushdiscoveryorder.append(stepname)
        return func

    return dec


def _pushdiscovery(pushop):
    """Run all discovery steps"""
    for stepname in pushdiscoveryorder:
        step = pushdiscoverymapping[stepname]
        step(pushop)


def _checksubrepostate(pushop):
    """Ensure all outgoing referenced subrepo revisions are present locally"""

    repo = pushop.repo

    # If the repository does not use subrepos, skip the expensive
    # manifest checks.
    if not len(repo.file(b'.hgsub')) or not len(repo.file(b'.hgsubstate')):
        return

    for n in pushop.outgoing.missing:
        ctx = repo[n]

        if b'.hgsub' in ctx.manifest() and b'.hgsubstate' in ctx.files():
            for subpath in sorted(ctx.substate):
                sub = ctx.sub(subpath)
                sub.verify(onpush=True)


@pushdiscovery(b'changeset')
def _pushdiscoverychangeset(pushop):
    """discover the changeset that need to be pushed"""
    fci = discovery.findcommonincoming
    if pushop.revs:
        commoninc = fci(
            pushop.repo,
            pushop.remote,
            force=pushop.force,
            ancestorsof=pushop.revs,
        )
    else:
        commoninc = fci(pushop.repo, pushop.remote, force=pushop.force)
    common, inc, remoteheads = commoninc
    fco = discovery.findcommonoutgoing
    outgoing = fco(
        pushop.repo,
        pushop.remote,
        onlyheads=pushop.revs,
        commoninc=commoninc,
        force=pushop.force,
    )
    pushop.outgoing = outgoing
    pushop.remoteheads = remoteheads
    pushop.incoming = inc


@pushdiscovery(b'phase')
def _pushdiscoveryphase(pushop):
    """discover the phase that needs to be pushed

    (computed for both success and failure case for changesets push)"""
    outgoing = pushop.outgoing
    unfi = pushop.repo.unfiltered()
    remotephases = listkeys(pushop.remote, b'phases')

    if (
        pushop.ui.configbool(b'ui', b'_usedassubrepo')
        and remotephases  # server supports phases
        and not pushop.outgoing.missing  # no changesets to be pushed
        and remotephases.get(b'publishing', False)
    ):
        # When:
        # - this is a subrepo push
        # - and remote support phase
        # - and no changeset are to be pushed
        # - and remote is publishing
        # We may be in issue 3781 case!
        # We drop the possible phase synchronisation done by
        # courtesy to publish changesets possibly locally draft
        # on the remote.
        pushop.outdatedphases = []
        pushop.fallbackoutdatedphases = []
        return

    pushop.remotephases = phases.remotephasessummary(
        pushop.repo, pushop.fallbackheads, remotephases
    )
    droots = pushop.remotephases.draftroots

    extracond = b''
    if not pushop.remotephases.publishing:
        extracond = b' and public()'
    revset = b'heads((%%ln::%%ln) %s)' % extracond
    # Get the list of all revs draft on remote by public here.
    # XXX Beware that revset break if droots is not strictly
    # XXX root we may want to ensure it is but it is costly
    fallback = list(unfi.set(revset, droots, pushop.fallbackheads))
    if not pushop.remotephases.publishing and pushop.publish:
        future = list(
            unfi.set(
                b'%ln and (not public() or %ln::)', pushop.futureheads, droots
            )
        )
    elif not outgoing.missing:
        future = fallback
    else:
        # adds changeset we are going to push as draft
        #
        # should not be necessary for publishing server, but because of an
        # issue fixed in xxxxx we have to do it anyway.
        fdroots = list(
            unfi.set(b'roots(%ln  + %ln::)', outgoing.missing, droots)
        )
        fdroots = [f.node() for f in fdroots]
        future = list(unfi.set(revset, fdroots, pushop.futureheads))
    pushop.outdatedphases = future
    pushop.fallbackoutdatedphases = fallback


@pushdiscovery(b'obsmarker')
def _pushdiscoveryobsmarkers(pushop):
    if not obsolete.isenabled(pushop.repo, obsolete.exchangeopt):
        return

    if not pushop.repo.obsstore:
        return

    if b'obsolete' not in listkeys(pushop.remote, b'namespaces'):
        return

    repo = pushop.repo
    # very naive computation, that can be quite expensive on big repo.
    # However: evolution is currently slow on them anyway.
    nodes = (c.node() for c in repo.set(b'::%ln', pushop.futureheads))
    pushop.outobsmarkers = pushop.repo.obsstore.relevantmarkers(nodes)

