    newtags = util.sortdict()
    for tag, taghist in filetags.items():
        newtags[tag] = (taghist[-1], taghist[:-1])
    return newtags


def _updatetags(filetags, alltags, tagtype=None, tagtypes=None):
    """Incorporate the tag info read from one file into dictionnaries

    The first one, 'alltags', is a "tagmaps" (see 'findglobaltags' for details).

    The second one, 'tagtypes', is optional and will be updated to track the
    "tagtype" of entries in the tagmaps. When set, the 'tagtype' argument also
    needs to be set."""
    if tagtype is None:
        assert tagtypes is None

    for name, nodehist in filetags.items():
        if name not in alltags:
            alltags[name] = nodehist
            if tagtype is not None:
                tagtypes[name] = tagtype
            continue

        # we prefer alltags[name] if:
        #  it supersedes us OR
        #  mutual supersedes and it has a higher rank
        # otherwise we win because we're tip-most
        anode, ahist = nodehist
        bnode, bhist = alltags[name]
        if (
            bnode != anode
            and anode in bhist
            and (bnode not in ahist or len(bhist) > len(ahist))
        ):
            anode = bnode
        elif tagtype is not None:
            tagtypes[name] = tagtype
        ahist.extend([n for n in bhist if n not in ahist])
        alltags[name] = anode, ahist


def _filename(repo):
    """name of a tagcache file for a given repo or repoview"""
    filename = b'tags2'
    if repo.filtername:
        filename = b'%s-%s' % (filename, repo.filtername)
    return filename


def _readtagcache(ui, repo):
    """Read the tag cache.

    Returns a tuple (heads, fnodes, validinfo, cachetags, shouldwrite).

    If the cache is completely up-to-date, "cachetags" is a dict of the
    form returned by _readtags() and "heads", "fnodes", and "validinfo" are
    None and "shouldwrite" is False.

    If the cache is not up to date, "cachetags" is None. "heads" is a list
    of all heads currently in the repository, ordered from tip to oldest.
    "validinfo" is a tuple describing cache validation info. This is used
    when writing the tags cache. "fnodes" is a mapping from head to .hgtags
    filenode. "shouldwrite" is True.

    If the cache is not up to date, the caller is responsible for reading tag
    info from each returned head. (See findglobaltags().)
    """
    try:
        cachefile = repo.cachevfs(_filename(repo), b'r')
        # force reading the file for static-http
        cachelines = iter(cachefile)
    except IOError:
        cachefile = None

    cacherev = None
    cachenode = None
    cachehash = None
    if cachefile:
        try:
            validline = next(cachelines)
            validline = validline.split()
            cacherev = int(validline[0])
            cachenode = bin(validline[1])
            if len(validline) > 2:
                cachehash = bin(validline[2])
        except Exception:
            # corruption of the cache, just recompute it.
            pass

    tipnode = repo.changelog.tip()
    tiprev = len(repo.changelog) - 1

    # Case 1 (common): tip is the same, so nothing has changed.
    # (Unchanged tip trivially means no changesets have been added.
    # But, thanks to localrepository.destroyed(), it also means none
    # have been destroyed by strip or rollback.)
    if (
        cacherev == tiprev
        and cachenode == tipnode
        and cachehash == scmutil.filteredhash(repo, tiprev)
    ):
        tags = _readtags(ui, repo, cachelines, cachefile.name)
        cachefile.close()
        return (None, None, None, tags, False)
    if cachefile:
        cachefile.close()  # ignore rest of file

    valid = (tiprev, tipnode, scmutil.filteredhash(repo, tiprev))

    repoheads = repo.heads()
    # Case 2 (uncommon): empty repo; get out quickly and don't bother
    # writing an empty cache.
    if repoheads == [repo.nullid]:
        return ([], {}, valid, {}, False)

    # Case 3 (uncommon): cache file missing or empty.

    # Case 4 (uncommon): tip rev decreased.  This should only happen
    # when we're called from localrepository.destroyed().  Refresh the
    # cache so future invocations will not see disappeared heads in the
    # cache.

    # Case 5 (common): tip has changed, so we've added/replaced heads.

    # As it happens, the code to handle cases 3, 4, 5 is the same.

    # N.B. in case 4 (nodes destroyed), "new head" really means "newly
    # exposed".
    if not len(repo.file(b'.hgtags')):
        # No tags have ever been committed, so we can avoid a
        # potentially expensive search.
        return ([], {}, valid, None, True)

    # Now we have to lookup the .hgtags filenode for every new head.
    # This is the most expensive part of finding tags, so performance
    # depends primarily on the size of newheads.  Worst case: no cache
    # file, so newheads == repoheads.
    # Reversed order helps the cache ('repoheads' is in descending order)
    cachefnode = _getfnodes(ui, repo, reversed(repoheads))

    # Caller has to iterate over all heads, but can use the filenodes in
    # cachefnode to get to each .hgtags revision quickly.
    return (repoheads, cachefnode, valid, None, True)


def _getfnodes(ui, repo, nodes):
    """return .hgtags fnodes for a list of changeset nodes

    Return value is a {node: fnode} mapping. There will be no entry for nodes
    without a '.hgtags' file.
    """
    starttime = util.timer()
    fnodescache = hgtagsfnodescache(repo.unfiltered())
    cachefnode = {}
    validated_fnodes = set()
    unknown_entries = set()

    flog = None
    for node in nodes:
        fnode = fnodescache.getfnode(node)
        if fnode != repo.nullid:
            if fnode not in validated_fnodes:
                if flog is None:
                    flog = repo.file(b'.hgtags')
                if flog.hasnode(fnode):
                    validated_fnodes.add(fnode)
                else:
                    unknown_entries.add(node)
            cachefnode[node] = fnode

    if unknown_entries:
        fixed_nodemap = fnodescache.refresh_invalid_nodes(unknown_entries)
        for node, fnode in fixed_nodemap.items():
            if fnode != repo.nullid:
                cachefnode[node] = fnode

    fnodescache.write()

    duration = util.timer() - starttime
    ui.log(
        b'tagscache',
        b'%d/%d cache hits/lookups in %0.4f seconds\n',
        fnodescache.hitcount,
        fnodescache.lookupcount,
        duration,
    )
    return cachefnode


def _writetagcache(ui, repo, valid, cachetags):
    filename = _filename(repo)
    try:
        cachefile = repo.cachevfs(filename, b'w', atomictemp=True)
    except (OSError, IOError):
        return

    ui.log(
        b'tagscache',
        b'writing .hg/cache/%s with %d tags\n',
        filename,
        len(cachetags),
    )

    if valid[2]:
        cachefile.write(
            b'%d %s %s\n' % (valid[0], hex(valid[1]), hex(valid[2]))
        )
    else:
        cachefile.write(b'%d %s\n' % (valid[0], hex(valid[1])))

    # Tag names in the cache are in UTF-8 -- which is the whole reason
    # we keep them in UTF-8 throughout this module.  If we converted
    # them local encoding on input, we would lose info writing them to
    # the cache.
    for (name, (node, hist)) in sorted(cachetags.items()):
        for n in hist:
            cachefile.write(b"%s %s\n" % (hex(n), name))
        cachefile.write(b"%s %s\n" % (hex(node), name))

    try:
        cachefile.close()
    except (OSError, IOError):
        pass


def tag(repo, names, node, message, local, user, date, editor=False):
    """tag a revision with one or more symbolic names.

    names is a list of strings or, when adding a single tag, names may be a
    string.

    if local is True, the tags are stored in a per-repository file.
    otherwise, they are stored in the .hgtags file, and a new
    changeset is committed with the change.

    keyword arguments:

    local: whether to store tags in non-version-controlled file
    (default False)

    message: commit message to use if committing

    user: name of user to use if committing

    date: date tuple to use if committing"""

    if not local:
        m = matchmod.exact([b'.hgtags'])
        st = repo.status(match=m, unknown=True, ignored=True)
        if any(
            (
                st.modified,
                st.added,
                st.removed,
                st.deleted,
                st.unknown,
                st.ignored,
            )
        ):
            raise error.Abort(
                _(b'working copy of .hgtags is changed'),
                hint=_(b'please commit .hgtags manually'),
            )

    with repo.wlock():
        repo.tags()  # instantiate the cache
        _tag(repo, names, node, message, local, user, date, editor=editor)


def _tag(
    repo, names, node, message, local, user, date, extra=None, editor=False
):
    if isinstance(names, bytes):
        names = (names,)

    branches = repo.branchmap()
    for name in names:
        repo.hook(b'pretag', throw=True, node=hex(node), tag=name, local=local)
        if name in branches:
            repo.ui.warn(
                _(b"warning: tag %s conflicts with existing branch name\n")
                % name
            )

    def writetags(fp, names, munge, prevtags):
        fp.seek(0, io.SEEK_END)
        if prevtags and not prevtags.endswith(b'\n'):
            fp.write(b'\n')
        for name in names:
            if munge:
                m = munge(name)
            else:
                m = name

            if repo._tagscache.tagtypes and name in repo._tagscache.tagtypes:
                old = repo.tags().get(name, repo.nullid)
                fp.write(b'%s %s\n' % (hex(old), m))
            fp.write(b'%s %s\n' % (hex(node), m))
        fp.close()

    prevtags = b''
    if local:
        try:
            fp = repo.vfs(b'localtags', b'r+')
        except IOError:
            fp = repo.vfs(b'localtags', b'a')
        else:
            prevtags = fp.read()

        # local tags are stored in the current charset
        writetags(fp, names, None, prevtags)
        for name in names:
            repo.hook(b'tag', node=hex(node), tag=name, local=local)
        return

    try:
        fp = repo.wvfs(b'.hgtags', b'rb+')
    except FileNotFoundError:
        fp = repo.wvfs(b'.hgtags', b'ab')
    else:
        prevtags = fp.read()

    # committed tags are stored in UTF-8
    writetags(fp, names, encoding.fromlocal, prevtags)

    fp.close()

    repo.invalidatecaches()

    if b'.hgtags' not in repo.dirstate:
        repo[None].add([b'.hgtags'])

    m = matchmod.exact([b'.hgtags'])
    tagnode = repo.commit(
        message, user, date, extra=extra, match=m, editor=editor
    )

    for name in names:
        repo.hook(b'tag', node=hex(node), tag=name, local=local)

    return tagnode


_fnodescachefile = b'hgtagsfnodes1'
_fnodesrecsize = 4 + 20  # changeset fragment + filenode
_fnodesmissingrec = b'\xff' * 24


class hgtagsfnodescache:
    """Persistent cache mapping revisions to .hgtags filenodes.

    The cache is an array of records. Each item in the array corresponds to
    a changelog revision. Values in the array contain the first 4 bytes of
    the node hash and the 20 bytes .hgtags filenode for that revision.

    The first 4 bytes are present as a form of verification. Repository
    stripping and rewriting may change the node at a numeric revision in the
    changelog. The changeset fragment serves as a verifier to detect
    rewriting. This logic is shared with the rev branch cache (see
    branchmap.py).

    The instance holds in memory the full cache content but entries are
    only parsed on read.

    Instances behave like lists. ``c[i]`` works where i is a rev or
    changeset node. Missing indexes are populated automatically on access.
    """

    def __init__(self, repo):
        assert repo.filtername is None

        self._repo = repo

        # Only for reporting purposes.
        self.lookupcount = 0
        self.hitcount = 0

        try:
            data = repo.cachevfs.read(_fnodescachefile)
        except (OSError, IOError):
            data = b""
        self._raw = bytearray(data)

        # The end state of self._raw is an array that is of the exact length
        # required to hold a record for every revision in the repository.
        # We truncate or extend the array as necessary. self._dirtyoffset is
        # defined to be the start offset at which we need to write the output
        # file. This offset is also adjusted when new entries are calculated
        # for array members.
        cllen = len(repo.changelog)
        wantedlen = cllen * _fnodesrecsize
        rawlen = len(self._raw)

        self._dirtyoffset = None

        rawlentokeep = min(
            wantedlen, (rawlen // _fnodesrecsize) * _fnodesrecsize
        )
        if rawlen > rawlentokeep:
            # There's no easy way to truncate array instances. This seems
            # slightly less evil than copying a potentially large array slice.
            for i in range(rawlen - rawlentokeep):
                self._raw.pop()
            rawlen = len(self._raw)
            self._dirtyoffset = rawlen
        if rawlen < wantedlen:
            if self._dirtyoffset is None:
                self._dirtyoffset = rawlen
            # TODO: zero fill entire record, because it's invalid not missing?
            self._raw.extend(b'\xff' * (wantedlen - rawlen))

    def getfnode(self, node, computemissing=True):
        """Obtain the filenode of the .hgtags file at a specified revision.

        If the value is in the cache, the entry will be validated and returned.
        Otherwise, the filenode will be computed and returned unless
        "computemissing" is False.  In that case, None will be returned if
        the entry is missing or False if the entry is invalid without
        any potentially expensive computation being performed.

        If an .hgtags does not exist at the specified revision, nullid is
        returned.
        """
        if node == self._repo.nullid:
            return node

        rev = self._repo.changelog.rev(node)

        self.lookupcount += 1

        offset = rev * _fnodesrecsize
        record = b'%s' % self._raw[offset : offset + _fnodesrecsize]
        properprefix = node[0:4]

        # Validate and return existing entry.
        if record != _fnodesmissingrec and len(record) == _fnodesrecsize:
            fileprefix = record[0:4]

            if fileprefix == properprefix:
                self.hitcount += 1
                return record[4:]

            # Fall through.

        # If we get here, the entry is either missing or invalid.

        if not computemissing:
            if record != _fnodesmissingrec:
                return False
            return None

        fnode = self._computefnode(node)
        self._writeentry(offset, properprefix, fnode)
        return fnode

    def _computefnode(self, node):
        """Finds the tag filenode for a node which is missing or invalid
        in cache"""
        ctx = self._repo[node]
        rev = ctx.rev()
        fnode = None
        cl = self._repo.changelog
        p1rev, p2rev = cl._uncheckedparentrevs(rev)
        p1node = cl.node(p1rev)
        p1fnode = self.getfnode(p1node, computemissing=False)
        if p2rev != nullrev:
            # There is some no-merge changeset where p1 is null and p2 is set
            # Processing them as merge is just slower, but still gives a good
            # result.
            p2node = cl.node(p2rev)
            p2fnode = self.getfnode(p2node, computemissing=False)
            if p1fnode != p2fnode:
                # we cannot rely on readfast because we don't know against what
                # parent the readfast delta is computed
                p1fnode = None
        if p1fnode:
            mctx = ctx.manifestctx()
            fnode = mctx.readfast().get(b'.hgtags')
            if fnode is None:
                fnode = p1fnode
        if fnode is None:
            # Populate missing entry.
            try:
                fnode = ctx.filenode(b'.hgtags')
            except error.LookupError:
                # No .hgtags file on this revision.
                fnode = self._repo.nullid
        return fnode

    def setfnode(self, node, fnode):
        """Set the .hgtags filenode for a given changeset."""
        assert len(fnode) == 20
        ctx = self._repo[node]

        # Do a lookup first to avoid writing if nothing has changed.
        if self.getfnode(ctx.node(), computemissing=False) == fnode:
            return

        self._writeentry(ctx.rev() * _fnodesrecsize, node[0:4], fnode)

    def refresh_invalid_nodes(self, nodes):
        """recomputes file nodes for a given set of nodes which has unknown
        filenodes for them in the cache
        Also updates the in-memory cache with the correct filenode.
        Caller needs to take care about calling `.write()` so that updates are
        persisted.
        Returns a map {node: recomputed fnode}
        """
        fixed_nodemap = {}
        for node in nodes:
            fnode = self._computefnode(node)
            fixed_nodemap[node] = fnode
            self.setfnode(node, fnode)
        return fixed_nodemap

    def _writeentry(self, offset, prefix, fnode):
        # Slices on array instances only accept other array.
        entry = bytearray(prefix + fnode)
        self._raw[offset : offset + _fnodesrecsize] = entry
        # self._dirtyoffset could be None.
        self._dirtyoffset = min(self._dirtyoffset or 0, offset or 0)

    def write(self):
        """Perform all necessary writes to cache file.

        This may no-op if no writes are needed or if a write lock could
        not be obtained.
        """
        if self._dirtyoffset is None:
            return

        data = self._raw[self._dirtyoffset :]
        if not data:
            return

        repo = self._repo

        try:
            lock = repo.lock(wait=False)
        except error.LockError:
            repo.ui.log(
                b'tagscache',
                b'not writing .hg/cache/%s because '
                b'lock cannot be acquired\n' % _fnodescachefile,
            )
            return

        try:
            f = repo.cachevfs.open(_fnodescachefile, b'ab')
            try:
                # if the file has been truncated
                actualoffset = f.tell()
                if actualoffset < self._dirtyoffset:
                    self._dirtyoffset = actualoffset
                    data = self._raw[self._dirtyoffset :]
                f.seek(self._dirtyoffset)
                f.truncate()
                repo.ui.log(
                    b'tagscache',
                    b'writing %d bytes to cache/%s\n'
                    % (len(data), _fnodescachefile),
                )
                f.write(data)
                self._dirtyoffset = None
            finally:
                f.close()
        except (IOError, OSError) as inst:
            repo.ui.log(
                b'tagscache',
                b"couldn't write cache/%s: %s\n"
                % (_fnodescachefile, stringutil.forcebytestr(inst)),
            )
        finally:
            lock.release()
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               usr/lib/python3/dist-packages/mercurial/templatefilters.py                                          0000644 0000000 0000000 00000036256 14355257011 022526  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # templatefilters.py - common template expansion filters
#
# Copyright 2005-2008 Olivia Mackall <olivia@selenic.com>
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.


import os
import re
import time

from .i18n import _
from .node import hex
from . import (
    encoding,
    error,
    pycompat,
    registrar,
    smartset,
    templateutil,
    url,
    util,
)
from .utils import (
    cborutil,
    dateutil,
    stringutil,
)

urlerr = util.urlerr
urlreq = util.urlreq

# filters are callables like:
#   fn(obj)
# with:
#   obj - object to be filtered (text, date, list and so on)
filters = {}

templatefilter = registrar.templatefilter(filters)


@templatefilter(b'addbreaks', intype=bytes)
def addbreaks(text):
    """Any text. Add an XHTML "<br />" tag before the end of
    every line except the last.
    """
    return text.replace(b'\n', b'<br/>\n')


agescales = [
    (b"year", 3600 * 24 * 365, b'Y'),
    (b"month", 3600 * 24 * 30, b'M'),
    (b"week", 3600 * 24 * 7, b'W'),
    (b"day", 3600 * 24, b'd'),
    (b"hour", 3600, b'h'),
    (b"minute", 60, b'm'),
    (b"second", 1, b's'),
]


@templatefilter(b'age', intype=templateutil.date)
def age(date, abbrev=False):
    """Date. Returns a human-readable date/time difference between the
    given date/time and the current date/time.
    """

    def plural(t, c):
        if c == 1:
            return t
        return t + b"s"

    def fmt(t, c, a):
        if abbrev:
            return b"%d%s" % (c, a)
        return b"%d %s" % (c, plural(t, c))

    now = time.time()
    then = date[0]
    future = False
    if then > now:
        future = True
        delta = max(1, int(then - now))
        if delta > agescales[0][1] * 30:
            return b'in the distant future'
    else:
        delta = max(1, int(now - then))
        if delta > agescales[0][1] * 2:
            return dateutil.shortdate(date)

    for t, s, a in agescales:
        n = delta // s
        if n >= 2 or s == 1:
            if future:
                return b'%s from now' % fmt(t, n, a)
            return b'%s ago' % fmt(t, n, a)


@templatefilter(b'basename', intype=bytes)
def basename(path):
    """Any text. Treats the text as a path, and returns the last
    component of the path after splitting by the path separator.
    For example, "foo/bar/baz" becomes "baz" and "foo/bar//" becomes "".
    """
    return os.path.basename(path)


def _tocborencodable(obj):
    if isinstance(obj, smartset.abstractsmartset):
        return list(obj)
    return obj


@templatefilter(b'cbor')
def cbor(obj):
    """Any object. Serializes the object to CBOR bytes."""
    # cborutil is stricter about type than json() filter
    obj = pycompat.rapply(_tocborencodable, obj)
    return b''.join(cborutil.streamencode(obj))


@templatefilter(b'commondir')
def commondir(filelist):
    """List of text. Treats each list item as file name with /
    as path separator and returns the longest common directory
    prefix shared by all list items.
    Returns the empty string if no common prefix exists.

    The list items are not normalized, i.e. "foo/../bar" is handled as
    file "bar" in the directory "foo/..". Leading slashes are ignored.

    For example, ["foo/bar/baz", "foo/baz/bar"] becomes "foo" and
    ["foo/bar", "baz"] becomes "".
    """

    def common(a, b):
        if len(a) > len(b):
            a = b[: len(a)]
        elif len(b) > len(a):
            b = b[: len(a)]
        if a == b:
            return a
        for i in range(len(a)):
            if a[i] != b[i]:
                return a[:i]
        return a

    try:
        if not filelist:
            return b""
        dirlist = [f.lstrip(b'/').split(b'/')[:-1] for f in filelist]
        if len(dirlist) == 1:
            return b'/'.join(dirlist[0])
        a = min(dirlist)
        b = max(dirlist)
        # The common prefix of a and b is shared with all
        # elements of the list since Python sorts lexicographical
        # and [1, x] after [1].
        return b'/'.join(common(a, b))
    except TypeError:
        raise error.ParseError(_(b'argument is not a list of text'))


@templatefilter(b'count')
def count(i):
    """List or text. Returns the length as an integer."""
    try:
        return len(i)
    except TypeError:
        raise error.ParseError(_(b'not countable'))


@templatefilter(b'dirname', intype=bytes)
def dirname(path):
    """Any text. Treats the text as a path, and strips the last
    component of the path after splitting by the path separator.
    """
    return os.path.dirname(path)


@templatefilter(b'domain', intype=bytes)
def domain(author):
    """Any text. Finds the first string that looks like an email
    address, and extracts just the domain component. Example: ``User
    <user@example.com>`` becomes ``example.com``.
    """
    f = author.find(b'@')
    if f == -1:
        return b''
    author = author[f + 1 :]
    f = author.find(b'>')
    if f >= 0:
        author = author[:f]
    return author


@templatefilter(b'email', intype=bytes)
def email(text):
    """Any text. Extracts the first string that looks like an email
    address. Example: ``User <user@example.com>`` becomes
    ``user@example.com``.
    """
    return stringutil.email(text)


@templatefilter(b'escape', intype=bytes)
def escape(text):
    """Any text. Replaces the special XML/XHTML characters "&", "<"
    and ">" with XML entities, and filters out NUL characters.
    """
    return url.escape(text.replace(b'\0', b''), True)


para_re = None
space_re = None


def fill(text, width, initindent=b'', hangindent=b''):
    '''fill many paragraphs with optional indentation.'''
    global para_re, space_re
    if para_re is None:
        para_re = re.compile(b'(\n\n|\n\\s*[-*]\\s*)', re.M)
        space_re = re.compile(br'  +')

    def findparas():
        start = 0
        while True:
            m = para_re.search(text, start)
            if not m:
                uctext = encoding.unifromlocal(text[start:])
                w = len(uctext)
                while w > 0 and uctext[w - 1].isspace():
                    w -= 1
                yield (
                    encoding.unitolocal(uctext[:w]),
                    encoding.unitolocal(uctext[w:]),
                )
                break
            yield text[start : m.start(0)], m.group(1)
            start = m.end(1)

    return b"".join(
        [
            stringutil.wrap(
                space_re.sub(b' ', stringutil.wrap(para, width)),
                width,
                initindent,
                hangindent,
            )
            + rest
            for para, rest in findparas()
        ]
    )


@templatefilter(b'fill68', intype=bytes)
def fill68(text):
    """Any text. Wraps the text to fit in 68 columns."""
    return fill(text, 68)


@templatefilter(b'fill76', intype=bytes)
def fill76(text):
    """Any text. Wraps the text to fit in 76 columns."""
    return fill(text, 76)


@templatefilter(b'firstline', intype=bytes)
def firstline(text):
    """Any text. Returns the first line of text."""
    return stringutil.firstline(text)


@templatefilter(b'hex', intype=bytes)
def hexfilter(text):
    """Any text. Convert a binary Mercurial node identifier into
    its long hexadecimal representation.
    """
    return hex(text)


@templatefilter(b'hgdate', intype=templateutil.date)
def hgdate(text):
    """Date. Returns the date as a pair of numbers: "1157407993
    25200" (Unix timestamp, timezone offset).
    """
    return b"%d %d" % text


@templatefilter(b'isodate', intype=templateutil.date)
def isodate(text):
    """Date. Returns the date in ISO 8601 format: "2009-08-18 13:00
    +0200".
    """
    return dateutil.datestr(text, b'%Y-%m-%d %H:%M %1%2')


@templatefilter(b'isodatesec', intype=templateutil.date)
def isodatesec(text):
    """Date. Returns the date in ISO 8601 format, including
    seconds: "2009-08-18 13:00:13 +0200". See also the rfc3339date
    filter.
    """
    return dateutil.datestr(text, b'%Y-%m-%d %H:%M:%S %1%2')


def indent(text, prefix, firstline=b''):
    '''indent each non-empty line of text after first with prefix.'''
    lines = text.splitlines()
    num_lines = len(lines)
    endswithnewline = text[-1:] == b'\n'

    def indenter():
        for i in range(num_lines):
            l = lines[i]
            if l.strip():
                yield prefix if i else firstline
            yield l
            if i < num_lines - 1 or endswithnewline:
                yield b'\n'

    return b"".join(indenter())


@templatefilter(b'json')
def json(obj, paranoid=True):
    """Any object. Serializes the object to a JSON formatted text."""
    if obj is None:
        return b'null'
    elif obj is False:
        return b'false'
    elif obj is True:
        return b'true'
    elif isinstance(obj, (int, int, float)):
        return pycompat.bytestr(obj)
    elif isinstance(obj, bytes):
        return b'"%s"' % encoding.jsonescape(obj, paranoid=paranoid)
    elif isinstance(obj, type(u'')):
        raise error.ProgrammingError(
            b'Mercurial only does output with bytes: %r' % obj
        )
    elif util.safehasattr(obj, b'keys'):
        out = [
            b'"%s": %s'
            % (encoding.jsonescape(k, paranoid=paranoid), json(v, paranoid))
            for k, v in sorted(obj.items())
        ]
        return b'{' + b', '.join(out) + b'}'
    elif util.safehasattr(obj, b'__iter__'):
        out = [json(i, paranoid) for i in obj]
        return b'[' + b', '.join(out) + b']'
    raise error.ProgrammingError(b'cannot encode %r' % obj)


@templatefilter(b'lower', intype=bytes)
def lower(text):
    """Any text. Converts the text to lowercase."""
    return encoding.lower(text)


@templatefilter(b'nonempty', intype=bytes)
def nonempty(text):
    """Any text. Returns '(none)' if the string is empty."""
    return text or b"(none)"


@templatefilter(b'obfuscate', intype=bytes)
def obfuscate(text):
    """Any text. Returns the input text rendered as a sequence of
    XML entities.
    """
    text = str(text, pycompat.sysstr(encoding.encoding), r'replace')
    return b''.join([b'&#%d;' % ord(c) for c in text])


@templatefilter(b'permissions', intype=bytes)
def permissions(flags):
    if b"l" in flags:
        return b"lrwxrwxrwx"
    if b"x" in flags:
        return b"-rwxr-xr-x"
    return b"-rw-r--r--"


@templatefilter(b'person', intype=bytes)
def person(author):
    """Any text. Returns the name before an email address,
    interpreting it as per RFC 5322.
    """
    return stringutil.person(author)


@templatefilter(b'reverse')
def reverse(list_):
    """List. Reverses the order of list items."""
    if isinstance(list_, list):
        return templateutil.hybridlist(list_[::-1], name=b'item')
    raise error.ParseError(_(b'not reversible'))


@templatefilter(b'revescape', intype=bytes)
def revescape(text):
    """Any text. Escapes all "special" characters, except @.
    Forward slashes are escaped twice to prevent web servers from prematurely
    unescaping them. For example, "@foo bar/baz" becomes "@foo%20bar%252Fbaz".
    """
    return urlreq.quote(text, safe=b'/@').replace(b'/', b'%252F')


@templatefilter(b'rfc3339date', intype=templateutil.date)
def rfc3339date(text):
    """Date. Returns a date using the Internet date format
    specified in RFC 3339: "2009-08-18T13:00:13+02:00".
    """
    return dateutil.datestr(text, b"%Y-%m-%dT%H:%M:%S%1:%2")


@templatefilter(b'rfc822date', intype=templateutil.date)
def rfc822date(text):
    """Date. Returns a date using the same format used in email
    headers: "Tue, 18 Aug 2009 13:00:13 +0200".
    """
    return dateutil.datestr(text, b"%a, %d %b %Y %H:%M:%S %1%2")


