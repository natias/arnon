            directive = block[b'admonitiontitle']
            title = block[b'lines'][0].strip() if block[b'lines'] else None

            if i + 1 == len(blocks):
                raise error.Abort(
                    _(
                        b'changeset %s: release notes directive %s '
                        b'lacks content'
                    )
                    % (ctx, directive)
                )

            # Now search ahead and find all paragraphs attached to this
            # admonition.
            paragraphs = []
            for j in range(i + 1, len(blocks)):
                pblock = blocks[j]

                # Margin blocks may appear between paragraphs. Ignore them.
                if pblock[b'type'] == b'margin':
                    continue

                if pblock[b'type'] == b'admonition':
                    break

                if pblock[b'type'] != b'paragraph':
                    repo.ui.warn(
                        _(
                            b'changeset %s: unexpected block in release '
                            b'notes directive %s\n'
                        )
                        % (ctx, directive)
                    )

                if pblock[b'indent'] > 0:
                    paragraphs.append(pblock[b'lines'])
                else:
                    break

            # TODO consider using title as paragraph for more concise notes.
            if not paragraphs:
                repo.ui.warn(
                    _(b"error parsing releasenotes for revision: '%s'\n")
                    % hex(ctx.node())
                )
            if title:
                notes.addtitleditem(directive, title, paragraphs)
            else:
                notes.addnontitleditem(directive, paragraphs)

    return notes


def parsereleasenotesfile(sections, text):
    """Parse text content containing generated release notes."""
    notes = parsedreleasenotes()

    blocks = minirst.parse(text)[0]

    def gatherparagraphsbullets(offset, title=False):
        notefragment = []

        for i in range(offset + 1, len(blocks)):
            block = blocks[i]

            if block[b'type'] == b'margin':
                continue
            elif block[b'type'] == b'section':
                break
            elif block[b'type'] == b'bullet':
                if block[b'indent'] != 0:
                    raise error.Abort(_(b'indented bullet lists not supported'))
                if title:
                    lines = [l[1:].strip() for l in block[b'lines']]
                    notefragment.append(lines)
                    continue
                else:
                    lines = [[l[1:].strip() for l in block[b'lines']]]

                    for block in blocks[i + 1 :]:
                        if block[b'type'] in (b'bullet', b'section'):
                            break
                        if block[b'type'] == b'paragraph':
                            lines.append(block[b'lines'])
                    notefragment.append(lines)
                    continue
            elif block[b'type'] != b'paragraph':
                raise error.Abort(
                    _(b'unexpected block type in release notes: %s')
                    % block[b'type']
                )
            if title:
                notefragment.append(block[b'lines'])

        return notefragment

    currentsection = None
    for i, block in enumerate(blocks):
        if block[b'type'] != b'section':
            continue

        title = block[b'lines'][0]

        # TODO the parsing around paragraphs and bullet points needs some
        # work.
        if block[b'underline'] == b'=':  # main section
            name = sections.sectionfromtitle(title)
            if not name:
                raise error.Abort(
                    _(b'unknown release notes section: %s') % title
                )

            currentsection = name
            bullet_points = gatherparagraphsbullets(i)
            if bullet_points:
                for para in bullet_points:
                    notes.addnontitleditem(currentsection, para)

        elif block[b'underline'] == b'-':  # sub-section
            if title == BULLET_SECTION:
                bullet_points = gatherparagraphsbullets(i)
                for para in bullet_points:
                    notes.addnontitleditem(currentsection, para)
            else:
                paragraphs = gatherparagraphsbullets(i, True)
                notes.addtitleditem(currentsection, title, paragraphs)
        else:
            raise error.Abort(_(b'unsupported section type for %s') % title)

    return notes


def serializenotes(sections, notes):
    """Serialize release notes from parsed fragments and notes.

    This function essentially takes the output of ``parsenotesfromrevisions()``
    and ``parserelnotesfile()`` and produces output combining the 2.
    """
    lines = []

    for sectionname, sectiontitle in sections:
        if sectionname not in notes:
            continue

        lines.append(sectiontitle)
        lines.append(b'=' * len(sectiontitle))
        lines.append(b'')

        # First pass to emit sub-sections.
        for title, paragraphs in notes.titledforsection(sectionname):
            lines.append(title)
            lines.append(b'-' * len(title))
            lines.append(b'')

            for i, para in enumerate(paragraphs):
                if i:
                    lines.append(b'')
                lines.extend(
                    stringutil.wrap(b' '.join(para), width=78).splitlines()
                )

            lines.append(b'')

        # Second pass to emit bullet list items.

        # If the section has titled and non-titled items, we can't
        # simply emit the bullet list because it would appear to come
        # from the last title/section. So, we emit a new sub-section
        # for the non-titled items.
        nontitled = notes.nontitledforsection(sectionname)
        if notes.titledforsection(sectionname) and nontitled:
            # TODO make configurable.
            lines.append(BULLET_SECTION)
            lines.append(b'-' * len(BULLET_SECTION))
            lines.append(b'')

        for paragraphs in nontitled:
            lines.extend(
                stringutil.wrap(
                    b' '.join(paragraphs[0]),
                    width=78,
                    initindent=b'* ',
                    hangindent=b'  ',
                ).splitlines()
            )

            for para in paragraphs[1:]:
                lines.append(b'')
                lines.extend(
                    stringutil.wrap(
                        b' '.join(para),
                        width=78,
                        initindent=b'  ',
                        hangindent=b'  ',
                    ).splitlines()
                )

            lines.append(b'')

    if lines and lines[-1]:
        lines.append(b'')

    return b'\n'.join(lines)


@command(
    b'releasenotes',
    [
        (
            b'r',
            b'rev',
            b'',
            _(b'revisions to process for release notes'),
            _(b'REV'),
        ),
        (
            b'c',
            b'check',
            False,
            _(b'checks for validity of admonitions (if any)'),
            _(b'REV'),
        ),
        (
            b'l',
            b'list',
            False,
            _(b'list the available admonitions with their title'),
            None,
        ),
    ],
    _(b'hg releasenotes [-r REV] [-c] FILE'),
    helpcategory=command.CATEGORY_CHANGE_NAVIGATION,
)
def releasenotes(ui, repo, file_=None, **opts):
    """parse release notes from commit messages into an output file

    Given an output file and set of revisions, this command will parse commit
    messages for release notes then add them to the output file.

    Release notes are defined in commit messages as ReStructuredText
    directives. These have the form::

       .. directive:: title

          content

    Each ``directive`` maps to an output section in a generated release notes
    file, which itself is ReStructuredText. For example, the ``.. feature::``
    directive would map to a ``New Features`` section.

    Release note directives can be either short-form or long-form. In short-
    form, ``title`` is omitted and the release note is rendered as a bullet
    list. In long form, a sub-section with the title ``title`` is added to the
    section.

    The ``FILE`` argument controls the output file to write gathered release
    notes to. The format of the file is::

       Section 1
       =========

       ...

       Section 2
       =========

       ...

    Only sections with defined release notes are emitted.

    If a section only has short-form notes, it will consist of bullet list::

       Section
       =======

       * Release note 1
       * Release note 2

    If a section has long-form notes, sub-sections will be emitted::

       Section
       =======

       Note 1 Title
       ------------

       Description of the first long-form note.

       Note 2 Title
       ------------

       Description of the second long-form note.

    If the ``FILE`` argument points to an existing file, that file will be
    parsed for release notes having the format that would be generated by this
    command. The notes from the processed commit messages will be *merged*
    into this parsed set.

    During release notes merging:

    * Duplicate items are automatically ignored
    * Items that are different are automatically ignored if the similarity is
      greater than a threshold.

    This means that the release notes file can be updated independently from
    this command and changes should not be lost when running this command on
    that file. A particular use case for this is to tweak the wording of a
    release note after it has been added to the release notes file.

    The -c/--check option checks the commit message for invalid admonitions.

    The -l/--list option, presents the user with a list of existing available
    admonitions along with their title. This also includes the custom
    admonitions (if any).
    """

    opts = pycompat.byteskwargs(opts)
    sections = releasenotessections(ui, repo)

    cmdutil.check_incompatible_arguments(opts, b'list', [b'rev', b'check'])

    if opts.get(b'list'):
        return _getadmonitionlist(ui, sections)

    rev = opts.get(b'rev')
    revs = logcmdutil.revrange(repo, [rev or b'not public()'])
    if opts.get(b'check'):
        return checkadmonitions(ui, repo, sections.names(), revs)

    incoming = parsenotesfromrevisions(repo, sections.names(), revs)

    if file_ is None:
        ui.pager(b'releasenotes')
        return ui.write(serializenotes(sections, incoming))

    try:
        with open(file_, b'rb') as fh:
            notes = parsereleasenotesfile(sections, fh.read())
    except FileNotFoundError:
        notes = parsedreleasenotes()

    notes.merge(ui, incoming)

    with open(file_, b'wb') as fh:
        fh.write(serializenotes(sections, notes))


@command(b'debugparsereleasenotes', norepo=True)
def debugparsereleasenotes(ui, path, repo=None):
    """parse release notes and print resulting data structure"""
    if path == b'-':
        text = procutil.stdin.read()
    else:
        with open(path, b'rb') as fh:
            text = fh.read()

    sections = releasenotessections(ui, repo)

    notes = parsereleasenotesfile(sections, text)

    for section in notes:
        ui.write(_(b'section: %s\n') % section)
        for title, paragraphs in notes.titledforsection(section):
            ui.write(_(b'  subsection: %s\n') % title)
            for para in paragraphs:
                ui.write(_(b'    paragraph: %s\n') % b' '.join(para))

        for paragraphs in notes.nontitledforsection(section):
            ui.write(_(b'  bullet point:\n'))
            for para in paragraphs:
                ui.write(_(b'    paragraph: %s\n') % b' '.join(para))
                                                                                                                                                                                                                                                    usr/lib/python3/dist-packages/hgext/relink.py                                                       0000644 0000000 0000000 00000015171 14355257011 017733  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # Mercurial extension to provide 'hg relink' command
#
# Copyright (C) 2007 Brendan Cully <brendan@kublai.com>
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.

"""recreates hardlinks between repository clones"""

import os
import stat

from mercurial.i18n import _
from mercurial.pycompat import open
from mercurial import (
    error,
    hg,
    registrar,
    util,
)
from mercurial.utils import (
    stringutil,
    urlutil,
)

cmdtable = {}
command = registrar.command(cmdtable)
# Note for extension authors: ONLY specify testedwith = 'ships-with-hg-core' for
# extensions which SHIP WITH MERCURIAL. Non-mainline extensions should
# be specifying the version(s) of Mercurial they are tested with, or
# leave the attribute unspecified.
testedwith = b'ships-with-hg-core'


@command(
    b'relink', [], _(b'[ORIGIN]'), helpcategory=command.CATEGORY_MAINTENANCE
)
def relink(ui, repo, origin=None, **opts):
    """recreate hardlinks between two repositories

    When repositories are cloned locally, their data files will be
    hardlinked so that they only use the space of a single repository.

    Unfortunately, subsequent pulls into either repository will break
    hardlinks for any files touched by the new changesets, even if
    both repositories end up pulling the same changes.

    Similarly, passing --rev to "hg clone" will fail to use any
    hardlinks, falling back to a complete copy of the source
    repository.

    This command lets you recreate those hardlinks and reclaim that
    wasted space.

    This repository will be relinked to share space with ORIGIN, which
    must be on the same local disk. If ORIGIN is omitted, looks for
    "default-relink", then "default", in [paths].

    Do not attempt any read operations on this repository while the
    command is running. (Both repositories will be locked against
    writes.)
    """
    if not util.safehasattr(util, b'samefile') or not util.safehasattr(
        util, b'samedevice'
    ):
        raise error.Abort(_(b'hardlinks are not supported on this system'))

    if origin is None and b'default-relink' in ui.paths:
        origin = b'default-relink'
    path, __ = urlutil.get_unique_pull_path(b'relink', repo, ui, origin)
    src = hg.repository(repo.baseui, path)
    ui.status(_(b'relinking %s to %s\n') % (src.store.path, repo.store.path))
    if repo.root == src.root:
        ui.status(_(b'there is nothing to relink\n'))
        return

    if not util.samedevice(src.store.path, repo.store.path):
        # No point in continuing
        raise error.Abort(_(b'source and destination are on different devices'))

    with repo.lock(), src.lock():
        candidates = sorted(collect(src, ui))
        targets = prune(candidates, src.store.path, repo.store.path, ui)
        do_relink(src.store.path, repo.store.path, targets, ui)


def collect(src, ui):
    seplen = len(os.path.sep)
    candidates = []
    live = len(src[b'tip'].manifest())
    # Your average repository has some files which were deleted before
    # the tip revision. We account for that by assuming that there are
    # 3 tracked files for every 2 live files as of the tip version of
    # the repository.
    #
    # mozilla-central as of 2010-06-10 had a ratio of just over 7:5.
    total = live * 3 // 2
    src = src.store.path
    progress = ui.makeprogress(_(b'collecting'), unit=_(b'files'), total=total)
    pos = 0
    ui.status(
        _(b"tip has %d files, estimated total number of files: %d\n")
        % (live, total)
    )
    for dirpath, dirnames, filenames in os.walk(src):
        dirnames.sort()
        relpath = dirpath[len(src) + seplen :]
        for filename in sorted(filenames):
            if filename[-2:] not in (b'.d', b'.i'):
                continue
            st = os.stat(os.path.join(dirpath, filename))
            if not stat.S_ISREG(st.st_mode):
                continue
            pos += 1
            candidates.append((os.path.join(relpath, filename), st))
            progress.update(pos, item=filename)

    progress.complete()
    ui.status(_(b'collected %d candidate storage files\n') % len(candidates))
    return candidates


def prune(candidates, src, dst, ui):
    def linkfilter(src, dst, st):
        try:
            ts = os.stat(dst)
        except OSError:
            # Destination doesn't have this file?
            return False
        if util.samefile(src, dst):
            return False
        if not util.samedevice(src, dst):
            # No point in continuing
            raise error.Abort(
                _(b'source and destination are on different devices')
            )
        if st.st_size != ts.st_size:
            return False
        return st

    targets = []
    progress = ui.makeprogress(
        _(b'pruning'), unit=_(b'files'), total=len(candidates)
    )
    pos = 0
    for fn, st in candidates:
        pos += 1
        srcpath = os.path.join(src, fn)
        tgt = os.path.join(dst, fn)
        ts = linkfilter(srcpath, tgt, st)
        if not ts:
            ui.debug(b'not linkable: %s\n' % fn)
            continue
        targets.append((fn, ts.st_size))
        progress.update(pos, item=fn)

    progress.complete()
    ui.status(
        _(b'pruned down to %d probably relinkable files\n') % len(targets)
    )
    return targets


def do_relink(src, dst, files, ui):
    def relinkfile(src, dst):
        bak = dst + b'.bak'
        os.rename(dst, bak)
        try:
            util.oslink(src, dst)
        except OSError:
            os.rename(bak, dst)
            raise
        os.remove(bak)

    CHUNKLEN = 65536
    relinked = 0
    savedbytes = 0

    progress = ui.makeprogress(
        _(b'relinking'), unit=_(b'files'), total=len(files)
    )
    pos = 0
    for f, sz in files:
        pos += 1
        source = os.path.join(src, f)
        tgt = os.path.join(dst, f)
        # Binary mode, so that read() works correctly, especially on Windows
        sfp = open(source, b'rb')
        dfp = open(tgt, b'rb')
        sin = sfp.read(CHUNKLEN)
        while sin:
            din = dfp.read(CHUNKLEN)
            if sin != din:
                break
            sin = sfp.read(CHUNKLEN)
        sfp.close()
        dfp.close()
        if sin:
            ui.debug(b'not linkable: %s\n' % f)
            continue
        try:
            relinkfile(source, tgt)
            progress.update(pos, item=f)
            relinked += 1
            savedbytes += sz
        except OSError as inst:
            ui.warn(b'%s: %s\n' % (tgt, stringutil.forcebytestr(inst)))

    progress.complete()

    ui.status(
        _(b'relinked %d files (%s reclaimed)\n')
        % (relinked, util.bytecount(savedbytes))
    )
                                                                                                                                                                                                                                                                                                                                                                                                       usr/lib/python3/dist-packages/hgext/remotefilelog/                                                  0000755 0000000 0000000 00000000000 14714551121 020723  5                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        usr/lib/python3/dist-packages/hgext/remotefilelog/__init__.py                                       0000644 0000000 0000000 00000122235 14355257011 023043  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # __init__.py - remotefilelog extension
#
# Copyright 2013 Facebook, Inc.
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.
"""remotefilelog causes Mercurial to lazilly fetch file contents (EXPERIMENTAL)

This extension is HIGHLY EXPERIMENTAL. There are NO BACKWARDS COMPATIBILITY
GUARANTEES. This means that repositories created with this extension may
only be usable with the exact version of this extension/Mercurial that was
used. The extension attempts to enforce this in order to prevent repository
corruption.

remotefilelog works by fetching file contents lazily and storing them
in a cache on the client rather than in revlogs. This allows enormous
histories to be transferred only partially, making them easier to
operate on.

Configs:

    ``packs.maxchainlen`` specifies the maximum delta chain length in pack files

    ``packs.maxpacksize`` specifies the maximum pack file size

    ``packs.maxpackfilecount`` specifies the maximum number of packs in the
      shared cache (trees only for now)

    ``remotefilelog.backgroundprefetch`` runs prefetch in background when True

    ``remotefilelog.bgprefetchrevs`` specifies revisions to fetch on commit and
      update, and on other commands that use them. Different from pullprefetch.

    ``remotefilelog.gcrepack`` does garbage collection during repack when True

    ``remotefilelog.nodettl`` specifies maximum TTL of a node in seconds before
      it is garbage collected

    ``remotefilelog.repackonhggc`` runs repack on hg gc when True

    ``remotefilelog.prefetchdays`` specifies the maximum age of a commit in
      days after which it is no longer prefetched.

    ``remotefilelog.prefetchdelay`` specifies delay between background
      prefetches in seconds after operations that change the working copy parent

    ``remotefilelog.data.gencountlimit`` constraints the minimum number of data
      pack files required to be considered part of a generation. In particular,
      minimum number of packs files > gencountlimit.

    ``remotefilelog.data.generations`` list for specifying the lower bound of
      each generation of the data pack files. For example, list ['100MB','1MB']
      or ['1MB', '100MB'] will lead to three generations: [0, 1MB), [
      1MB, 100MB) and [100MB, infinity).

    ``remotefilelog.data.maxrepackpacks`` the maximum number of pack files to
      include in an incremental data repack.

    ``remotefilelog.data.repackmaxpacksize`` the maximum size of a pack file for
      it to be considered for an incremental data repack.

    ``remotefilelog.data.repacksizelimit`` the maximum total size of pack files
      to include in an incremental data repack.

    ``remotefilelog.history.gencountlimit`` constraints the minimum number of
      history pack files required to be considered part of a generation. In
      particular, minimum number of packs files > gencountlimit.

    ``remotefilelog.history.generations`` list for specifying the lower bound of
      each generation of the history pack files. For example, list [
      '100MB', '1MB'] or ['1MB', '100MB'] will lead to three generations: [
      0, 1MB), [1MB, 100MB) and [100MB, infinity).

    ``remotefilelog.history.maxrepackpacks`` the maximum number of pack files to
      include in an incremental history repack.

    ``remotefilelog.history.repackmaxpacksize`` the maximum size of a pack file
      for it to be considered for an incremental history repack.

    ``remotefilelog.history.repacksizelimit`` the maximum total size of pack
      files to include in an incremental history repack.

    ``remotefilelog.backgroundrepack`` automatically consolidate packs in the
      background

    ``remotefilelog.cachepath`` path to cache

    ``remotefilelog.cachegroup`` if set, make cache directory sgid to this
      group

    ``remotefilelog.cacheprocess`` binary to invoke for fetching file data

    ``remotefilelog.debug`` turn on remotefilelog-specific debug output

    ``remotefilelog.excludepattern`` pattern of files to exclude from pulls

    ``remotefilelog.includepattern`` pattern of files to include in pulls

    ``remotefilelog.fetchwarning``: message to print when too many
      single-file fetches occur

    ``remotefilelog.getfilesstep`` number of files to request in a single RPC

    ``remotefilelog.getfilestype`` if set to 'threaded' use threads to fetch
      files, otherwise use optimistic fetching

    ``remotefilelog.pullprefetch`` revset for selecting files that should be
      eagerly downloaded rather than lazily

    ``remotefilelog.reponame`` name of the repo. If set, used to partition
      data from other repos in a shared store.

    ``remotefilelog.server`` if true, enable server-side functionality

    ``remotefilelog.servercachepath`` path for caching blobs on the server

    ``remotefilelog.serverexpiration`` number of days to keep cached server
      blobs

    ``remotefilelog.validatecache`` if set, check cache entries for corruption
      before returning blobs

    ``remotefilelog.validatecachelog`` if set, check cache entries for
      corruption before returning metadata

"""

import os
import time
import traceback

from mercurial.node import (
    hex,
    wdirrev,
)
from mercurial.i18n import _
from mercurial.pycompat import open
from mercurial import (
    changegroup,
    changelog,
    commands,
    configitems,
    context,
    copies,
    debugcommands as hgdebugcommands,
    dispatch,
    error,
    exchange,
    extensions,
    hg,
    localrepo,
    match as matchmod,
    merge,
    mergestate as mergestatemod,
    patch,
    pycompat,
    registrar,
    repair,
    repoview,
    revset,
    scmutil,
    smartset,
    streamclone,
    util,
)
from . import (
    constants,
    debugcommands,
    fileserverclient,
    remotefilectx,
    remotefilelog,
    remotefilelogserver,
    repack as repackmod,
    shallowbundle,
    shallowrepo,
    shallowstore,
    shallowutil,
    shallowverifier,
)

# ensures debug commands are registered
hgdebugcommands.command

cmdtable = {}
command = registrar.command(cmdtable)

configtable = {}
configitem = registrar.configitem(configtable)

configitem(b'remotefilelog', b'debug', default=False)

configitem(b'remotefilelog', b'reponame', default=b'')
configitem(b'remotefilelog', b'cachepath', default=None)
configitem(b'remotefilelog', b'cachegroup', default=None)
configitem(b'remotefilelog', b'cacheprocess', default=None)
configitem(b'remotefilelog', b'cacheprocess.includepath', default=None)
configitem(b"remotefilelog", b"cachelimit", default=b"1000 GB")

configitem(
    b'remotefilelog',
    b'fallbackpath',
    default=configitems.dynamicdefault,
    alias=[(b'remotefilelog', b'fallbackrepo')],
)

configitem(b'remotefilelog', b'validatecachelog', default=None)
configitem(b'remotefilelog', b'validatecache', default=b'on')
configitem(b'remotefilelog', b'server', default=None)
configitem(b'remotefilelog', b'servercachepath', default=None)
configitem(b"remotefilelog", b"serverexpiration", default=30)
configitem(b'remotefilelog', b'backgroundrepack', default=False)
configitem(b'remotefilelog', b'bgprefetchrevs', default=None)
configitem(b'remotefilelog', b'pullprefetch', default=None)
configitem(b'remotefilelog', b'backgroundprefetch', default=False)
configitem(b'remotefilelog', b'prefetchdelay', default=120)
configitem(b'remotefilelog', b'prefetchdays', default=14)
# Other values include 'local' or 'none'. Any unrecognized value is 'all'.
configitem(b'remotefilelog', b'strip.includefiles', default='all')

configitem(b'remotefilelog', b'getfilesstep', default=10000)
configitem(b'remotefilelog', b'getfilestype', default=b'optimistic')
configitem(b'remotefilelog', b'batchsize', configitems.dynamicdefault)
configitem(b'remotefilelog', b'fetchwarning', default=b'')

configitem(b'remotefilelog', b'includepattern', default=None)
configitem(b'remotefilelog', b'excludepattern', default=None)

configitem(b'remotefilelog', b'gcrepack', default=False)
configitem(b'remotefilelog', b'repackonhggc', default=False)
configitem(b'repack', b'chainorphansbysize', default=True, experimental=True)

configitem(b'packs', b'maxpacksize', default=0)
configitem(b'packs', b'maxchainlen', default=1000)

configitem(b'devel', b'remotefilelog.bg-wait', default=False)

#  default TTL limit is 30 days
_defaultlimit = 60 * 60 * 24 * 30
configitem(b'remotefilelog', b'nodettl', default=_defaultlimit)

configitem(b'remotefilelog', b'data.gencountlimit', default=2),
configitem(
    b'remotefilelog', b'data.generations', default=[b'1GB', b'100MB', b'1MB']
)
configitem(b'remotefilelog', b'data.maxrepackpacks', default=50)
configitem(b'remotefilelog', b'data.repackmaxpacksize', default=b'4GB')
configitem(b'remotefilelog', b'data.repacksizelimit', default=b'100MB')

configitem(b'remotefilelog', b'history.gencountlimit', default=2),
configitem(b'remotefilelog', b'history.generations', default=[b'100MB'])
configitem(b'remotefilelog', b'history.maxrepackpacks', default=50)
configitem(b'remotefilelog', b'history.repackmaxpacksize', default=b'400MB')
configitem(b'remotefilelog', b'history.repacksizelimit', default=b'100MB')

# Note for extension authors: ONLY specify testedwith = 'ships-with-hg-core' for
# extensions which SHIP WITH MERCURIAL. Non-mainline extensions should
# be specifying the version(s) of Mercurial they are tested with, or
# leave the attribute unspecified.
testedwith = b'ships-with-hg-core'

repoclass = localrepo.localrepository
repoclass._basesupported.add(constants.SHALLOWREPO_REQUIREMENT)

isenabled = shallowutil.isenabled


def uisetup(ui):
    """Wraps user facing Mercurial commands to swap them out with shallow
    versions.
    """
    hg.wirepeersetupfuncs.append(fileserverclient.peersetup)

    entry = extensions.wrapcommand(commands.table, b'clone', cloneshallow)
    entry[1].append(
        (
            b'',
            b'shallow',
            None,
            _(b"create a shallow clone which uses remote file history"),
        )
    )

    extensions.wrapcommand(
        commands.table, b'debugindex', debugcommands.debugindex
    )
    extensions.wrapcommand(
        commands.table, b'debugindexdot', debugcommands.debugindexdot
    )
    extensions.wrapcommand(commands.table, b'log', log)
    extensions.wrapcommand(commands.table, b'pull', pull)

    # Prevent 'hg manifest --all'
    def _manifest(orig, ui, repo, *args, **opts):
        if isenabled(repo) and opts.get('all'):
            raise error.Abort(_(b"--all is not supported in a shallow repo"))

        return orig(ui, repo, *args, **opts)

    extensions.wrapcommand(commands.table, b"manifest", _manifest)

    # Wrap remotefilelog with lfs code
    def _lfsloaded(loaded=False):
        lfsmod = None
        try:
            lfsmod = extensions.find(b'lfs')
        except KeyError:
            pass
        if lfsmod:
            lfsmod.wrapfilelog(remotefilelog.remotefilelog)
            fileserverclient._lfsmod = lfsmod

    extensions.afterloaded(b'lfs', _lfsloaded)

    # debugdata needs remotefilelog.len to work
    extensions.wrapcommand(commands.table, b'debugdata', debugdatashallow)

    changegroup.cgpacker = shallowbundle.shallowcg1packer

    extensions.wrapfunction(
        changegroup, b'_addchangegroupfiles', shallowbundle.addchangegroupfiles
    )
    extensions.wrapfunction(
        changegroup, b'makechangegroup', shallowbundle.makechangegroup
    )
    extensions.wrapfunction(localrepo, b'makestore', storewrapper)
    extensions.wrapfunction(exchange, b'pull', exchangepull)
    extensions.wrapfunction(merge, b'applyupdates', applyupdates)
    extensions.wrapfunction(merge, b'_checkunknownfiles', checkunknownfiles)
    extensions.wrapfunction(context.workingctx, b'_checklookup', checklookup)
    extensions.wrapfunction(scmutil, b'_findrenames', findrenames)
    extensions.wrapfunction(
        copies, b'_computeforwardmissing', computeforwardmissing
    )
    extensions.wrapfunction(dispatch, b'runcommand', runcommand)
    extensions.wrapfunction(repair, b'_collectbrokencsets', _collectbrokencsets)
    extensions.wrapfunction(context.changectx, b'filectx', filectx)
    extensions.wrapfunction(context.workingctx, b'filectx', workingfilectx)
    extensions.wrapfunction(patch, b'trydiff', trydiff)
    extensions.wrapfunction(hg, b'verify', _verify)
    scmutil.fileprefetchhooks.add(b'remotefilelog', _fileprefetchhook)

    # disappointing hacks below
    extensions.wrapfunction(scmutil, b'getrenamedfn', getrenamedfn)
    extensions.wrapfunction(revset, b'filelog', filelogrevset)
    revset.symbols[b'filelog'] = revset.filelog


def cloneshallow(orig, ui, repo, *args, **opts):
    if opts.get('shallow'):
        repos = []

        def pull_shallow(orig, self, *args, **kwargs):
            if not isenabled(self):
                repos.append(self.unfiltered())
                # set up the client hooks so the post-clone update works
                setupclient(self.ui, self.unfiltered())

                # setupclient fixed the class on the repo itself
                # but we also need to fix it on the repoview
                if isinstance(self, repoview.repoview):
                    self.__class__.__bases__ = (
                        self.__class__.__bases__[0],
                        self.unfiltered().__class__,
                    )
                self.requirements.add(constants.SHALLOWREPO_REQUIREMENT)
                with self.lock():
                    # acquire store lock before writing requirements as some
                    # requirements might be written to .hg/store/requires
                    scmutil.writereporequirements(self)

                # Since setupclient hadn't been called, exchange.pull was not
                # wrapped. So we need to manually invoke our version of it.
                return exchangepull(orig, self, *args, **kwargs)
            else:
                return orig(self, *args, **kwargs)

        extensions.wrapfunction(exchange, b'pull', pull_shallow)

        # Wrap the stream logic to add requirements and to pass include/exclude
        # patterns around.
        def setup_streamout(repo, remote):
            # Replace remote.stream_out with a version that sends file
            # patterns.
            def stream_out_shallow(orig):
                caps = remote.capabilities()
                if constants.NETWORK_CAP_LEGACY_SSH_GETFILES in caps:
                    opts = {}
                    if repo.includepattern:
                        opts['includepattern'] = b'\0'.join(repo.includepattern)
                    if repo.excludepattern:
                        opts['excludepattern'] = b'\0'.join(repo.excludepattern)
                    return remote._callstream(b'stream_out_shallow', **opts)
                else:
                    return orig()

            extensions.wrapfunction(remote, b'stream_out', stream_out_shallow)

        def stream_wrap(orig, op):
            setup_streamout(op.repo, op.remote)
            return orig(op)

        extensions.wrapfunction(
            streamclone, b'maybeperformlegacystreamclone', stream_wrap
        )

        def canperformstreamclone(orig, pullop, bundle2=False):
            # remotefilelog is currently incompatible with the
            # bundle2 flavor of streamclones, so force us to use
            # v1 instead.
            if b'v2' in pullop.remotebundle2caps.get(b'stream', []):
                pullop.remotebundle2caps[b'stream'] = [
                    c for c in pullop.remotebundle2caps[b'stream'] if c != b'v2'
                ]
            if bundle2:
                return False, None
