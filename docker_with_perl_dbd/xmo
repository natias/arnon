            chunk = self.read(32768)
            while chunk:
                chunk = self.read(32668)

        if not 0 <= newpos <= self._chunkindex[-1][0]:
            raise ValueError(b'Offset out of range')

        if self._pos != newpos:
            chunk, internaloffset = self._findchunk(newpos)
            self._payloadstream = util.chunkbuffer(self._payloadchunks(chunk))
            adjust = self.read(internaloffset)
            if len(adjust) != internaloffset:
                raise error.Abort(_(b'Seek failed\n'))
            self._pos = newpos

    def _seekfp(self, offset, whence=0):
        """move the underlying file pointer

        This method is meant for internal usage by the bundle2 protocol only.
        They directly manipulate the low level stream including bundle2 level
        instruction.

        Do not use it to implement higher-level logic or methods."""
        if self._seekable:
            return self._fp.seek(offset, whence)
        else:
            raise NotImplementedError(_(b'File pointer is not seekable'))

    def _tellfp(self):
        """return the file offset, or None if file is not seekable

        This method is meant for internal usage by the bundle2 protocol only.
        They directly manipulate the low level stream including bundle2 level
        instruction.

        Do not use it to implement higher-level logic or methods."""
        if self._seekable:
            try:
                return self._fp.tell()
            except IOError as e:
                if e.errno == errno.ESPIPE:
                    self._seekable = False
                else:
                    raise
        return None


# These are only the static capabilities.
# Check the 'getrepocaps' function for the rest.
capabilities = {
    b'HG20': (),
    b'bookmarks': (),
    b'error': (b'abort', b'unsupportedcontent', b'pushraced', b'pushkey'),
    b'listkeys': (),
    b'pushkey': (),
    b'digests': tuple(sorted(util.DIGESTS.keys())),
    b'remote-changegroup': (b'http', b'https'),
    b'hgtagsfnodes': (),
    b'phases': (b'heads',),
    b'stream': (b'v2',),
}


def getrepocaps(repo, allowpushback=False, role=None):
    """return the bundle2 capabilities for a given repo

    Exists to allow extensions (like evolution) to mutate the capabilities.

    The returned value is used for servers advertising their capabilities as
    well as clients advertising their capabilities to servers as part of
    bundle2 requests. The ``role`` argument specifies which is which.
    """
    if role not in (b'client', b'server'):
        raise error.ProgrammingError(b'role argument must be client or server')

    caps = capabilities.copy()
    caps[b'changegroup'] = tuple(
        sorted(changegroup.supportedincomingversions(repo))
    )
    if obsolete.isenabled(repo, obsolete.exchangeopt):
        supportedformat = tuple(b'V%i' % v for v in obsolete.formats)
        caps[b'obsmarkers'] = supportedformat
    if allowpushback:
        caps[b'pushback'] = ()
    cpmode = repo.ui.config(b'server', b'concurrent-push-mode')
    if cpmode == b'check-related':
        caps[b'checkheads'] = (b'related',)
    if b'phases' in repo.ui.configlist(b'devel', b'legacy.exchange'):
        caps.pop(b'phases')

    # Don't advertise stream clone support in server mode if not configured.
    if role == b'server':
        streamsupported = repo.ui.configbool(
            b'server', b'uncompressed', untrusted=True
        )
        featuresupported = repo.ui.configbool(b'server', b'bundle2.stream')

        if not streamsupported or not featuresupported:
            caps.pop(b'stream')
    # Else always advertise support on client, because payload support
    # should always be advertised.

    # b'rev-branch-cache is no longer advertised, but still supported
    # for legacy clients.

    return caps


def bundle2caps(remote):
    """return the bundle capabilities of a peer as dict"""
    raw = remote.capable(b'bundle2')
    if not raw and raw != b'':
        return {}
    capsblob = urlreq.unquote(remote.capable(b'bundle2'))
    return decodecaps(capsblob)


def obsmarkersversion(caps):
    """extract the list of supported obsmarkers versions from a bundle2caps dict"""
    obscaps = caps.get(b'obsmarkers', ())
    return [int(c[1:]) for c in obscaps if c.startswith(b'V')]


def writenewbundle(
    ui,
    repo,
    source,
    filename,
    bundletype,
    outgoing,
    opts,
    vfs=None,
    compression=None,
    compopts=None,
):
    if bundletype.startswith(b'HG10'):
        cg = changegroup.makechangegroup(repo, outgoing, b'01', source)
        return writebundle(
            ui,
            cg,
            filename,
            bundletype,
            vfs=vfs,
            compression=compression,
            compopts=compopts,
        )
    elif not bundletype.startswith(b'HG20'):
        raise error.ProgrammingError(b'unknown bundle type: %s' % bundletype)

    caps = {}
    if opts.get(b'obsolescence', False):
        caps[b'obsmarkers'] = (b'V1',)
    bundle = bundle20(ui, caps)
    bundle.setcompression(compression, compopts)
    _addpartsfromopts(ui, repo, bundle, source, outgoing, opts)
    chunkiter = bundle.getchunks()

    return changegroup.writechunks(ui, chunkiter, filename, vfs=vfs)


def _addpartsfromopts(ui, repo, bundler, source, outgoing, opts):
    # We should eventually reconcile this logic with the one behind
    # 'exchange.getbundle2partsgenerator'.
    #
    # The type of input from 'getbundle' and 'writenewbundle' are a bit
    # different right now. So we keep them separated for now for the sake of
    # simplicity.

    # we might not always want a changegroup in such bundle, for example in
    # stream bundles
    if opts.get(b'changegroup', True):
        cgversion = opts.get(b'cg.version')
        if cgversion is None:
            cgversion = changegroup.safeversion(repo)
        cg = changegroup.makechangegroup(repo, outgoing, cgversion, source)
        part = bundler.newpart(b'changegroup', data=cg.getchunks())
        part.addparam(b'version', cg.version)
        if b'clcount' in cg.extras:
            part.addparam(
                b'nbchanges', b'%d' % cg.extras[b'clcount'], mandatory=False
            )
        if opts.get(b'phases') and repo.revs(
            b'%ln and secret()', outgoing.ancestorsof
        ):
            part.addparam(
                b'targetphase', b'%d' % phases.secret, mandatory=False
            )
    if repository.REPO_FEATURE_SIDE_DATA in repo.features:
        part.addparam(b'exp-sidedata', b'1')

    if opts.get(b'streamv2', False):
        addpartbundlestream2(bundler, repo, stream=True)

    if opts.get(b'tagsfnodescache', True):
        addparttagsfnodescache(repo, bundler, outgoing)

    if opts.get(b'revbranchcache', True):
        addpartrevbranchcache(repo, bundler, outgoing)

    if opts.get(b'obsolescence', False):
        obsmarkers = repo.obsstore.relevantmarkers(outgoing.missing)
        buildobsmarkerspart(
            bundler,
            obsmarkers,
            mandatory=opts.get(b'obsolescence-mandatory', True),
        )

    if opts.get(b'phases', False):
        headsbyphase = phases.subsetphaseheads(repo, outgoing.missing)
        phasedata = phases.binaryencode(headsbyphase)
        bundler.newpart(b'phase-heads', data=phasedata)


def addparttagsfnodescache(repo, bundler, outgoing):
    # we include the tags fnode cache for the bundle changeset
    # (as an optional parts)
    cache = tags.hgtagsfnodescache(repo.unfiltered())
    chunks = []

    # .hgtags fnodes are only relevant for head changesets. While we could
    # transfer values for all known nodes, there will likely be little to
    # no benefit.
    #
    # We don't bother using a generator to produce output data because
    # a) we only have 40 bytes per head and even esoteric numbers of heads
    # consume little memory (1M heads is 40MB) b) we don't want to send the
    # part if we don't have entries and knowing if we have entries requires
    # cache lookups.
    for node in outgoing.ancestorsof:
        # Don't compute missing, as this may slow down serving.
        fnode = cache.getfnode(node, computemissing=False)
        if fnode:
            chunks.extend([node, fnode])

    if chunks:
        bundler.newpart(b'hgtagsfnodes', data=b''.join(chunks))


def addpartrevbranchcache(repo, bundler, outgoing):
    # we include the rev branch cache for the bundle changeset
    # (as an optional parts)
    cache = repo.revbranchcache()
    cl = repo.unfiltered().changelog
    branchesdata = collections.defaultdict(lambda: (set(), set()))
    for node in outgoing.missing:
        branch, close = cache.branchinfo(cl.rev(node))
        branchesdata[branch][close].add(node)

    def generate():
        for branch, (nodes, closed) in sorted(branchesdata.items()):
            utf8branch = encoding.fromlocal(branch)
            yield rbcstruct.pack(len(utf8branch), len(nodes), len(closed))
            yield utf8branch
            for n in sorted(nodes):
                yield n
            for n in sorted(closed):
                yield n

    bundler.newpart(b'cache:rev-branch-cache', data=generate(), mandatory=False)


def _formatrequirementsspec(requirements):
    requirements = [req for req in requirements if req != b"shared"]
    return urlreq.quote(b','.join(sorted(requirements)))


def _formatrequirementsparams(requirements):
    requirements = _formatrequirementsspec(requirements)
    params = b"%s%s" % (urlreq.quote(b"requirements="), requirements)
    return params


def format_remote_wanted_sidedata(repo):
    """Formats a repo's wanted sidedata categories into a bytestring for
    capabilities exchange."""
    wanted = b""
    if repo._wanted_sidedata:
        wanted = b','.join(
            pycompat.bytestr(c) for c in sorted(repo._wanted_sidedata)
        )
    return wanted


def read_remote_wanted_sidedata(remote):
    sidedata_categories = remote.capable(b'exp-wanted-sidedata')
    return read_wanted_sidedata(sidedata_categories)


def read_wanted_sidedata(formatted):
    if formatted:
        return set(formatted.split(b','))
    return set()


def addpartbundlestream2(bundler, repo, **kwargs):
    if not kwargs.get('stream', False):
        return

    if not streamclone.allowservergeneration(repo):
        raise error.Abort(
            _(
                b'stream data requested but server does not allow '
                b'this feature'
            ),
            hint=_(
                b'well-behaved clients should not be '
                b'requesting stream data from servers not '
                b'advertising it; the client may be buggy'
            ),
        )

    # Stream clones don't compress well. And compression undermines a
    # goal of stream clones, which is to be fast. Communicate the desire
    # to avoid compression to consumers of the bundle.
    bundler.prefercompressed = False

    # get the includes and excludes
    includepats = kwargs.get('includepats')
    excludepats = kwargs.get('excludepats')

    narrowstream = repo.ui.configbool(
        b'experimental', b'server.stream-narrow-clones'
    )

    if (includepats or excludepats) and not narrowstream:
        raise error.Abort(_(b'server does not support narrow stream clones'))

    includeobsmarkers = False
    if repo.obsstore:
        remoteversions = obsmarkersversion(bundler.capabilities)
        if not remoteversions:
            raise error.Abort(
                _(
                    b'server has obsolescence markers, but client '
                    b'cannot receive them via stream clone'
                )
            )
        elif repo.obsstore._version in remoteversions:
            includeobsmarkers = True

    filecount, bytecount, it = streamclone.generatev2(
        repo, includepats, excludepats, includeobsmarkers
    )
    requirements = streamclone.streamed_requirements(repo)
    requirements = _formatrequirementsspec(requirements)
    part = bundler.newpart(b'stream2', data=it)
    part.addparam(b'bytecount', b'%d' % bytecount, mandatory=True)
    part.addparam(b'filecount', b'%d' % filecount, mandatory=True)
    part.addparam(b'requirements', requirements, mandatory=True)


def buildobsmarkerspart(bundler, markers, mandatory=True):
    """add an obsmarker part to the bundler with <markers>

    No part is created if markers is empty.
    Raises ValueError if the bundler doesn't support any known obsmarker format.
    """
    if not markers:
        return None

    remoteversions = obsmarkersversion(bundler.capabilities)
    version = obsolete.commonversion(remoteversions)
    if version is None:
        raise ValueError(b'bundler does not support common obsmarker format')
    stream = obsolete.encodemarkers(markers, True, version=version)
    return bundler.newpart(b'obsmarkers', data=stream, mandatory=mandatory)


def writebundle(
    ui, cg, filename, bundletype, vfs=None, compression=None, compopts=None
):
    """Write a bundle file and return its filename.

    Existing files will not be overwritten.
    If no filename is specified, a temporary file is created.
    bz2 compression can be turned off.
    The bundle file will be deleted in case of errors.
    """

    if bundletype == b"HG20":
        bundle = bundle20(ui)
        bundle.setcompression(compression, compopts)
        part = bundle.newpart(b'changegroup', data=cg.getchunks())
        part.addparam(b'version', cg.version)
        if b'clcount' in cg.extras:
            part.addparam(
                b'nbchanges', b'%d' % cg.extras[b'clcount'], mandatory=False
            )
        chunkiter = bundle.getchunks()
    else:
        # compression argument is only for the bundle2 case
        assert compression is None
        if cg.version != b'01':
            raise error.Abort(
                _(b'old bundle types only supports v1 changegroups')
            )
        header, comp = bundletypes[bundletype]
        if comp not in util.compengines.supportedbundletypes:
            raise error.Abort(_(b'unknown stream compression type: %s') % comp)
        compengine = util.compengines.forbundletype(comp)

        def chunkiter():
            yield header
            for chunk in compengine.compressstream(cg.getchunks(), compopts):
                yield chunk

        chunkiter = chunkiter()

    # parse the changegroup data, otherwise we will block
    # in case of sshrepo because we don't know the end of the stream
    return changegroup.writechunks(ui, chunkiter, filename, vfs=vfs)


def combinechangegroupresults(op):
    """logic to combine 0 or more addchangegroup results into one"""
    results = [r.get(b'return', 0) for r in op.records[b'changegroup']]
    changedheads = 0
    result = 1
    for ret in results:
        # If any changegroup result is 0, return 0
        if ret == 0:
            result = 0
            break
        if ret < -1:
            changedheads += ret + 1
        elif ret > 1:
            changedheads += ret - 1
    if changedheads > 0:
        result = 1 + changedheads
    elif changedheads < 0:
        result = -1 + changedheads
    return result


@parthandler(
    b'changegroup',
    (
        b'version',
        b'nbchanges',
        b'exp-sidedata',
        b'exp-wanted-sidedata',
        b'treemanifest',
        b'targetphase',
    ),
)
def handlechangegroup(op, inpart):
    """apply a changegroup part on the repo"""
    from . import localrepo

    tr = op.gettransaction()
    unpackerversion = inpart.params.get(b'version', b'01')
    # We should raise an appropriate exception here
    cg = changegroup.getunbundler(unpackerversion, inpart, None)
    # the source and url passed here are overwritten by the one contained in
    # the transaction.hookargs argument. So 'bundle2' is a placeholder
    nbchangesets = None
    if b'nbchanges' in inpart.params:
        nbchangesets = int(inpart.params.get(b'nbchanges'))
    if b'treemanifest' in inpart.params and not scmutil.istreemanifest(op.repo):
        if len(op.repo.changelog) != 0:
            raise error.Abort(
                _(
                    b"bundle contains tree manifests, but local repo is "
                    b"non-empty and does not use tree manifests"
                )
            )
        op.repo.requirements.add(requirements.TREEMANIFEST_REQUIREMENT)
        op.repo.svfs.options = localrepo.resolvestorevfsoptions(
            op.repo.ui, op.repo.requirements, op.repo.features
        )
        scmutil.writereporequirements(op.repo)

    extrakwargs = {}
    targetphase = inpart.params.get(b'targetphase')
    if targetphase is not None:
        extrakwargs['targetphase'] = int(targetphase)

    remote_sidedata = inpart.params.get(b'exp-wanted-sidedata')
    extrakwargs['sidedata_categories'] = read_wanted_sidedata(remote_sidedata)

    ret = _processchangegroup(
        op,
        cg,
        tr,
        op.source,
        b'bundle2',
        expectedtotal=nbchangesets,
        **extrakwargs
    )
    if op.reply is not None:
        # This is definitely not the final form of this
        # return. But one need to start somewhere.
        part = op.reply.newpart(b'reply:changegroup', mandatory=False)
        part.addparam(
            b'in-reply-to', pycompat.bytestr(inpart.id), mandatory=False
        )
        part.addparam(b'return', b'%i' % ret, mandatory=False)
    assert not inpart.read()


_remotechangegroupparams = tuple(
    [b'url', b'size', b'digests']
    + [b'digest:%s' % k for k in util.DIGESTS.keys()]
)


@parthandler(b'remote-changegroup', _remotechangegroupparams)
def handleremotechangegroup(op, inpart):
    """apply a bundle10 on the repo, given an url and validation information

    All the information about the remote bundle to import are given as
    parameters. The parameters include:
      - url: the url to the bundle10.
      - size: the bundle10 file size. It is used to validate what was
        retrieved by the client matches the server knowledge about the bundle.
      - digests: a space separated list of the digest types provided as
        parameters.
      - digest:<digest-type>: the hexadecimal representation of the digest with
        that name. Like the size, it is used to validate what was retrieved by
        the client matches what the server knows about the bundle.

    When multiple digest types are given, all of them are checked.
    """
    try:
        raw_url = inpart.params[b'url']
    except KeyError:
        raise error.Abort(_(b'remote-changegroup: missing "%s" param') % b'url')
    parsed_url = urlutil.url(raw_url)
    if parsed_url.scheme not in capabilities[b'remote-changegroup']:
        raise error.Abort(
            _(b'remote-changegroup does not support %s urls')
            % parsed_url.scheme
        )

    try:
        size = int(inpart.params[b'size'])
    except ValueError:
        raise error.Abort(
            _(b'remote-changegroup: invalid value for param "%s"') % b'size'
        )
    except KeyError:
        raise error.Abort(
            _(b'remote-changegroup: missing "%s" param') % b'size'
        )

    digests = {}
    for typ in inpart.params.get(b'digests', b'').split():
        param = b'digest:%s' % typ
        try:
            value = inpart.params[param]
        except KeyError:
            raise error.Abort(
                _(b'remote-changegroup: missing "%s" param') % param
            )
        digests[typ] = value

    real_part = util.digestchecker(url.open(op.ui, raw_url), size, digests)

    tr = op.gettransaction()
    from . import exchange

    cg = exchange.readbundle(op.repo.ui, real_part, raw_url)
    if not isinstance(cg, changegroup.cg1unpacker):
        raise error.Abort(
            _(b'%s: not a bundle version 1.0') % urlutil.hidepassword(raw_url)
        )
    ret = _processchangegroup(op, cg, tr, op.source, b'bundle2')
    if op.reply is not None:
        # This is definitely not the final form of this
        # return. But one need to start somewhere.
        part = op.reply.newpart(b'reply:changegroup')
        part.addparam(
            b'in-reply-to', pycompat.bytestr(inpart.id), mandatory=False
        )
        part.addparam(b'return', b'%i' % ret, mandatory=False)
    try:
        real_part.validate()
    except error.Abort as e:
        raise error.Abort(
            _(b'bundle at %s is corrupted:\n%s')
            % (urlutil.hidepassword(raw_url), e.message)
        )
    assert not inpart.read()


@parthandler(b'reply:changegroup', (b'return', b'in-reply-to'))
def handlereplychangegroup(op, inpart):
    ret = int(inpart.params[b'return'])
    replyto = int(inpart.params[b'in-reply-to'])
    op.records.add(b'changegroup', {b'return': ret}, replyto)


@parthandler(b'check:bookmarks')
def handlecheckbookmarks(op, inpart):
    """check location of bookmarks

    This part is to be used to detect push race regarding bookmark, it
    contains binary encoded (bookmark, node) tuple. If the local state does
    not marks the one in the part, a PushRaced exception is raised
    """
    bookdata = bookmarks.binarydecode(op.repo, inpart)

    msgstandard = (
        b'remote repository changed while pushing - please try again '
        b'(bookmark "%s" move from %s to %s)'
    )
    msgmissing = (
        b'remote repository changed while pushing - please try again '
        b'(bookmark "%s" is missing, expected %s)'
    )
    msgexist = (
        b'remote repository changed while pushing - please try again '
        b'(bookmark "%s" set on %s, expected missing)'
    )
    for book, node in bookdata:
        currentnode = op.repo._bookmarks.get(book)
        if currentnode != node:
            if node is None:
                finalmsg = msgexist % (book, short(currentnode))
            elif currentnode is None:
                finalmsg = msgmissing % (book, short(node))
            else:
                finalmsg = msgstandard % (
                    book,
                    short(node),
                    short(currentnode),
                )
            raise error.PushRaced(finalmsg)


@parthandler(b'check:heads')
def handlecheckheads(op, inpart):
    """check that head of the repo did not change

    This is used to detect a push race when using unbundle.
    This replaces the "heads" argument of unbundle."""
    h = inpart.read(20)
    heads = []
    while len(h) == 20:
        heads.append(h)
        h = inpart.read(20)
    assert not h
    # Trigger a transaction so that we are guaranteed to have the lock now.
    if op.ui.configbool(b'experimental', b'bundle2lazylocking'):
        op.gettransaction()
    if sorted(heads) != sorted(op.repo.heads()):
        raise error.PushRaced(
            b'remote repository changed while pushing - please try again'
        )


@parthandler(b'check:updated-heads')
def handlecheckupdatedheads(op, inpart):
    """check for race on the heads touched by a push

    This is similar to 'check:heads' but focus on the heads actually updated
    during the push. If other activities happen on unrelated heads, it is
    ignored.

    This allow server with high traffic to avoid push contention as long as
    unrelated parts of the graph are involved."""
    h = inpart.read(20)
    heads = []
    while len(h) == 20:
        heads.append(h)
        h = inpart.read(20)
    assert not h
    # trigger a transaction so that we are guaranteed to have the lock now.
    if op.ui.configbool(b'experimental', b'bundle2lazylocking'):
        op.gettransaction()

    currentheads = set()
    for ls in op.repo.branchmap().iterheads():
        currentheads.update(ls)

    for h in heads:
        if h not in currentheads:
            raise error.PushRaced(
                b'remote repository changed while pushing - '
                b'please try again'
            )


@parthandler(b'check:phases')
def handlecheckphases(op, inpart):
    """check that phase boundaries of the repository did not change

    This is used to detect a push race.
    """
    phasetonodes = phases.binarydecode(inpart)
    unfi = op.repo.unfiltered()
    cl = unfi.changelog
    phasecache = unfi._phasecache
    msg = (
        b'remote repository changed while pushing - please try again '
        b'(%s is %s expected %s)'
    )
    for expectedphase, nodes in phasetonodes.items():
        for n in nodes:
            actualphase = phasecache.phase(unfi, cl.rev(n))
            if actualphase != expectedphase:
                finalmsg = msg % (
                    short(n),
                    phases.phasenames[actualphase],
                    phases.phasenames[expectedphase],
                )
                raise error.PushRaced(finalmsg)


@parthandler(b'output')
def handleoutput(op, inpart):
    """forward output captured on the server to the client"""
    for line in inpart.read().splitlines():
        op.ui.status(_(b'remote: %s\n') % line)


@parthandler(b'replycaps')
def handlereplycaps(op, inpart):
    """Notify that a reply bundle should be created

    The payload contains the capabilities information for the reply"""
    caps = decodecaps(inpart.read())
    if op.reply is None:
        op.reply = bundle20(op.ui, caps)


class AbortFromPart(error.Abort):
    """Sub-class of Abort that denotes an error from a bundle2 part."""


@parthandler(b'error:abort', (b'message', b'hint'))
def handleerrorabort(op, inpart):
    """Used to transmit abort error over the wire"""
    raise AbortFromPart(
        inpart.params[b'message'], hint=inpart.params.get(b'hint')
    )


@parthandler(
    b'error:pushkey',
    (b'namespace', b'key', b'new', b'old', b'ret', b'in-reply-to'),
)
def handleerrorpushkey(op, inpart):
    """Used to transmit failure of a mandatory pushkey over the wire"""
    kwargs = {}
    for name in (b'namespace', b'key', b'new', b'old', b'ret'):
        value = inpart.params.get(name)
        if value is not None:
            kwargs[name] = value
    raise error.PushkeyFailed(
        inpart.params[b'in-reply-to'], **pycompat.strkwargs(kwargs)
    )


@parthandler(b'error:unsupportedcontent', (b'parttype', b'params'))
def handleerrorunsupportedcontent(op, inpart):
    """Used to transmit unknown content error over the wire"""
    kwargs = {}
    parttype = inpart.params.get(b'parttype')
    if parttype is not None:
        kwargs[b'parttype'] = parttype
    params = inpart.params.get(b'params')
    if params is not None:
        kwargs[b'params'] = params.split(b'\0')

    raise error.BundleUnknownFeatureError(**pycompat.strkwargs(kwargs))


@parthandler(b'error:pushraced', (b'message',))
def handleerrorpushraced(op, inpart):
    """Used to transmit push race error over the wire"""
    raise error.ResponseError(_(b'push failed:'), inpart.params[b'message'])


@parthandler(b'listkeys', (b'namespace',))
def handlelistkeys(op, inpart):
    """retrieve pushkey namespace content stored in a bundle2"""
    namespace = inpart.params[b'namespace']
    r = pushkey.decodekeys(inpart.read())
    op.records.add(b'listkeys', (namespace, r))


@parthandler(b'pushkey', (b'namespace', b'key', b'old', b'new'))
def handlepushkey(op, inpart):
    """process a pushkey request"""
    dec = pushkey.decode
    namespace = dec(inpart.params[b'namespace'])
    key = dec(inpart.params[b'key'])
    old = dec(inpart.params[b'old'])
    new = dec(inpart.params[b'new'])
    # Grab the transaction to ensure that we have the lock before performing the
    # pushkey.
    if op.ui.configbool(b'experimental', b'bundle2lazylocking'):
        op.gettransaction()
    ret = op.repo.pushkey(namespace, key, old, new)
    record = {b'namespace': namespace, b'key': key, b'old': old, b'new': new}
    op.records.add(b'pushkey', record)
    if op.reply is not None:
        rpart = op.reply.newpart(b'reply:pushkey')
        rpart.addparam(
            b'in-reply-to', pycompat.bytestr(inpart.id), mandatory=False
        )
        rpart.addparam(b'return', b'%i' % ret, mandatory=False)
    if inpart.mandatory and not ret:
        kwargs = {}
        for key in (b'namespace', b'key', b'new', b'old', b'ret'):
            if key in inpart.params:
                kwargs[key] = inpart.params[key]
        raise error.PushkeyFailed(
            partid=b'%d' % inpart.id, **pycompat.strkwargs(kwargs)
        )


@parthandler(b'bookmarks')
def handlebookmark(op, inpart):
    """transmit bookmark information

    The part contains binary encoded bookmark information.

    The exact behavior of this part can be controlled by the 'bookmarks' mode
    on the bundle operation.

    When mode is 'apply' (the default) the bookmark information is applied as
    is to the unbundling repository. Make sure a 'check:bookmarks' part is
    issued earlier to check for push races in such update. This behavior is
    suitable for pushing.

    When mode is 'records', the information is recorded into the 'bookmarks'
    records of the bundle operation. This behavior is suitable for pulling.
    """
    changes = bookmarks.binarydecode(op.repo, inpart)

    pushkeycompat = op.repo.ui.configbool(
        b'server', b'bookmarks-pushkey-compat'
    )
    bookmarksmode = op.modes.get(b'bookmarks', b'apply')

    if bookmarksmode == b'apply':
        tr = op.gettransaction()
        bookstore = op.repo._bookmarks
        if pushkeycompat:
            allhooks = []
            for book, node in changes:
                hookargs = tr.hookargs.copy()
                hookargs[b'pushkeycompat'] = b'1'
                hookargs[b'namespace'] = b'bookmarks'
                hookargs[b'key'] = book
                hookargs[b'old'] = hex(bookstore.get(book, b''))
                hookargs[b'new'] = hex(node if node is not None else b'')
                allhooks.append(hookargs)

            for hookargs in allhooks:
                op.repo.hook(
                    b'prepushkey', throw=True, **pycompat.strkwargs(hookargs)
                )

        for book, node in changes:
            if bookmarks.isdivergent(book):
                msg = _(b'cannot accept divergent bookmark %s!') % book
                raise error.Abort(msg)

        bookstore.applychanges(op.repo, op.gettransaction(), changes)

        if pushkeycompat:

            def runhook(unused_success):
                for hookargs in allhooks:
                    op.repo.hook(b'pushkey', **pycompat.strkwargs(hookargs))

            op.repo._afterlock(runhook)

    elif bookmarksmode == b'records':
        for book, node in changes:
            record = {b'bookmark': book, b'node': node}
            op.records.add(b'bookmarks', record)
    else:
        raise error.ProgrammingError(
            b'unknown bookmark mode: %s' % bookmarksmode
        )


@parthandler(b'phase-heads')
def handlephases(op, inpart):
    """apply phases from bundle part to repo"""
    headsbyphase = phases.binarydecode(inpart)
    phases.updatephases(op.repo.unfiltered(), op.gettransaction, headsbyphase)


@parthandler(b'reply:pushkey', (b'return', b'in-reply-to'))
def handlepushkeyreply(op, inpart):
    """retrieve the result of a pushkey request"""
    ret = int(inpart.params[b'return'])
    partid = int(inpart.params[b'in-reply-to'])
    op.records.add(b'pushkey', {b'return': ret}, partid)


@parthandler(b'obsmarkers')
def handleobsmarker(op, inpart):
    """add a stream of obsmarkers to the repo"""
    tr = op.gettransaction()
    markerdata = inpart.read()
    if op.ui.config(b'experimental', b'obsmarkers-exchange-debug'):
        op.ui.writenoi18n(
            b'obsmarker-exchange: %i bytes received\n' % len(markerdata)
        )
    # The mergemarkers call will crash if marker creation is not enabled.
    # we want to avoid this if the part is advisory.
    if not inpart.mandatory and op.repo.obsstore.readonly:
        op.repo.ui.debug(
            b'ignoring obsolescence markers, feature not enabled\n'
        )
        return
    new = op.repo.obsstore.mergemarkers(tr, markerdata)
    op.repo.invalidatevolatilesets()
    op.records.add(b'obsmarkers', {b'new': new})
    if op.reply is not None:
        rpart = op.reply.newpart(b'reply:obsmarkers')
        rpart.addparam(
            b'in-reply-to', pycompat.bytestr(inpart.id), mandatory=False
        )
        rpart.addparam(b'new', b'%i' % new, mandatory=False)


@parthandler(b'reply:obsmarkers', (b'new', b'in-reply-to'))
def handleobsmarkerreply(op, inpart):
    """retrieve the result of a pushkey request"""
    ret = int(inpart.params[b'new'])
    partid = int(inpart.params[b'in-reply-to'])
    op.records.add(b'obsmarkers', {b'new': ret}, partid)


@parthandler(b'hgtagsfnodes')
def handlehgtagsfnodes(op, inpart):
    """Applies .hgtags fnodes cache entries to the local repo.

    Payload is pairs of 20 byte changeset nodes and filenodes.
    """
    # Grab the transaction so we ensure that we have the lock at this point.
    if op.ui.configbool(b'experimental', b'bundle2lazylocking'):
        op.gettransaction()
    cache = tags.hgtagsfnodescache(op.repo.unfiltered())

    count = 0
    while True:
        node = inpart.read(20)
        fnode = inpart.read(20)
        if len(node) < 20 or len(fnode) < 20:
            op.ui.debug(b'ignoring incomplete received .hgtags fnodes data\n')
            break
        cache.setfnode(node, fnode)
        count += 1

    cache.write()
    op.ui.debug(b'applied %i hgtags fnodes cache entries\n' % count)


rbcstruct = struct.Struct(b'>III')


@parthandler(b'cache:rev-branch-cache')
def handlerbc(op, inpart):
    """Legacy part, ignored for compatibility with bundles from or
    for Mercurial before 5.7. Newer Mercurial computes the cache
    efficiently enough during unbundling that the additional transfer
    is unnecessary."""


@parthandler(b'pushvars')
def bundle2getvars(op, part):
    '''unbundle a bundle2 containing shellvars on the server'''
    # An option to disable unbundling on server-side for security reasons
    if op.ui.configbool(b'push', b'pushvars.server'):
        hookargs = {}
        for key, value in part.advisoryparams:
            key = key.upper()
            # We want pushed variables to have USERVAR_ prepended so we know
            # they came from the --pushvar flag.
            key = b"USERVAR_" + key
            hookargs[key] = value
        op.addhookargs(hookargs)


@parthandler(b'stream2', (b'requirements', b'filecount', b'bytecount'))
def handlestreamv2bundle(op, part):

    requirements = urlreq.unquote(part.params[b'requirements'])
    requirements = requirements.split(b',') if requirements else []
    filecount = int(part.params[b'filecount'])
    bytecount = int(part.params[b'bytecount'])

    repo = op.repo
    if len(repo):
        msg = _(b'cannot apply stream clone to non empty repository')
        raise error.Abort(msg)

    repo.ui.debug(b'applying stream bundle\n')
    streamclone.applybundlev2(repo, part, filecount, bytecount, requirements)


def widen_bundle(
