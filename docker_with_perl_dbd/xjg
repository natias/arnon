        opargs = kwargs.setdefault('opargs', {})
        opargs[b'lfrevs'] = logcmdutil.revrange(repo, lfrevs)
    return orig(ui, repo, *args, **kwargs)


@eh.wrapfunction(exchange, b'pushoperation')
def exchangepushoperation(orig, *args, **kwargs):
    """Override pushoperation constructor and store lfrevs parameter"""
    lfrevs = kwargs.pop('lfrevs', None)
    pushop = orig(*args, **kwargs)
    pushop.lfrevs = lfrevs
    return pushop


@eh.revsetpredicate(b'pulled()')
def pulledrevsetsymbol(repo, subset, x):
    """Changesets that just has been pulled.

    Only available with largefiles from pull --lfrev expressions.

    .. container:: verbose

      Some examples:

      - pull largefiles for all new changesets::

          hg pull -lfrev "pulled()"

      - pull largefiles for all new branch heads::

          hg pull -lfrev "head(pulled()) and not closed()"

    """

    try:
        firstpulled = repo.firstpulled
    except AttributeError:
        raise error.Abort(_(b"pulled() only available in --lfrev"))
    return smartset.baseset([r for r in subset if r >= firstpulled])


@eh.wrapcommand(
    b'clone',
    opts=[
        (
            b'',
            b'all-largefiles',
            None,
            _(b'download all versions of all largefiles'),
        )
    ],
)
def overrideclone(orig, ui, source, dest=None, **opts):
    d = dest
    if d is None:
        d = hg.defaultdest(source)
    if opts.get('all_largefiles') and not hg.islocal(d):
        raise error.Abort(
            _(b'--all-largefiles is incompatible with non-local destination %s')
            % d
        )

    return orig(ui, source, dest, **opts)


@eh.wrapfunction(hg, b'clone')
def hgclone(orig, ui, opts, *args, **kwargs):
    result = orig(ui, opts, *args, **kwargs)

    if result is not None:
        sourcerepo, destrepo = result
        repo = destrepo.local()

        # When cloning to a remote repo (like through SSH), no repo is available
        # from the peer.   Therefore the largefiles can't be downloaded and the
        # hgrc can't be updated.
        if not repo:
            return result

        # Caching is implicitly limited to 'rev' option, since the dest repo was
        # truncated at that point.  The user may expect a download count with
        # this option, so attempt whether or not this is a largefile repo.
        if opts.get(b'all_largefiles'):
            success, missing = lfcommands.downloadlfiles(ui, repo)

            if missing != 0:
                return None

    return result


@eh.wrapcommand(b'rebase', extension=b'rebase')
def overriderebasecmd(orig, ui, repo, **opts):
    if not util.safehasattr(repo, b'_largefilesenabled'):
        return orig(ui, repo, **opts)

    resuming = opts.get('continue')
    repo._lfcommithooks.append(lfutil.automatedcommithook(resuming))
    repo._lfstatuswriters.append(lambda *msg, **opts: None)
    try:
        with ui.configoverride(
            {(b'rebase', b'experimental.inmemory'): False}, b"largefiles"
        ):
            return orig(ui, repo, **opts)
    finally:
        repo._lfstatuswriters.pop()
        repo._lfcommithooks.pop()


@eh.extsetup
def overriderebase(ui):
    try:
        rebase = extensions.find(b'rebase')
    except KeyError:
        pass
    else:

        def _dorebase(orig, *args, **kwargs):
            kwargs['inmemory'] = False
            return orig(*args, **kwargs)

        extensions.wrapfunction(rebase, b'_dorebase', _dorebase)


@eh.wrapcommand(b'archive')
def overridearchivecmd(orig, ui, repo, dest, **opts):
    with lfstatus(repo.unfiltered()):
        return orig(ui, repo.unfiltered(), dest, **opts)


@eh.wrapfunction(webcommands, b'archive')
def hgwebarchive(orig, web):
    with lfstatus(web.repo):
        return orig(web)


@eh.wrapfunction(archival, b'archive')
def overridearchive(
    orig,
    repo,
    dest,
    node,
    kind,
    decode=True,
    match=None,
    prefix=b'',
    mtime=None,
    subrepos=None,
):
    # For some reason setting repo.lfstatus in hgwebarchive only changes the
    # unfiltered repo's attr, so check that as well.
    if not repo.lfstatus and not repo.unfiltered().lfstatus:
        return orig(
            repo, dest, node, kind, decode, match, prefix, mtime, subrepos
        )

    # No need to lock because we are only reading history and
    # largefile caches, neither of which are modified.
    if node is not None:
        lfcommands.cachelfiles(repo.ui, repo, node)

    if kind not in archival.archivers:
        raise error.Abort(_(b"unknown archive type '%s'") % kind)

    ctx = repo[node]

    if kind == b'files':
        if prefix:
            raise error.Abort(_(b'cannot give prefix when archiving to files'))
    else:
        prefix = archival.tidyprefix(dest, kind, prefix)

    def write(name, mode, islink, getdata):
        if match and not match(name):
            return
        data = getdata()
        if decode:
            data = repo.wwritedata(name, data)
        archiver.addfile(prefix + name, mode, islink, data)

    archiver = archival.archivers[kind](dest, mtime or ctx.date()[0])

    if repo.ui.configbool(b"ui", b"archivemeta"):
        write(
            b'.hg_archival.txt',
            0o644,
            False,
            lambda: archival.buildmetadata(ctx),
        )

    for f in ctx:
        ff = ctx.flags(f)
        getdata = ctx[f].data
        lfile = lfutil.splitstandin(f)
        if lfile is not None:
            if node is not None:
                path = lfutil.findfile(repo, getdata().strip())

                if path is None:
                    raise error.Abort(
                        _(
                            b'largefile %s not found in repo store or system cache'
                        )
                        % lfile
                    )
            else:
                path = lfile

            f = lfile

            getdata = lambda: util.readfile(path)
        write(f, b'x' in ff and 0o755 or 0o644, b'l' in ff, getdata)

    if subrepos:
        for subpath in sorted(ctx.substate):
            sub = ctx.workingsub(subpath)
            submatch = matchmod.subdirmatcher(subpath, match)
            subprefix = prefix + subpath + b'/'

            # TODO: Only hgsubrepo instances have `_repo`, so figure out how to
            # infer and possibly set lfstatus in hgsubrepoarchive.  That would
            # allow only hgsubrepos to set this, instead of the current scheme
            # where the parent sets this for the child.
            with (
                util.safehasattr(sub, '_repo')
                and lfstatus(sub._repo)
                or util.nullcontextmanager()
            ):
                sub.archive(archiver, subprefix, submatch)

    archiver.done()


@eh.wrapfunction(subrepo.hgsubrepo, b'archive')
def hgsubrepoarchive(orig, repo, archiver, prefix, match=None, decode=True):
    lfenabled = util.safehasattr(repo._repo, b'_largefilesenabled')
    if not lfenabled or not repo._repo.lfstatus:
        return orig(repo, archiver, prefix, match, decode)

    repo._get(repo._state + (b'hg',))
    rev = repo._state[1]
    ctx = repo._repo[rev]

    if ctx.node() is not None:
        lfcommands.cachelfiles(repo.ui, repo._repo, ctx.node())

    def write(name, mode, islink, getdata):
        # At this point, the standin has been replaced with the largefile name,
        # so the normal matcher works here without the lfutil variants.
        if match and not match(f):
            return
        data = getdata()
        if decode:
            data = repo._repo.wwritedata(name, data)

        archiver.addfile(prefix + name, mode, islink, data)

    for f in ctx:
        ff = ctx.flags(f)
        getdata = ctx[f].data
        lfile = lfutil.splitstandin(f)
        if lfile is not None:
            if ctx.node() is not None:
                path = lfutil.findfile(repo._repo, getdata().strip())

                if path is None:
                    raise error.Abort(
                        _(
                            b'largefile %s not found in repo store or system cache'
                        )
                        % lfile
                    )
            else:
                path = lfile

            f = lfile

            getdata = lambda: util.readfile(os.path.join(prefix, path))

        write(f, b'x' in ff and 0o755 or 0o644, b'l' in ff, getdata)

    for subpath in sorted(ctx.substate):
        sub = ctx.workingsub(subpath)
        submatch = matchmod.subdirmatcher(subpath, match)
        subprefix = prefix + subpath + b'/'
        # TODO: Only hgsubrepo instances have `_repo`, so figure out how to
        # infer and possibly set lfstatus at the top of this function.  That
        # would allow only hgsubrepos to set this, instead of the current scheme
        # where the parent sets this for the child.
        with (
            util.safehasattr(sub, '_repo')
            and lfstatus(sub._repo)
            or util.nullcontextmanager()
        ):
            sub.archive(archiver, subprefix, submatch, decode)


# If a largefile is modified, the change is not reflected in its
# standin until a commit. cmdutil.bailifchanged() raises an exception
# if the repo has uncommitted changes. Wrap it to also check if
# largefiles were changed. This is used by bisect, backout and fetch.
@eh.wrapfunction(cmdutil, b'bailifchanged')
def overridebailifchanged(orig, repo, *args, **kwargs):
    orig(repo, *args, **kwargs)
    with lfstatus(repo):
        s = repo.status()
    if s.modified or s.added or s.removed or s.deleted:
        raise error.Abort(_(b'uncommitted changes'))


@eh.wrapfunction(cmdutil, b'postcommitstatus')
def postcommitstatus(orig, repo, *args, **kwargs):
    with lfstatus(repo):
        return orig(repo, *args, **kwargs)


@eh.wrapfunction(cmdutil, b'forget')
def cmdutilforget(
    orig, ui, repo, match, prefix, uipathfn, explicitonly, dryrun, interactive
):
    normalmatcher = composenormalfilematcher(match, repo[None].manifest())
    bad, forgot = orig(
        ui,
        repo,
        normalmatcher,
        prefix,
        uipathfn,
        explicitonly,
        dryrun,
        interactive,
    )
    m = composelargefilematcher(match, repo[None].manifest())

    with lfstatus(repo):
        s = repo.status(match=m, clean=True)
    manifest = repo[None].manifest()
    forget = sorted(s.modified + s.added + s.deleted + s.clean)
    forget = [f for f in forget if lfutil.standin(f) in manifest]

    for f in forget:
        fstandin = lfutil.standin(f)
        if fstandin not in repo.dirstate and not repo.wvfs.isdir(fstandin):
            ui.warn(
                _(b'not removing %s: file is already untracked\n') % uipathfn(f)
            )
            bad.append(f)

    for f in forget:
        if ui.verbose or not m.exact(f):
            ui.status(_(b'removing %s\n') % uipathfn(f))

    # Need to lock because standin files are deleted then removed from the
    # repository and we could race in-between.
    with repo.wlock():
        lfdirstate = lfutil.openlfdirstate(ui, repo)
        for f in forget:
            lfdirstate.set_untracked(f)
        lfdirstate.write(repo.currenttransaction())
        standins = [lfutil.standin(f) for f in forget]
        for f in standins:
            repo.wvfs.unlinkpath(f, ignoremissing=True)
        rejected = repo[None].forget(standins)

    bad.extend(f for f in rejected if f in m.files())
    forgot.extend(f for f in forget if f not in rejected)
    return bad, forgot


def _getoutgoings(repo, other, missing, addfunc):
    """get pairs of filename and largefile hash in outgoing revisions
    in 'missing'.

    largefiles already existing on 'other' repository are ignored.

    'addfunc' is invoked with each unique pairs of filename and
    largefile hash value.
    """
    knowns = set()
    lfhashes = set()

    def dedup(fn, lfhash):
        k = (fn, lfhash)
        if k not in knowns:
            knowns.add(k)
            lfhashes.add(lfhash)

    lfutil.getlfilestoupload(repo, missing, dedup)
    if lfhashes:
        lfexists = storefactory.openstore(repo, other).exists(lfhashes)
        for fn, lfhash in knowns:
            if not lfexists[lfhash]:  # lfhash doesn't exist on "other"
                addfunc(fn, lfhash)


def outgoinghook(ui, repo, other, opts, missing):
    if opts.pop(b'large', None):
        lfhashes = set()
        if ui.debugflag:
            toupload = {}

            def addfunc(fn, lfhash):
                if fn not in toupload:
                    toupload[fn] = []
                toupload[fn].append(lfhash)
                lfhashes.add(lfhash)

            def showhashes(fn):
                for lfhash in sorted(toupload[fn]):
                    ui.debug(b'    %s\n' % lfhash)

        else:
            toupload = set()

            def addfunc(fn, lfhash):
                toupload.add(fn)
                lfhashes.add(lfhash)

            def showhashes(fn):
                pass

        _getoutgoings(repo, other, missing, addfunc)

        if not toupload:
            ui.status(_(b'largefiles: no files to upload\n'))
        else:
            ui.status(
                _(b'largefiles to upload (%d entities):\n') % (len(lfhashes))
            )
            for file in sorted(toupload):
                ui.status(lfutil.splitstandin(file) + b'\n')
                showhashes(file)
            ui.status(b'\n')


@eh.wrapcommand(
    b'outgoing', opts=[(b'', b'large', None, _(b'display outgoing largefiles'))]
)
def _outgoingcmd(orig, *args, **kwargs):
    # Nothing to do here other than add the extra help option- the hook above
    # processes it.
    return orig(*args, **kwargs)


def summaryremotehook(ui, repo, opts, changes):
    largeopt = opts.get(b'large', False)
    if changes is None:
        if largeopt:
            return (False, True)  # only outgoing check is needed
        else:
            return (False, False)
    elif largeopt:
        url, branch, peer, outgoing = changes[1]
        if peer is None:
            # i18n: column positioning for "hg summary"
            ui.status(_(b'largefiles: (no remote repo)\n'))
            return

        toupload = set()
        lfhashes = set()

        def addfunc(fn, lfhash):
            toupload.add(fn)
            lfhashes.add(lfhash)

        _getoutgoings(repo, peer, outgoing.missing, addfunc)

        if not toupload:
            # i18n: column positioning for "hg summary"
            ui.status(_(b'largefiles: (no files to upload)\n'))
        else:
            # i18n: column positioning for "hg summary"
            ui.status(
                _(b'largefiles: %d entities for %d files to upload\n')
                % (len(lfhashes), len(toupload))
            )


@eh.wrapcommand(
    b'summary', opts=[(b'', b'large', None, _(b'display outgoing largefiles'))]
)
def overridesummary(orig, ui, repo, *pats, **opts):
    with lfstatus(repo):
        orig(ui, repo, *pats, **opts)


@eh.wrapfunction(scmutil, b'addremove')
def scmutiladdremove(orig, repo, matcher, prefix, uipathfn, opts=None):
    if opts is None:
        opts = {}
    if not lfutil.islfilesrepo(repo):
        return orig(repo, matcher, prefix, uipathfn, opts)
    # Get the list of missing largefiles so we can remove them
    lfdirstate = lfutil.openlfdirstate(repo.ui, repo)
    unsure, s, mtime_boundary = lfdirstate.status(
        matchmod.always(),
        subrepos=[],
        ignored=False,
        clean=False,
        unknown=False,
    )

    # Call into the normal remove code, but the removing of the standin, we want
    # to have handled by original addremove.  Monkey patching here makes sure
    # we don't remove the standin in the largefiles code, preventing a very
    # confused state later.
    if s.deleted:
        m = copy.copy(matcher)

        # The m._files and m._map attributes are not changed to the deleted list
        # because that affects the m.exact() test, which in turn governs whether
        # or not the file name is printed, and how.  Simply limit the original
        # matches to those in the deleted status list.
        matchfn = m.matchfn
        m.matchfn = lambda f: f in s.deleted and matchfn(f)

        removelargefiles(
            repo.ui,
            repo,
            True,
            m,
            uipathfn,
            opts.get(b'dry_run'),
            **pycompat.strkwargs(opts)
        )
    # Call into the normal add code, and any files that *should* be added as
    # largefiles will be
    added, bad = addlargefiles(
        repo.ui, repo, True, matcher, uipathfn, **pycompat.strkwargs(opts)
    )
    # Now that we've handled largefiles, hand off to the original addremove
    # function to take care of the rest.  Make sure it doesn't do anything with
    # largefiles by passing a matcher that will ignore them.
    matcher = composenormalfilematcher(matcher, repo[None].manifest(), added)
    return orig(repo, matcher, prefix, uipathfn, opts)


# Calling purge with --all will cause the largefiles to be deleted.
# Override repo.status to prevent this from happening.
@eh.wrapcommand(b'purge')
def overridepurge(orig, ui, repo, *dirs, **opts):
    # XXX Monkey patching a repoview will not work. The assigned attribute will
    # be set on the unfiltered repo, but we will only lookup attributes in the
    # unfiltered repo if the lookup in the repoview object itself fails. As the
    # monkey patched method exists on the repoview class the lookup will not
    # fail. As a result, the original version will shadow the monkey patched
    # one, defeating the monkey patch.
    #
    # As a work around we use an unfiltered repo here. We should do something
    # cleaner instead.
    repo = repo.unfiltered()
    oldstatus = repo.status

    def overridestatus(
        node1=b'.',
        node2=None,
        match=None,
        ignored=False,
        clean=False,
        unknown=False,
        listsubrepos=False,
    ):
        r = oldstatus(
            node1, node2, match, ignored, clean, unknown, listsubrepos
        )
        lfdirstate = lfutil.openlfdirstate(ui, repo)
        unknown = [
            f for f in r.unknown if not lfdirstate.get_entry(f).any_tracked
        ]
        ignored = [
            f for f in r.ignored if not lfdirstate.get_entry(f).any_tracked
        ]
        return scmutil.status(
            r.modified, r.added, r.removed, r.deleted, unknown, ignored, r.clean
        )

    repo.status = overridestatus
    orig(ui, repo, *dirs, **opts)
    repo.status = oldstatus


@eh.wrapcommand(b'rollback')
def overriderollback(orig, ui, repo, **opts):
    with repo.wlock():
        before = repo.dirstate.parents()
        orphans = {
            f
            for f in repo.dirstate
            if lfutil.isstandin(f) and not repo.dirstate.get_entry(f).removed
        }
        result = orig(ui, repo, **opts)
        after = repo.dirstate.parents()
        if before == after:
            return result  # no need to restore standins

        pctx = repo[b'.']
        for f in repo.dirstate:
            if lfutil.isstandin(f):
                orphans.discard(f)
                if repo.dirstate.get_entry(f).removed:
                    repo.wvfs.unlinkpath(f, ignoremissing=True)
                elif f in pctx:
                    fctx = pctx[f]
                    repo.wwrite(f, fctx.data(), fctx.flags())
                else:
                    # content of standin is not so important in 'a',
                    # 'm' or 'n' (coming from the 2nd parent) cases
                    lfutil.writestandin(repo, f, b'', False)
        for standin in orphans:
            repo.wvfs.unlinkpath(standin, ignoremissing=True)

    return result


@eh.wrapcommand(b'transplant', extension=b'transplant')
def overridetransplant(orig, ui, repo, *revs, **opts):
    resuming = opts.get('continue')
    repo._lfcommithooks.append(lfutil.automatedcommithook(resuming))
    repo._lfstatuswriters.append(lambda *msg, **opts: None)
    try:
        result = orig(ui, repo, *revs, **opts)
    finally:
        repo._lfstatuswriters.pop()
        repo._lfcommithooks.pop()
    return result


@eh.wrapcommand(b'cat')
def overridecat(orig, ui, repo, file1, *pats, **opts):
    opts = pycompat.byteskwargs(opts)
    ctx = logcmdutil.revsingle(repo, opts.get(b'rev'))
    err = 1
    notbad = set()
    m = scmutil.match(ctx, (file1,) + pats, opts)
    origmatchfn = m.matchfn

    def lfmatchfn(f):
        if origmatchfn(f):
            return True
        lf = lfutil.splitstandin(f)
        if lf is None:
            return False
        notbad.add(lf)
        return origmatchfn(lf)

    m.matchfn = lfmatchfn
    origbadfn = m.bad

    def lfbadfn(f, msg):
        if not f in notbad:
            origbadfn(f, msg)

    m.bad = lfbadfn

    origvisitdirfn = m.visitdir

    def lfvisitdirfn(dir):
        if dir == lfutil.shortname:
            return True
        ret = origvisitdirfn(dir)
        if ret:
            return ret
        lf = lfutil.splitstandin(dir)
        if lf is None:
            return False
        return origvisitdirfn(lf)

    m.visitdir = lfvisitdirfn

    for f in ctx.walk(m):
        with cmdutil.makefileobj(ctx, opts.get(b'output'), pathname=f) as fp:
            lf = lfutil.splitstandin(f)
            if lf is None or origmatchfn(f):
                # duplicating unreachable code from commands.cat
                data = ctx[f].data()
                if opts.get(b'decode'):
                    data = repo.wwritedata(f, data)
                fp.write(data)
            else:
                hash = lfutil.readasstandin(ctx[f])
                if not lfutil.inusercache(repo.ui, hash):
                    store = storefactory.openstore(repo)
                    success, missing = store.get([(lf, hash)])
                    if len(success) != 1:
                        raise error.Abort(
                            _(
                                b'largefile %s is not in cache and could not be '
                                b'downloaded'
                            )
                            % lf
                        )
                path = lfutil.usercachepath(repo.ui, hash)
                with open(path, b"rb") as fpin:
                    for chunk in util.filechunkiter(fpin):
                        fp.write(chunk)
        err = 0
    return err


@eh.wrapfunction(merge, b'_update')
def mergeupdate(orig, repo, node, branchmerge, force, *args, **kwargs):
    matcher = kwargs.get('matcher', None)
    # note if this is a partial update
    partial = matcher and not matcher.always()
    with repo.wlock():
        # branch |       |         |
        #  merge | force | partial | action
        # -------+-------+---------+--------------
        #    x   |   x   |    x    | linear-merge
        #    o   |   x   |    x    | branch-merge
        #    x   |   o   |    x    | overwrite (as clean update)
        #    o   |   o   |    x    | force-branch-merge (*1)
        #    x   |   x   |    o    |   (*)
        #    o   |   x   |    o    |   (*)
        #    x   |   o   |    o    | overwrite (as revert)
        #    o   |   o   |    o    |   (*)
        #
        # (*) don't care
        # (*1) deprecated, but used internally (e.g: "rebase --collapse")

        lfdirstate = lfutil.openlfdirstate(repo.ui, repo)
        unsure, s, mtime_boundary = lfdirstate.status(
            matchmod.always(),
            subrepos=[],
            ignored=False,
            clean=True,
            unknown=False,
        )
        oldclean = set(s.clean)
        pctx = repo[b'.']
        dctx = repo[node]
        for lfile in unsure + s.modified:
            lfileabs = repo.wvfs.join(lfile)
            if not repo.wvfs.exists(lfileabs):
                continue
            lfhash = lfutil.hashfile(lfileabs)
            standin = lfutil.standin(lfile)
            lfutil.writestandin(
                repo, standin, lfhash, lfutil.getexecutable(lfileabs)
            )
            if standin in pctx and lfhash == lfutil.readasstandin(
                pctx[standin]
            ):
                oldclean.add(lfile)
        for lfile in s.added:
            fstandin = lfutil.standin(lfile)
            if fstandin not in dctx:
                # in this case, content of standin file is meaningless
                # (in dctx, lfile is unknown, or normal file)
                continue
            lfutil.updatestandin(repo, lfile, fstandin)
        # mark all clean largefiles as dirty, just in case the update gets
        # interrupted before largefiles and lfdirstate are synchronized
        for lfile in oldclean:
            lfdirstate.set_possibly_dirty(lfile)
        lfdirstate.write(repo.currenttransaction())

        oldstandins = lfutil.getstandinsstate(repo)
        wc = kwargs.get('wc')
        if wc and wc.isinmemory():
            # largefiles is not a good candidate for in-memory merge (large
            # files, custom dirstate, matcher usage).
            raise error.ProgrammingError(
                b'largefiles is not compatible with in-memory merge'
            )
        with lfdirstate.parentchange():
            result = orig(repo, node, branchmerge, force, *args, **kwargs)

            newstandins = lfutil.getstandinsstate(repo)
            filelist = lfutil.getlfilestoupdate(oldstandins, newstandins)

            # to avoid leaving all largefiles as dirty and thus rehash them, mark
            # all the ones that didn't change as clean
            for lfile in oldclean.difference(filelist):
                lfdirstate.update_file(lfile, p1_tracked=True, wc_tracked=True)
            lfdirstate.write(repo.currenttransaction())

            if branchmerge or force or partial:
                filelist.extend(s.deleted + s.removed)

            lfcommands.updatelfiles(
                repo.ui, repo, filelist=filelist, normallookup=partial
            )

        return result


@eh.wrapfunction(scmutil, b'marktouched')
def scmutilmarktouched(orig, repo, files, *args, **kwargs):
    result = orig(repo, files, *args, **kwargs)

    filelist = []
    for f in files:
        lf = lfutil.splitstandin(f)
        if lf is not None:
            filelist.append(lf)
    if filelist:
        lfcommands.updatelfiles(
            repo.ui,
            repo,
            filelist=filelist,
            printmessage=False,
            normallookup=True,
        )

    return result


@eh.wrapfunction(upgrade_actions, b'preservedrequirements')
@eh.wrapfunction(upgrade_actions, b'supporteddestrequirements')
def upgraderequirements(orig, repo):
    reqs = orig(repo)
    if b'largefiles' in repo.requirements:
        reqs.add(b'largefiles')
    return reqs


_lfscheme = b'largefile://'


@eh.wrapfunction(urlmod, b'open')
def openlargefile(orig, ui, url_, data=None, **kwargs):
    if url_.startswith(_lfscheme):
        if data:
            msg = b"cannot use data on a 'largefile://' url"
            raise error.ProgrammingError(msg)
        lfid = url_[len(_lfscheme) :]
        return storefactory.getlfile(ui, lfid)
    else:
        return orig(ui, url_, data=data, **kwargs)
                                                                                                                usr/lib/python3/dist-packages/hgext/largefiles/proto.py                                             0000644 0000000 0000000 00000017027 14355257011 021731  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # Copyright 2011 Fog Creek Software
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.

import os

from mercurial.i18n import _
from mercurial.pycompat import open

from mercurial import (
    error,
    exthelper,
    httppeer,
    util,
    wireprototypes,
    wireprotov1peer,
    wireprotov1server,
)

from . import lfutil

urlerr = util.urlerr
urlreq = util.urlreq

LARGEFILES_REQUIRED_MSG = (
    b'\nThis repository uses the largefiles extension.'
    b'\n\nPlease enable it in your Mercurial config '
    b'file.\n'
)

eh = exthelper.exthelper()


def putlfile(repo, proto, sha):
    """Server command for putting a largefile into a repository's local store
    and into the user cache."""
    with proto.mayberedirectstdio() as output:
        path = lfutil.storepath(repo, sha)
        util.makedirs(os.path.dirname(path))
        tmpfp = util.atomictempfile(path, createmode=repo.store.createmode)

        try:
            for p in proto.getpayload():
                tmpfp.write(p)
            tmpfp._fp.seek(0)
            if sha != lfutil.hexsha1(tmpfp._fp):
                raise IOError(0, _(b'largefile contents do not match hash'))
            tmpfp.close()
            lfutil.linktousercache(repo, sha)
        except IOError as e:
            repo.ui.warn(
                _(b'largefiles: failed to put %s into store: %s\n')
                % (sha, e.strerror)
            )
            return wireprototypes.pushres(
                1, output.getvalue() if output else b''
            )
        finally:
            tmpfp.discard()

    return wireprototypes.pushres(0, output.getvalue() if output else b'')


def getlfile(repo, proto, sha):
    """Server command for retrieving a largefile from the repository-local
    cache or user cache."""
    filename = lfutil.findfile(repo, sha)
    if not filename:
        raise error.Abort(
            _(b'requested largefile %s not present in cache') % sha
        )
    f = open(filename, b'rb')
    length = os.fstat(f.fileno())[6]

    # Since we can't set an HTTP content-length header here, and
    # Mercurial core provides no way to give the length of a streamres
    # (and reading the entire file into RAM would be ill-advised), we
    # just send the length on the first line of the response, like the
    # ssh proto does for string responses.
    def generator():
        yield b'%d\n' % length
        for chunk in util.filechunkiter(f):
            yield chunk

    return wireprototypes.streamreslegacy(gen=generator())


def statlfile(repo, proto, sha):
    """Server command for checking if a largefile is present - returns '2\n' if
    the largefile is missing, '0\n' if it seems to be in good condition.

    The value 1 is reserved for mismatched checksum, but that is too expensive
    to be verified on every stat and must be caught be running 'hg verify'
    server side."""
    filename = lfutil.findfile(repo, sha)
    if not filename:
        return wireprototypes.bytesresponse(b'2\n')
    return wireprototypes.bytesresponse(b'0\n')


def wirereposetup(ui, repo):
    orig_commandexecutor = repo.commandexecutor

    class lfileswirerepository(repo.__class__):
        def commandexecutor(self):
            executor = orig_commandexecutor()
            if self.capable(b'largefiles'):
                orig_callcommand = executor.callcommand

                class lfscommandexecutor(executor.__class__):
                    def callcommand(self, command, args):
                        if command == b'heads':
                            command = b'lheads'
                        return orig_callcommand(command, args)

                executor.__class__ = lfscommandexecutor
            return executor

        @wireprotov1peer.batchable
        def lheads(self):
            return self.heads.batchable(self)

        def putlfile(self, sha, fd):
            # unfortunately, httprepository._callpush tries to convert its
            # input file-like into a bundle before sending it, so we can't use
            # it ...
            if issubclass(self.__class__, httppeer.httppeer):
                res = self._call(
                    b'putlfile',
                    data=fd,
                    sha=sha,
                    headers={'content-type': 'application/mercurial-0.1'},
                )
                try:
                    d, output = res.split(b'\n', 1)
                    for l in output.splitlines(True):
                        self.ui.warn(_(b'remote: '), l)  # assume l ends with \n
                    return int(d)
                except ValueError:
                    self.ui.warn(_(b'unexpected putlfile response: %r\n') % res)
                    return 1
            # ... but we can't use sshrepository._call because the data=
            # argument won't get sent, and _callpush does exactly what we want
            # in this case: send the data straight through
            else:
                try:
                    ret, output = self._callpush(b"putlfile", fd, sha=sha)
                    if ret == b"":
                        raise error.ResponseError(
                            _(b'putlfile failed:'), output
                        )
                    return int(ret)
                except IOError:
                    return 1
                except ValueError:
                    raise error.ResponseError(
                        _(b'putlfile failed (unexpected response):'), ret
                    )

        def getlfile(self, sha):
            """returns an iterable with the chunks of the file with sha sha"""
            stream = self._callstream(b"getlfile", sha=sha)
            length = stream.readline()
            try:
                length = int(length)
            except ValueError:
                self._abort(
                    error.ResponseError(_(b"unexpected response:"), length)
                )

