                b'%s> readinto(%d) -> %r' % (self.name, len(dest), res)
            )

        data = dest[0:res] if res is not None else b''

        # _writedata() uses "in" operator and is confused by memoryview because
        # characters are ints on Python 3.
        if isinstance(data, memoryview):
            data = data.tobytes()

        self._writedata(data)

    def write(self, res, data):
        if not self.writes:
            return

        # Python 2 returns None from some write() calls. Python 3 (reasonably)
        # returns the integer bytes written.
        if res is None and data:
            res = len(data)

        if self.logdataapis:
            self.fh.write(b'%s> write(%d) -> %r' % (self.name, len(data), res))

        self._writedata(data)

    def flush(self, res):
        if not self.writes:
            return

        self.fh.write(b'%s> flush() -> %r\n' % (self.name, res))

    # For observedbufferedinputpipe.
    def bufferedread(self, res, size):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> bufferedread(%d) -> %d' % (self.name, size, len(res))
            )

        self._writedata(res)

    def bufferedreadline(self, res):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> bufferedreadline() -> %d' % (self.name, len(res))
            )

        self._writedata(res)


def makeloggingfileobject(
    logh, fh, name, reads=True, writes=True, logdata=False, logdataapis=True
):
    """Turn a file object into a logging file object."""

    observer = fileobjectobserver(
        logh,
        name,
        reads=reads,
        writes=writes,
        logdata=logdata,
        logdataapis=logdataapis,
    )
    return fileobjectproxy(fh, observer)


class socketobserver(baseproxyobserver):
    """Logs socket activity."""

    def __init__(
        self,
        fh,
        name,
        reads=True,
        writes=True,
        states=True,
        logdata=False,
        logdataapis=True,
    ):
        super(socketobserver, self).__init__(fh, name, logdata, logdataapis)
        self.reads = reads
        self.writes = writes
        self.states = states

    def makefile(self, res, mode=None, bufsize=None):
        if not self.states:
            return

        self.fh.write(b'%s> makefile(%r, %r)\n' % (self.name, mode, bufsize))

    def recv(self, res, size, flags=0):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> recv(%d, %d) -> %d' % (self.name, size, flags, len(res))
            )
        self._writedata(res)

    def recvfrom(self, res, size, flags=0):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> recvfrom(%d, %d) -> %d'
                % (self.name, size, flags, len(res[0]))
            )

        self._writedata(res[0])

    def recvfrom_into(self, res, buf, size, flags=0):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> recvfrom_into(%d, %d) -> %d'
                % (self.name, size, flags, res[0])
            )

        self._writedata(buf[0 : res[0]])

    def recv_into(self, res, buf, size=0, flags=0):
        if not self.reads:
            return

        if self.logdataapis:
            self.fh.write(
                b'%s> recv_into(%d, %d) -> %d' % (self.name, size, flags, res)
            )

        self._writedata(buf[0:res])

    def send(self, res, data, flags=0):
        if not self.writes:
            return

        self.fh.write(
            b'%s> send(%d, %d) -> %d' % (self.name, len(data), flags, len(res))
        )
        self._writedata(data)

    def sendall(self, res, data, flags=0):
        if not self.writes:
            return

        if self.logdataapis:
            # Returns None on success. So don't bother reporting return value.
            self.fh.write(
                b'%s> sendall(%d, %d)' % (self.name, len(data), flags)
            )

        self._writedata(data)

    def sendto(self, res, data, flagsoraddress, address=None):
        if not self.writes:
            return

        if address:
            flags = flagsoraddress
        else:
            flags = 0

        if self.logdataapis:
            self.fh.write(
                b'%s> sendto(%d, %d, %r) -> %d'
                % (self.name, len(data), flags, address, res)
            )

        self._writedata(data)

    def setblocking(self, res, flag):
        if not self.states:
            return

        self.fh.write(b'%s> setblocking(%r)\n' % (self.name, flag))

    def settimeout(self, res, value):
        if not self.states:
            return

        self.fh.write(b'%s> settimeout(%r)\n' % (self.name, value))

    def gettimeout(self, res):
        if not self.states:
            return

        self.fh.write(b'%s> gettimeout() -> %f\n' % (self.name, res))

    def setsockopt(self, res, level, optname, value):
        if not self.states:
            return

        self.fh.write(
            b'%s> setsockopt(%r, %r, %r) -> %r\n'
            % (self.name, level, optname, value, res)
        )


def makeloggingsocket(
    logh,
    fh,
    name,
    reads=True,
    writes=True,
    states=True,
    logdata=False,
    logdataapis=True,
):
    """Turn a socket into a logging socket."""

    observer = socketobserver(
        logh,
        name,
        reads=reads,
        writes=writes,
        states=states,
        logdata=logdata,
        logdataapis=logdataapis,
    )
    return socketproxy(fh, observer)


def version():
    """Return version information if available."""
    try:
        from . import __version__

        return __version__.version
    except ImportError:
        return b'unknown'


def versiontuple(v=None, n=4):
    """Parses a Mercurial version string into an N-tuple.

    The version string to be parsed is specified with the ``v`` argument.
    If it isn't defined, the current Mercurial version string will be parsed.

    ``n`` can be 2, 3, or 4. Here is how some version strings map to
    returned values:

    >>> v = b'3.6.1+190-df9b73d2d444'
    >>> versiontuple(v, 2)
    (3, 6)
    >>> versiontuple(v, 3)
    (3, 6, 1)
    >>> versiontuple(v, 4)
    (3, 6, 1, '190-df9b73d2d444')

    >>> versiontuple(b'3.6.1+190-df9b73d2d444+20151118')
    (3, 6, 1, '190-df9b73d2d444+20151118')

    >>> v = b'3.6'
    >>> versiontuple(v, 2)
    (3, 6)
    >>> versiontuple(v, 3)
    (3, 6, None)
    >>> versiontuple(v, 4)
    (3, 6, None, None)

    >>> v = b'3.9-rc'
    >>> versiontuple(v, 2)
    (3, 9)
    >>> versiontuple(v, 3)
    (3, 9, None)
    >>> versiontuple(v, 4)
    (3, 9, None, 'rc')

    >>> v = b'3.9-rc+2-02a8fea4289b'
    >>> versiontuple(v, 2)
    (3, 9)
    >>> versiontuple(v, 3)
    (3, 9, None)
    >>> versiontuple(v, 4)
    (3, 9, None, 'rc+2-02a8fea4289b')

    >>> versiontuple(b'4.6rc0')
    (4, 6, None, 'rc0')
    >>> versiontuple(b'4.6rc0+12-425d55e54f98')
    (4, 6, None, 'rc0+12-425d55e54f98')
    >>> versiontuple(b'.1.2.3')
    (None, None, None, '.1.2.3')
    >>> versiontuple(b'12.34..5')
    (12, 34, None, '..5')
    >>> versiontuple(b'1.2.3.4.5.6')
    (1, 2, 3, '.4.5.6')
    """
    if not v:
        v = version()
    m = remod.match(br'(\d+(?:\.\d+){,2})[+-]?(.*)', v)
    if not m:
        vparts, extra = b'', v
    elif m.group(2):
        vparts, extra = m.groups()
    else:
        vparts, extra = m.group(1), None

    assert vparts is not None  # help pytype

    vints = []
    for i in vparts.split(b'.'):
        try:
            vints.append(int(i))
        except ValueError:
            break
    # (3, 6) -> (3, 6, None)
    while len(vints) < 3:
        vints.append(None)

    if n == 2:
        return (vints[0], vints[1])
    if n == 3:
        return (vints[0], vints[1], vints[2])
    if n == 4:
        return (vints[0], vints[1], vints[2], extra)

    raise error.ProgrammingError(b"invalid version part request: %d" % n)


def cachefunc(func):
    '''cache the result of function calls'''
    # XXX doesn't handle keywords args
    if func.__code__.co_argcount == 0:
        listcache = []

        def f():
            if len(listcache) == 0:
                listcache.append(func())
            return listcache[0]

        return f
    cache = {}
    if func.__code__.co_argcount == 1:
        # we gain a small amount of time because
        # we don't need to pack/unpack the list
        def f(arg):
            if arg not in cache:
                cache[arg] = func(arg)
            return cache[arg]

    else:

        def f(*args):
            if args not in cache:
                cache[args] = func(*args)
            return cache[args]

    return f


class cow:
    """helper class to make copy-on-write easier

    Call preparewrite before doing any writes.
    """

    def preparewrite(self):
        """call this before writes, return self or a copied new object"""
        if getattr(self, '_copied', 0):
            self._copied -= 1
            # Function cow.__init__ expects 1 arg(s), got 2 [wrong-arg-count]
            return self.__class__(self)  # pytype: disable=wrong-arg-count
        return self

    def copy(self):
        """always do a cheap copy"""
        self._copied = getattr(self, '_copied', 0) + 1
        return self


class sortdict(collections.OrderedDict):
    """a simple sorted dictionary

    >>> d1 = sortdict([(b'a', 0), (b'b', 1)])
    >>> d2 = d1.copy()
    >>> d2
    sortdict([('a', 0), ('b', 1)])
    >>> d2.update([(b'a', 2)])
    >>> list(d2.keys()) # should still be in last-set order
    ['b', 'a']
    >>> d1.insert(1, b'a.5', 0.5)
    >>> d1
    sortdict([('a', 0), ('a.5', 0.5), ('b', 1)])
    """

    def __setitem__(self, key, value):
        if key in self:
            del self[key]
        super(sortdict, self).__setitem__(key, value)

    if pycompat.ispypy:
        # __setitem__() isn't called as of PyPy 5.8.0
        def update(self, src, **f):
            if isinstance(src, dict):
                src = src.items()
            for k, v in src:
                self[k] = v
            for k in f:
                self[k] = f[k]

    def insert(self, position, key, value):
        for (i, (k, v)) in enumerate(list(self.items())):
            if i == position:
                self[key] = value
            if i >= position:
                del self[k]
                self[k] = v


class cowdict(cow, dict):
    """copy-on-write dict

    Be sure to call d = d.preparewrite() before writing to d.

    >>> a = cowdict()
    >>> a is a.preparewrite()
    True
    >>> b = a.copy()
    >>> b is a
    True
    >>> c = b.copy()
    >>> c is a
    True
    >>> a = a.preparewrite()
    >>> b is a
    False
    >>> a is a.preparewrite()
    True
    >>> c = c.preparewrite()
    >>> b is c
    False
    >>> b is b.preparewrite()
    True
    """


class cowsortdict(cow, sortdict):
    """copy-on-write sortdict

    Be sure to call d = d.preparewrite() before writing to d.
    """


class transactional:  # pytype: disable=ignored-metaclass
    """Base class for making a transactional type into a context manager."""

    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def close(self):
        """Successfully closes the transaction."""

    @abc.abstractmethod
    def release(self):
        """Marks the end of the transaction.

        If the transaction has not been closed, it will be aborted.
        """

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            if exc_type is None:
                self.close()
        finally:
            self.release()


@contextlib.contextmanager
def acceptintervention(tr=None):
    """A context manager that closes the transaction on InterventionRequired

    If no transaction was provided, this simply runs the body and returns
    """
    if not tr:
        yield
        return
    try:
        yield
        tr.close()
    except error.InterventionRequired:
        tr.close()
        raise
    finally:
        tr.release()


@contextlib.contextmanager
def nullcontextmanager(enter_result=None):
    yield enter_result


class _lrucachenode:
    """A node in a doubly linked list.

    Holds a reference to nodes on either side as well as a key-value
    pair for the dictionary entry.
    """

    __slots__ = ('next', 'prev', 'key', 'value', 'cost')

    def __init__(self):
        self.next = self
        self.prev = self

        self.key = _notset
        self.value = None
        self.cost = 0

    def markempty(self):
        """Mark the node as emptied."""
        self.key = _notset
        self.value = None
        self.cost = 0


class lrucachedict:
    """Dict that caches most recent accesses and sets.

    The dict consists of an actual backing dict - indexed by original
    key - and a doubly linked circular list defining the order of entries in
    the cache.

    The head node is the newest entry in the cache. If the cache is full,
    we recycle head.prev and make it the new head. Cache accesses result in
    the node being moved to before the existing head and being marked as the
    new head node.

    Items in the cache can be inserted with an optional "cost" value. This is
    simply an integer that is specified by the caller. The cache can be queried
    for the total cost of all items presently in the cache.

    The cache can also define a maximum cost. If a cache insertion would
    cause the total cost of the cache to go beyond the maximum cost limit,
    nodes will be evicted to make room for the new code. This can be used
    to e.g. set a max memory limit and associate an estimated bytes size
    cost to each item in the cache. By default, no maximum cost is enforced.
    """

    def __init__(self, max, maxcost=0):
        self._cache = {}

        self._head = _lrucachenode()
        self._size = 1
        self.capacity = max
        self.totalcost = 0
        self.maxcost = maxcost

    def __len__(self):
        return len(self._cache)

    def __contains__(self, k):
        return k in self._cache

    def __iter__(self):
        # We don't have to iterate in cache order, but why not.
        n = self._head
        for i in range(len(self._cache)):
            yield n.key
            n = n.next

    def __getitem__(self, k):
        node = self._cache[k]
        self._movetohead(node)
        return node.value

    def insert(self, k, v, cost=0):
        """Insert a new item in the cache with optional cost value."""
        node = self._cache.get(k)
        # Replace existing value and mark as newest.
        if node is not None:
            self.totalcost -= node.cost
            node.value = v
            node.cost = cost
            self.totalcost += cost
            self._movetohead(node)

            if self.maxcost:
                self._enforcecostlimit()

            return

        if self._size < self.capacity:
            node = self._addcapacity()
        else:
            # Grab the last/oldest item.
            node = self._head.prev

        # At capacity. Kill the old entry.
        if node.key is not _notset:
            self.totalcost -= node.cost
            del self._cache[node.key]

        node.key = k
        node.value = v
        node.cost = cost
        self.totalcost += cost
        self._cache[k] = node
        # And mark it as newest entry. No need to adjust order since it
        # is already self._head.prev.
        self._head = node

        if self.maxcost:
            self._enforcecostlimit()

    def __setitem__(self, k, v):
        self.insert(k, v)

    def __delitem__(self, k):
        self.pop(k)

    def pop(self, k, default=_notset):
        try:
            node = self._cache.pop(k)
        except KeyError:
            if default is _notset:
                raise
            return default

        assert node is not None  # help pytype
        value = node.value
        self.totalcost -= node.cost
        node.markempty()

        # Temporarily mark as newest item before re-adjusting head to make
        # this node the oldest item.
        self._movetohead(node)
        self._head = node.next

        return value

    # Additional dict methods.

    def get(self, k, default=None):
        try:
            return self.__getitem__(k)
        except KeyError:
            return default

    def peek(self, k, default=_notset):
        """Get the specified item without moving it to the head

        Unlike get(), this doesn't mutate the internal state. But be aware
        that it doesn't mean peek() is thread safe.
        """
        try:
            node = self._cache[k]
            assert node is not None  # help pytype
            return node.value
        except KeyError:
            if default is _notset:
                raise
            return default

    def clear(self):
        n = self._head
        while n.key is not _notset:
            self.totalcost -= n.cost
            n.markempty()
            n = n.next

        self._cache.clear()

    def copy(self, capacity=None, maxcost=0):
        """Create a new cache as a copy of the current one.

        By default, the new cache has the same capacity as the existing one.
        But, the cache capacity can be changed as part of performing the
        copy.

        Items in the copy have an insertion/access order matching this
        instance.
        """

        capacity = capacity or self.capacity
        maxcost = maxcost or self.maxcost
        result = lrucachedict(capacity, maxcost=maxcost)

        # We copy entries by iterating in oldest-to-newest order so the copy
        # has the correct ordering.

        # Find the first non-empty entry.
        n = self._head.prev
        while n.key is _notset and n is not self._head:
            n = n.prev

        # We could potentially skip the first N items when decreasing capacity.
        # But let's keep it simple unless it is a performance problem.
        for i in range(len(self._cache)):
            result.insert(n.key, n.value, cost=n.cost)
            n = n.prev

        return result

    def popoldest(self):
        """Remove the oldest item from the cache.

        Returns the (key, value) describing the removed cache entry.
        """
        if not self._cache:
            return

        # Walk the linked list backwards starting at tail node until we hit
        # a non-empty node.
        n = self._head.prev

        assert n is not None  # help pytype

        while n.key is _notset:
            n = n.prev

        assert n is not None  # help pytype

        key, value = n.key, n.value

        # And remove it from the cache and mark it as empty.
        del self._cache[n.key]
        self.totalcost -= n.cost
        n.markempty()

        return key, value

    def _movetohead(self, node):
        """Mark a node as the newest, making it the new head.

        When a node is accessed, it becomes the freshest entry in the LRU
        list, which is denoted by self._head.

        Visually, let's make ``N`` the new head node (* denotes head):

            previous/oldest <-> head <-> next/next newest

            ----<->--- A* ---<->-----
            |                       |
            E <-> D <-> N <-> C <-> B

        To:

            ----<->--- N* ---<->-----
            |                       |
            E <-> D <-> C <-> B <-> A

        This requires the following moves:

           C.next = D  (node.prev.next = node.next)
           D.prev = C  (node.next.prev = node.prev)
           E.next = N  (head.prev.next = node)
           N.prev = E  (node.prev = head.prev)
           N.next = A  (node.next = head)
           A.prev = N  (head.prev = node)
        """
        head = self._head
        # C.next = D
        node.prev.next = node.next
        # D.prev = C
        node.next.prev = node.prev
        # N.prev = E
        node.prev = head.prev
        # N.next = A
        # It is tempting to do just "head" here, however if node is
        # adjacent to head, this will do bad things.
        node.next = head.prev.next
        # E.next = N
        node.next.prev = node
        # A.prev = N
        node.prev.next = node

        self._head = node

    def _addcapacity(self):
        """Add a node to the circular linked list.

        The new node is inserted before the head node.
        """
        head = self._head
        node = _lrucachenode()
        head.prev.next = node
        node.prev = head.prev
        node.next = head
        head.prev = node
        self._size += 1
        return node

    def _enforcecostlimit(self):
        # This should run after an insertion. It should only be called if total
        # cost limits are being enforced.
        # The most recently inserted node is never evicted.
        if len(self) <= 1 or self.totalcost <= self.maxcost:
            return

        # This is logically equivalent to calling popoldest() until we
        # free up enough cost. We don't do that since popoldest() needs
        # to walk the linked list and doing this in a loop would be
        # quadratic. So we find the first non-empty node and then
        # walk nodes until we free up enough capacity.
        #
        # If we only removed the minimum number of nodes to free enough
        # cost at insert time, chances are high that the next insert would
        # also require pruning. This would effectively constitute quadratic
        # behavior for insert-heavy workloads. To mitigate this, we set a
        # target cost that is a percentage of the max cost. This will tend
        # to free more nodes when the high water mark is reached, which
        # lowers the chances of needing to prune on the subsequent insert.
        targetcost = int(self.maxcost * 0.75)

        n = self._head.prev
        while n.key is _notset:
            n = n.prev

        while len(self) > 1 and self.totalcost > targetcost:
            del self._cache[n.key]
            self.totalcost -= n.cost
            n.markempty()
            n = n.prev


def lrucachefunc(func):
    '''cache most recent results of function calls'''
    cache = {}
    order = collections.deque()
    if func.__code__.co_argcount == 1:

        def f(arg):
            if arg not in cache:
                if len(cache) > 20:
                    del cache[order.popleft()]
                cache[arg] = func(arg)
            else:
                order.remove(arg)
            order.append(arg)
            return cache[arg]

    else:

        def f(*args):
            if args not in cache:
                if len(cache) > 20:
                    del cache[order.popleft()]
                cache[args] = func(*args)
            else:
                order.remove(args)
            order.append(args)
            return cache[args]

    return f


class propertycache:
    def __init__(self, func):
        self.func = func
        self.name = func.__name__

    def __get__(self, obj, type=None):
        result = self.func(obj)
        self.cachevalue(obj, result)
        return result

    def cachevalue(self, obj, value):
        # __dict__ assignment required to bypass __setattr__ (eg: repoview)
        obj.__dict__[self.name] = value


def clearcachedproperty(obj, prop):
    '''clear a cached property value, if one has been set'''
    prop = pycompat.sysstr(prop)
    if prop in obj.__dict__:
        del obj.__dict__[prop]


def increasingchunks(source, min=1024, max=65536):
    """return no less than min bytes per chunk while data remains,
    doubling min after each chunk until it reaches max"""

    def log2(x):
        if not x:
            return 0
        i = 0
        while x:
            x >>= 1
            i += 1
        return i - 1

    buf = []
    blen = 0
    for chunk in source:
        buf.append(chunk)
        blen += len(chunk)
        if blen >= min:
            if min < max:
                min = min << 1
                nmin = 1 << log2(blen)
                if nmin > min:
                    min = nmin
                if min > max:
                    min = max
            yield b''.join(buf)
            blen = 0
            buf = []
    if buf:
        yield b''.join(buf)


def always(fn):
    return True


def never(fn):
    return False


def nogc(func):
    """disable garbage collector

    Python's garbage collector triggers a GC each time a certain number of
    container objects (the number being defined by gc.get_threshold()) are
    allocated even when marked not to be tracked by the collector. Tracking has
    no effect on when GCs are triggered, only on what objects the GC looks
    into. As a workaround, disable GC while building complex (huge)
    containers.

    This garbage collector issue have been fixed in 2.7. But it still affect
    CPython's performance.
    """

    def wrapper(*args, **kwargs):
        gcenabled = gc.isenabled()
        gc.disable()
        try:
            return func(*args, **kwargs)
        finally:
            if gcenabled:
                gc.enable()

    return wrapper


if pycompat.ispypy:
    # PyPy runs slower with gc disabled
    nogc = lambda x: x


def pathto(root, n1, n2):
    # type: (bytes, bytes, bytes) -> bytes
    """return the relative path from one place to another.
    root should use os.sep to separate directories
    n1 should use os.sep to separate directories
    n2 should use "/" to separate directories
    returns an os.sep-separated path.

    If n1 is a relative path, it's assumed it's
    relative to root.
    n2 should always be relative to root.
    """
    if not n1:
        return localpath(n2)
    if os.path.isabs(n1):
        if os.path.splitdrive(root)[0] != os.path.splitdrive(n1)[0]:
            return os.path.join(root, localpath(n2))
        n2 = b'/'.join((pconvert(root), n2))
    a, b = splitpath(n1), n2.split(b'/')
    a.reverse()
    b.reverse()
    while a and b and a[-1] == b[-1]:
        a.pop()
        b.pop()
    b.reverse()
    return pycompat.ossep.join(([b'..'] * len(a)) + b) or b'.'


def checksignature(func, depth=1):
    '''wrap a function with code to check for calling errors'''

    def check(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except TypeError:
            if len(traceback.extract_tb(sys.exc_info()[2])) == depth:
                raise error.SignatureError
            raise

    return check


# a whilelist of known filesystems where hardlink works reliably
_hardlinkfswhitelist = {
    b'apfs',
    b'btrfs',
    b'ext2',
    b'ext3',
