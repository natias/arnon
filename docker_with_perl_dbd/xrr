class filteredpropertycache(util.propertycache):
    """propertycache that must take filtering in account"""

    def cachevalue(self, obj, value):
        object.__setattr__(obj, self.name, value)


def hasunfilteredcache(repo, name):
    """check if a repo has an unfilteredpropertycache value for <name>"""
    return name in vars(repo.unfiltered())


def unfilteredmethod(orig):
    """decorate method that always need to be run on unfiltered version"""

    @functools.wraps(orig)
    def wrapper(repo, *args, **kwargs):
        return orig(repo.unfiltered(), *args, **kwargs)

    return wrapper


moderncaps = {
    b'lookup',
    b'branchmap',
    b'pushkey',
    b'known',
    b'getbundle',
    b'unbundle',
}
legacycaps = moderncaps.union({b'changegroupsubset'})


@interfaceutil.implementer(repository.ipeercommandexecutor)
class localcommandexecutor:
    def __init__(self, peer):
        self._peer = peer
        self._sent = False
        self._closed = False

    def __enter__(self):
        return self

    def __exit__(self, exctype, excvalue, exctb):
        self.close()

    def callcommand(self, command, args):
        if self._sent:
            raise error.ProgrammingError(
                b'callcommand() cannot be used after sendcommands()'
            )

        if self._closed:
            raise error.ProgrammingError(
                b'callcommand() cannot be used after close()'
            )

        # We don't need to support anything fancy. Just call the named
        # method on the peer and return a resolved future.
        fn = getattr(self._peer, pycompat.sysstr(command))

        f = futures.Future()

        try:
            result = fn(**pycompat.strkwargs(args))
        except Exception:
            pycompat.future_set_exception_info(f, sys.exc_info()[1:])
        else:
            f.set_result(result)

        return f

    def sendcommands(self):
        self._sent = True

    def close(self):
        self._closed = True


@interfaceutil.implementer(repository.ipeercommands)
class localpeer(repository.peer):
    '''peer for a local repo; reflects only the most recent API'''

    def __init__(self, repo, caps=None):
        super(localpeer, self).__init__()

        if caps is None:
            caps = moderncaps.copy()
        self._repo = repo.filtered(b'served')
        self.ui = repo.ui

        if repo._wanted_sidedata:
            formatted = bundle2.format_remote_wanted_sidedata(repo)
            caps.add(b'exp-wanted-sidedata=' + formatted)

        self._caps = repo._restrictcapabilities(caps)

    # Begin of _basepeer interface.

    def url(self):
        return self._repo.url()

    def local(self):
        return self._repo

    def peer(self):
        return self

    def canpush(self):
        return True

    def close(self):
        self._repo.close()

    # End of _basepeer interface.

    # Begin of _basewirecommands interface.

    def branchmap(self):
        return self._repo.branchmap()

    def capabilities(self):
        return self._caps

    def clonebundles(self):
        return self._repo.tryread(bundlecaches.CB_MANIFEST_FILE)

    def debugwireargs(self, one, two, three=None, four=None, five=None):
        """Used to test argument passing over the wire"""
        return b"%s %s %s %s %s" % (
            one,
            two,
            pycompat.bytestr(three),
            pycompat.bytestr(four),
            pycompat.bytestr(five),
        )

    def getbundle(
        self,
        source,
        heads=None,
        common=None,
        bundlecaps=None,
        remote_sidedata=None,
        **kwargs
    ):
        chunks = exchange.getbundlechunks(
            self._repo,
            source,
            heads=heads,
            common=common,
            bundlecaps=bundlecaps,
            remote_sidedata=remote_sidedata,
            **kwargs
        )[1]
        cb = util.chunkbuffer(chunks)

        if exchange.bundle2requested(bundlecaps):
            # When requesting a bundle2, getbundle returns a stream to make the
            # wire level function happier. We need to build a proper object
            # from it in local peer.
            return bundle2.getunbundler(self.ui, cb)
        else:
            return changegroup.getunbundler(b'01', cb, None)

    def heads(self):
        return self._repo.heads()

    def known(self, nodes):
        return self._repo.known(nodes)

    def listkeys(self, namespace):
        return self._repo.listkeys(namespace)

    def lookup(self, key):
        return self._repo.lookup(key)

    def pushkey(self, namespace, key, old, new):
        return self._repo.pushkey(namespace, key, old, new)

    def stream_out(self):
        raise error.Abort(_(b'cannot perform stream clone against local peer'))

    def unbundle(self, bundle, heads, url):
        """apply a bundle on a repo

        This function handles the repo locking itself."""
        try:
            try:
                bundle = exchange.readbundle(self.ui, bundle, None)
                ret = exchange.unbundle(self._repo, bundle, heads, b'push', url)
                if util.safehasattr(ret, b'getchunks'):
                    # This is a bundle20 object, turn it into an unbundler.
                    # This little dance should be dropped eventually when the
                    # API is finally improved.
                    stream = util.chunkbuffer(ret.getchunks())
                    ret = bundle2.getunbundler(self.ui, stream)
                return ret
            except Exception as exc:
                # If the exception contains output salvaged from a bundle2
                # reply, we need to make sure it is printed before continuing
                # to fail. So we build a bundle2 with such output and consume
                # it directly.
                #
                # This is not very elegant but allows a "simple" solution for
                # issue4594
                output = getattr(exc, '_bundle2salvagedoutput', ())
                if output:
                    bundler = bundle2.bundle20(self._repo.ui)
                    for out in output:
                        bundler.addpart(out)
                    stream = util.chunkbuffer(bundler.getchunks())
                    b = bundle2.getunbundler(self.ui, stream)
                    bundle2.processbundle(self._repo, b)
                raise
        except error.PushRaced as exc:
            raise error.ResponseError(
                _(b'push failed:'), stringutil.forcebytestr(exc)
            )

    # End of _basewirecommands interface.

    # Begin of peer interface.

    def commandexecutor(self):
        return localcommandexecutor(self)

    # End of peer interface.


@interfaceutil.implementer(repository.ipeerlegacycommands)
class locallegacypeer(localpeer):
    """peer extension which implements legacy methods too; used for tests with
    restricted capabilities"""

    def __init__(self, repo):
        super(locallegacypeer, self).__init__(repo, caps=legacycaps)

    # Begin of baselegacywirecommands interface.

    def between(self, pairs):
        return self._repo.between(pairs)

    def branches(self, nodes):
        return self._repo.branches(nodes)

    def changegroup(self, nodes, source):
        outgoing = discovery.outgoing(
            self._repo, missingroots=nodes, ancestorsof=self._repo.heads()
        )
        return changegroup.makechangegroup(self._repo, outgoing, b'01', source)

    def changegroupsubset(self, bases, heads, source):
        outgoing = discovery.outgoing(
            self._repo, missingroots=bases, ancestorsof=heads
        )
        return changegroup.makechangegroup(self._repo, outgoing, b'01', source)

    # End of baselegacywirecommands interface.


# Functions receiving (ui, features) that extensions can register to impact
# the ability to load repositories with custom requirements. Only
# functions defined in loaded extensions are called.
#
# The function receives a set of requirement strings that the repository
# is capable of opening. Functions will typically add elements to the
# set to reflect that the extension knows how to handle that requirements.
featuresetupfuncs = set()


def _getsharedvfs(hgvfs, requirements):
    """returns the vfs object pointing to root of shared source
    repo for a shared repository

    hgvfs is vfs pointing at .hg/ of current repo (shared one)
    requirements is a set of requirements of current repo (shared one)
    """
    # The ``shared`` or ``relshared`` requirements indicate the
    # store lives in the path contained in the ``.hg/sharedpath`` file.
    # This is an absolute path for ``shared`` and relative to
    # ``.hg/`` for ``relshared``.
    sharedpath = hgvfs.read(b'sharedpath').rstrip(b'\n')
    if requirementsmod.RELATIVE_SHARED_REQUIREMENT in requirements:
        sharedpath = util.normpath(hgvfs.join(sharedpath))

    sharedvfs = vfsmod.vfs(sharedpath, realpath=True)

    if not sharedvfs.exists():
        raise error.RepoError(
            _(b'.hg/sharedpath points to nonexistent directory %s')
            % sharedvfs.base
        )
    return sharedvfs


def _readrequires(vfs, allowmissing):
    """reads the require file present at root of this vfs
    and return a set of requirements

    If allowmissing is True, we suppress FileNotFoundError if raised"""
    # requires file contains a newline-delimited list of
    # features/capabilities the opener (us) must have in order to use
    # the repository. This file was introduced in Mercurial 0.9.2,
    # which means very old repositories may not have one. We assume
    # a missing file translates to no requirements.
    read = vfs.tryread if allowmissing else vfs.read
    return set(read(b'requires').splitlines())


def makelocalrepository(baseui, path, intents=None):
    """Create a local repository object.

    Given arguments needed to construct a local repository, this function
    performs various early repository loading functionality (such as
    reading the ``.hg/requires`` and ``.hg/hgrc`` files), validates that
    the repository can be opened, derives a type suitable for representing
    that repository, and returns an instance of it.

    The returned object conforms to the ``repository.completelocalrepository``
    interface.

    The repository type is derived by calling a series of factory functions
    for each aspect/interface of the final repository. These are defined by
    ``REPO_INTERFACES``.

    Each factory function is called to produce a type implementing a specific
    interface. The cumulative list of returned types will be combined into a
    new type and that type will be instantiated to represent the local
    repository.

    The factory functions each receive various state that may be consulted
    as part of deriving a type.

    Extensions should wrap these factory functions to customize repository type
    creation. Note that an extension's wrapped function may be called even if
    that extension is not loaded for the repo being constructed. Extensions
    should check if their ``__name__`` appears in the
    ``extensionmodulenames`` set passed to the factory function and no-op if
    not.
    """
    ui = baseui.copy()
    # Prevent copying repo configuration.
    ui.copy = baseui.copy

    # Working directory VFS rooted at repository root.
    wdirvfs = vfsmod.vfs(path, expandpath=True, realpath=True)

    # Main VFS for .hg/ directory.
    hgpath = wdirvfs.join(b'.hg')
    hgvfs = vfsmod.vfs(hgpath, cacheaudited=True)
    # Whether this repository is shared one or not
    shared = False
    # If this repository is shared, vfs pointing to shared repo
    sharedvfs = None

    # The .hg/ path should exist and should be a directory. All other
    # cases are errors.
    if not hgvfs.isdir():
        try:
            hgvfs.stat()
        except FileNotFoundError:
            pass
        except ValueError as e:
            # Can be raised on Python 3.8 when path is invalid.
            raise error.Abort(
                _(b'invalid path %s: %s') % (path, stringutil.forcebytestr(e))
            )

        raise error.RepoError(_(b'repository %s not found') % path)

    requirements = _readrequires(hgvfs, True)
    shared = (
        requirementsmod.SHARED_REQUIREMENT in requirements
        or requirementsmod.RELATIVE_SHARED_REQUIREMENT in requirements
    )
    storevfs = None
    if shared:
        # This is a shared repo
        sharedvfs = _getsharedvfs(hgvfs, requirements)
        storevfs = vfsmod.vfs(sharedvfs.join(b'store'))
    else:
        storevfs = vfsmod.vfs(hgvfs.join(b'store'))

    # if .hg/requires contains the sharesafe requirement, it means
    # there exists a `.hg/store/requires` too and we should read it
    # NOTE: presence of SHARESAFE_REQUIREMENT imply that store requirement
    # is present. We never write SHARESAFE_REQUIREMENT for a repo if store
    # is not present, refer checkrequirementscompat() for that
    #
    # However, if SHARESAFE_REQUIREMENT is not present, it means that the
    # repository was shared the old way. We check the share source .hg/requires
    # for SHARESAFE_REQUIREMENT to detect whether the current repository needs
    # to be reshared
    hint = _(b"see `hg help config.format.use-share-safe` for more information")
    if requirementsmod.SHARESAFE_REQUIREMENT in requirements:

        if (
            shared
            and requirementsmod.SHARESAFE_REQUIREMENT
            not in _readrequires(sharedvfs, True)
        ):
            mismatch_warn = ui.configbool(
                b'share', b'safe-mismatch.source-not-safe.warn'
            )
            mismatch_config = ui.config(
                b'share', b'safe-mismatch.source-not-safe'
            )
            mismatch_verbose_upgrade = ui.configbool(
                b'share', b'safe-mismatch.source-not-safe:verbose-upgrade'
            )
            if mismatch_config in (
                b'downgrade-allow',
                b'allow',
                b'downgrade-abort',
            ):
                # prevent cyclic import localrepo -> upgrade -> localrepo
                from . import upgrade

                upgrade.downgrade_share_to_non_safe(
                    ui,
                    hgvfs,
                    sharedvfs,
                    requirements,
                    mismatch_config,
                    mismatch_warn,
                    mismatch_verbose_upgrade,
                )
            elif mismatch_config == b'abort':
                raise error.Abort(
                    _(b"share source does not support share-safe requirement"),
                    hint=hint,
                )
            else:
                raise error.Abort(
                    _(
                        b"share-safe mismatch with source.\nUnrecognized"
                        b" value '%s' of `share.safe-mismatch.source-not-safe`"
                        b" set."
                    )
                    % mismatch_config,
                    hint=hint,
                )
        else:
            requirements |= _readrequires(storevfs, False)
    elif shared:
        sourcerequires = _readrequires(sharedvfs, False)
        if requirementsmod.SHARESAFE_REQUIREMENT in sourcerequires:
            mismatch_config = ui.config(b'share', b'safe-mismatch.source-safe')
            mismatch_warn = ui.configbool(
                b'share', b'safe-mismatch.source-safe.warn'
            )
            mismatch_verbose_upgrade = ui.configbool(
                b'share', b'safe-mismatch.source-safe:verbose-upgrade'
            )
            if mismatch_config in (
                b'upgrade-allow',
                b'allow',
                b'upgrade-abort',
            ):
                # prevent cyclic import localrepo -> upgrade -> localrepo
                from . import upgrade

                upgrade.upgrade_share_to_safe(
                    ui,
                    hgvfs,
                    storevfs,
                    requirements,
                    mismatch_config,
                    mismatch_warn,
                    mismatch_verbose_upgrade,
                )
            elif mismatch_config == b'abort':
                raise error.Abort(
                    _(
                        b'version mismatch: source uses share-safe'
                        b' functionality while the current share does not'
                    ),
                    hint=hint,
                )
            else:
                raise error.Abort(
                    _(
                        b"share-safe mismatch with source.\nUnrecognized"
                        b" value '%s' of `share.safe-mismatch.source-safe` set."
                    )
                    % mismatch_config,
                    hint=hint,
                )

    # The .hg/hgrc file may load extensions or contain config options
    # that influence repository construction. Attempt to load it and
    # process any new extensions that it may have pulled in.
    if loadhgrc(ui, wdirvfs, hgvfs, requirements, sharedvfs):
        afterhgrcload(ui, wdirvfs, hgvfs, requirements)
        extensions.loadall(ui)
        extensions.populateui(ui)

    # Set of module names of extensions loaded for this repository.
    extensionmodulenames = {m.__name__ for n, m in extensions.extensions(ui)}

    supportedrequirements = gathersupportedrequirements(ui)

    # We first validate the requirements are known.
    ensurerequirementsrecognized(requirements, supportedrequirements)

    # Then we validate that the known set is reasonable to use together.
    ensurerequirementscompatible(ui, requirements)

    # TODO there are unhandled edge cases related to opening repositories with
    # shared storage. If storage is shared, we should also test for requirements
    # compatibility in the pointed-to repo. This entails loading the .hg/hgrc in
    # that repo, as that repo may load extensions needed to open it. This is a
    # bit complicated because we don't want the other hgrc to overwrite settings
    # in this hgrc.
    #
    # This bug is somewhat mitigated by the fact that we copy the .hg/requires
    # file when sharing repos. But if a requirement is added after the share is
    # performed, thereby introducing a new requirement for the opener, we may
    # will not see that and could encounter a run-time error interacting with
    # that shared store since it has an unknown-to-us requirement.

    # At this point, we know we should be capable of opening the repository.
    # Now get on with doing that.

    features = set()

    # The "store" part of the repository holds versioned data. How it is
    # accessed is determined by various requirements. If `shared` or
    # `relshared` requirements are present, this indicates current repository
    # is a share and store exists in path mentioned in `.hg/sharedpath`
    if shared:
        storebasepath = sharedvfs.base
        cachepath = sharedvfs.join(b'cache')
        features.add(repository.REPO_FEATURE_SHARED_STORAGE)
    else:
        storebasepath = hgvfs.base
        cachepath = hgvfs.join(b'cache')
    wcachepath = hgvfs.join(b'wcache')

    # The store has changed over time and the exact layout is dictated by
    # requirements. The store interface abstracts differences across all
    # of them.
    store = makestore(
        requirements,
        storebasepath,
        lambda base: vfsmod.vfs(base, cacheaudited=True),
    )
    hgvfs.createmode = store.createmode

    storevfs = store.vfs
    storevfs.options = resolvestorevfsoptions(ui, requirements, features)

    if (
        requirementsmod.REVLOGV2_REQUIREMENT in requirements
        or requirementsmod.CHANGELOGV2_REQUIREMENT in requirements
    ):
        features.add(repository.REPO_FEATURE_SIDE_DATA)
        # the revlogv2 docket introduced race condition that we need to fix
        features.discard(repository.REPO_FEATURE_STREAM_CLONE)

    # The cache vfs is used to manage cache files.
    cachevfs = vfsmod.vfs(cachepath, cacheaudited=True)
    cachevfs.createmode = store.createmode
    # The cache vfs is used to manage cache files related to the working copy
    wcachevfs = vfsmod.vfs(wcachepath, cacheaudited=True)
    wcachevfs.createmode = store.createmode

    # Now resolve the type for the repository object. We do this by repeatedly
    # calling a factory function to produces types for specific aspects of the
    # repo's operation. The aggregate returned types are used as base classes
    # for a dynamically-derived type, which will represent our new repository.

    bases = []
    extrastate = {}

    for iface, fn in REPO_INTERFACES:
        # We pass all potentially useful state to give extensions tons of
        # flexibility.
        typ = fn()(
            ui=ui,
            intents=intents,
            requirements=requirements,
            features=features,
            wdirvfs=wdirvfs,
            hgvfs=hgvfs,
            store=store,
            storevfs=storevfs,
            storeoptions=storevfs.options,
            cachevfs=cachevfs,
            wcachevfs=wcachevfs,
            extensionmodulenames=extensionmodulenames,
            extrastate=extrastate,
            baseclasses=bases,
        )

        if not isinstance(typ, type):
            raise error.ProgrammingError(
                b'unable to construct type for %s' % iface
            )

        bases.append(typ)

    # type() allows you to use characters in type names that wouldn't be
    # recognized as Python symbols in source code. We abuse that to add
    # rich information about our constructed repo.
    name = pycompat.sysstr(
        b'derivedrepo:%s<%s>' % (wdirvfs.base, b','.join(sorted(requirements)))
    )

    cls = type(name, tuple(bases), {})

    return cls(
        baseui=baseui,
        ui=ui,
        origroot=path,
        wdirvfs=wdirvfs,
        hgvfs=hgvfs,
        requirements=requirements,
        supportedrequirements=supportedrequirements,
        sharedpath=storebasepath,
        store=store,
        cachevfs=cachevfs,
        wcachevfs=wcachevfs,
        features=features,
        intents=intents,
    )


def loadhgrc(ui, wdirvfs, hgvfs, requirements, sharedvfs=None):
    """Load hgrc files/content into a ui instance.

    This is called during repository opening to load any additional
    config files or settings relevant to the current repository.

    Returns a bool indicating whether any additional configs were loaded.

    Extensions should monkeypatch this function to modify how per-repo
    configs are loaded. For example, an extension may wish to pull in
    configs from alternate files or sources.

    sharedvfs is vfs object pointing to source repo if the current one is a
    shared one
    """
    if not rcutil.use_repo_hgrc():
        return False

    ret = False
    # first load config from shared source if we has to
    if requirementsmod.SHARESAFE_REQUIREMENT in requirements and sharedvfs:
        try:
            ui.readconfig(sharedvfs.join(b'hgrc'), root=sharedvfs.base)
            ret = True
        except IOError:
            pass

    try:
        ui.readconfig(hgvfs.join(b'hgrc'), root=wdirvfs.base)
        ret = True
    except IOError:
        pass

    try:
        ui.readconfig(hgvfs.join(b'hgrc-not-shared'), root=wdirvfs.base)
        ret = True
    except IOError:
        pass

    return ret


def afterhgrcload(ui, wdirvfs, hgvfs, requirements):
    """Perform additional actions after .hg/hgrc is loaded.

    This function is called during repository loading immediately after
    the .hg/hgrc file is loaded and before per-repo extensions are loaded.

    The function can be used to validate configs, automatically add
    options (including extensions) based on requirements, etc.
    """

    # Map of requirements to list of extensions to load automatically when
    # requirement is present.
    autoextensions = {
        b'git': [b'git'],
        b'largefiles': [b'largefiles'],
        b'lfs': [b'lfs'],
    }

    for requirement, names in sorted(autoextensions.items()):
        if requirement not in requirements:
            continue

        for name in names:
            if not ui.hasconfig(b'extensions', name):
                ui.setconfig(b'extensions', name, b'', source=b'autoload')


def gathersupportedrequirements(ui):
    """Determine the complete set of recognized requirements."""
    # Start with all requirements supported by this file.
    supported = set(localrepository._basesupported)

    # Execute ``featuresetupfuncs`` entries if they belong to an extension
    # relevant to this ui instance.
    modules = {m.__name__ for n, m in extensions.extensions(ui)}

    for fn in featuresetupfuncs:
        if fn.__module__ in modules:
            fn(ui, supported)

    # Add derived requirements from registered compression engines.
    for name in util.compengines:
        engine = util.compengines[name]
        if engine.available() and engine.revlogheader():
            supported.add(b'exp-compression-%s' % name)
            if engine.name() == b'zstd':
                supported.add(requirementsmod.REVLOG_COMPRESSION_ZSTD)

    return supported


def ensurerequirementsrecognized(requirements, supported):
    """Validate that a set of local requirements is recognized.

    Receives a set of requirements. Raises an ``error.RepoError`` if there
    exists any requirement in that set that currently loaded code doesn't
    recognize.

    Returns a set of supported requirements.
    """
    missing = set()

    for requirement in requirements:
        if requirement in supported:
            continue

        if not requirement or not requirement[0:1].isalnum():
            raise error.RequirementError(_(b'.hg/requires file is corrupt'))

        missing.add(requirement)

    if missing:
        raise error.RequirementError(
            _(b'repository requires features unknown to this Mercurial: %s')
            % b' '.join(sorted(missing)),
            hint=_(
                b'see https://mercurial-scm.org/wiki/MissingRequirement '
                b'for more information'
            ),
        )


def ensurerequirementscompatible(ui, requirements):
    """Validates that a set of recognized requirements is mutually compatible.

    Some requirements may not be compatible with others or require
    config options that aren't enabled. This function is called during
    repository opening to ensure that the set of requirements needed
    to open a repository is sane and compatible with config options.

    Extensions can monkeypatch this function to perform additional
    checking.

    ``error.RepoError`` should be raised on failure.
    """
    if (
        requirementsmod.SPARSE_REQUIREMENT in requirements
        and not sparse.enabled
    ):
        raise error.RepoError(
            _(
                b'repository is using sparse feature but '
                b'sparse is not enabled; enable the '
                b'"sparse" extensions to access'
            )
        )


def makestore(requirements, path, vfstype):
    """Construct a storage object for a repository."""
    if requirementsmod.STORE_REQUIREMENT in requirements:
        if requirementsmod.FNCACHE_REQUIREMENT in requirements:
            dotencode = requirementsmod.DOTENCODE_REQUIREMENT in requirements
            return storemod.fncachestore(path, vfstype, dotencode)

        return storemod.encodedstore(path, vfstype)

    return storemod.basicstore(path, vfstype)


def resolvestorevfsoptions(ui, requirements, features):
    """Resolve the options to pass to the store vfs opener.

    The returned dict is used to influence behavior of the storage layer.
    """
    options = {}

    if requirementsmod.TREEMANIFEST_REQUIREMENT in requirements:
        options[b'treemanifest'] = True

    # experimental config: format.manifestcachesize
    manifestcachesize = ui.configint(b'format', b'manifestcachesize')
    if manifestcachesize is not None:
        options[b'manifestcachesize'] = manifestcachesize

    # In the absence of another requirement superseding a revlog-related
    # requirement, we have to assume the repo is using revlog version 0.
    # This revlog format is super old and we don't bother trying to parse
    # opener options for it because those options wouldn't do anything
    # meaningful on such old repos.
    if (
        requirementsmod.REVLOGV1_REQUIREMENT in requirements
        or requirementsmod.REVLOGV2_REQUIREMENT in requirements
    ):
        options.update(resolverevlogstorevfsoptions(ui, requirements, features))
    else:  # explicitly mark repo as using revlogv0
        options[b'revlogv0'] = True

    if requirementsmod.COPIESSDC_REQUIREMENT in requirements:
        options[b'copies-storage'] = b'changeset-sidedata'
    else:
        writecopiesto = ui.config(b'experimental', b'copies.write-to')
        copiesextramode = (b'changeset-only', b'compatibility')
        if writecopiesto in copiesextramode:
            options[b'copies-storage'] = b'extra'

    return options


def resolverevlogstorevfsoptions(ui, requirements, features):
    """Resolve opener options specific to revlogs."""

    options = {}
    options[b'flagprocessors'] = {}

    if requirementsmod.REVLOGV1_REQUIREMENT in requirements:
        options[b'revlogv1'] = True
    if requirementsmod.REVLOGV2_REQUIREMENT in requirements:
        options[b'revlogv2'] = True
    if requirementsmod.CHANGELOGV2_REQUIREMENT in requirements:
        options[b'changelogv2'] = True

    if requirementsmod.GENERALDELTA_REQUIREMENT in requirements:
        options[b'generaldelta'] = True

    # experimental config: format.chunkcachesize
    chunkcachesize = ui.configint(b'format', b'chunkcachesize')
    if chunkcachesize is not None:
        options[b'chunkcachesize'] = chunkcachesize

    deltabothparents = ui.configbool(
        b'storage', b'revlog.optimize-delta-parent-choice'
    )
    options[b'deltabothparents'] = deltabothparents
    options[b'debug-delta'] = ui.configbool(b'debug', b'revlog.debug-delta')

    issue6528 = ui.configbool(b'storage', b'revlog.issue6528.fix-incoming')
    options[b'issue6528.fix-incoming'] = issue6528

    lazydelta = ui.configbool(b'storage', b'revlog.reuse-external-delta')
    lazydeltabase = False
    if lazydelta:
        lazydeltabase = ui.configbool(
            b'storage', b'revlog.reuse-external-delta-parent'
        )
    if lazydeltabase is None:
        lazydeltabase = not scmutil.gddeltaconfig(ui)
    options[b'lazydelta'] = lazydelta
    options[b'lazydeltabase'] = lazydeltabase

    chainspan = ui.configbytes(b'experimental', b'maxdeltachainspan')
    if 0 <= chainspan:
        options[b'maxdeltachainspan'] = chainspan

    mmapindexthreshold = ui.configbytes(b'experimental', b'mmapindexthreshold')
    if mmapindexthreshold is not None:
        options[b'mmapindexthreshold'] = mmapindexthreshold

    withsparseread = ui.configbool(b'experimental', b'sparse-read')
    srdensitythres = float(
        ui.config(b'experimental', b'sparse-read.density-threshold')
    )
    srmingapsize = ui.configbytes(b'experimental', b'sparse-read.min-gap-size')
    options[b'with-sparse-read'] = withsparseread
    options[b'sparse-read-density-threshold'] = srdensitythres
    options[b'sparse-read-min-gap-size'] = srmingapsize

    sparserevlog = requirementsmod.SPARSEREVLOG_REQUIREMENT in requirements
    options[b'sparse-revlog'] = sparserevlog
    if sparserevlog:
        options[b'generaldelta'] = True

    maxchainlen = None
    if sparserevlog:
        maxchainlen = revlogconst.SPARSE_REVLOG_MAX_CHAIN_LENGTH
    # experimental config: format.maxchainlen
    maxchainlen = ui.configint(b'format', b'maxchainlen', maxchainlen)
    if maxchainlen is not None:
        options[b'maxchainlen'] = maxchainlen

    for r in requirements:
        # we allow multiple compression engine requirement to co-exist because
        # strickly speaking, revlog seems to support mixed compression style.
        #
        # The compression used for new entries will be "the last one"
        prefix = r.startswith
        if prefix(b'revlog-compression-') or prefix(b'exp-compression-'):
            options[b'compengine'] = r.split(b'-', 2)[2]

    options[b'zlib.level'] = ui.configint(b'storage', b'revlog.zlib.level')
    if options[b'zlib.level'] is not None:
        if not (0 <= options[b'zlib.level'] <= 9):
            msg = _(b'invalid value for `storage.revlog.zlib.level` config: %d')
            raise error.Abort(msg % options[b'zlib.level'])
    options[b'zstd.level'] = ui.configint(b'storage', b'revlog.zstd.level')
    if options[b'zstd.level'] is not None:
        if not (0 <= options[b'zstd.level'] <= 22):
            msg = _(b'invalid value for `storage.revlog.zstd.level` config: %d')
            raise error.Abort(msg % options[b'zstd.level'])

    if requirementsmod.NARROW_REQUIREMENT in requirements:
        options[b'enableellipsis'] = True

    if ui.configbool(b'experimental', b'rust.index'):
        options[b'rust.index'] = True
    if requirementsmod.NODEMAP_REQUIREMENT in requirements:
        slow_path = ui.config(
            b'storage', b'revlog.persistent-nodemap.slow-path'
        )
        if slow_path not in (b'allow', b'warn', b'abort'):
            default = ui.config_default(
                b'storage', b'revlog.persistent-nodemap.slow-path'
            )
            msg = _(
                b'unknown value for config '
                b'"storage.revlog.persistent-nodemap.slow-path": "%s"\n'
            )
            ui.warn(msg % slow_path)
            if not ui.quiet:
                ui.warn(_(b'falling back to default value: %s\n') % default)
            slow_path = default

        msg = _(
            b"accessing `persistent-nodemap` repository without associated "
            b"fast implementation."
        )
        hint = _(
            b"check `hg help config.format.use-persistent-nodemap` "
            b"for details"
        )
        if not revlog.HAS_FAST_PERSISTENT_NODEMAP:
            if slow_path == b'warn':
                msg = b"warning: " + msg + b'\n'
                ui.warn(msg)
                if not ui.quiet:
                    hint = b'(' + hint + b')\n'
                    ui.warn(hint)
            if slow_path == b'abort':
                raise error.Abort(msg, hint=hint)
        options[b'persistent-nodemap'] = True
    if requirementsmod.DIRSTATE_V2_REQUIREMENT in requirements:
        slow_path = ui.config(b'storage', b'dirstate-v2.slow-path')
        if slow_path not in (b'allow', b'warn', b'abort'):
            default = ui.config_default(b'storage', b'dirstate-v2.slow-path')
            msg = _(b'unknown value for config "dirstate-v2.slow-path": "%s"\n')
            ui.warn(msg % slow_path)
            if not ui.quiet:
                ui.warn(_(b'falling back to default value: %s\n') % default)
            slow_path = default

        msg = _(
            b"accessing `dirstate-v2` repository without associated "
            b"fast implementation."
        )
        hint = _(
            b"check `hg help config.format.use-dirstate-v2` " b"for details"
        )
        if not dirstate.HAS_FAST_DIRSTATE_V2:
            if slow_path == b'warn':
                msg = b"warning: " + msg + b'\n'
                ui.warn(msg)
                if not ui.quiet:
                    hint = b'(' + hint + b')\n'
                    ui.warn(hint)
            if slow_path == b'abort':
                raise error.Abort(msg, hint=hint)
    if ui.configbool(b'storage', b'revlog.persistent-nodemap.mmap'):
        options[b'persistent-nodemap.mmap'] = True
    if ui.configbool(b'devel', b'persistent-nodemap'):
        options[b'devel-force-nodemap'] = True

    return options


def makemain(**kwargs):
    """Produce a type conforming to ``ilocalrepositorymain``."""
    return localrepository


