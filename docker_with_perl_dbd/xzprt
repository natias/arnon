extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtu64_sh (__m128h __A, unsigned long long __B)
{
  return __builtin_ia32_vcvtusi2sh64_round (__A, __B, _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundi64_sh (__m128h __A, long long __B, const int __R)
{
  return __builtin_ia32_vcvtsi2sh64_round (__A, __B, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundu64_sh (__m128h __A, unsigned long long __B, const int __R)
{
  return __builtin_ia32_vcvtusi2sh64_round (__A, __B, __R);
}

#else
#define _mm_cvt_roundi64_sh(A, B, C)		\
  (__builtin_ia32_vcvtsi2sh64_round ((A), (B), (C)))
#define _mm_cvt_roundu64_sh(A, B, C)		\
  (__builtin_ia32_vcvtusi2sh64_round ((A), (B), (C)))

#endif /* __OPTIMIZE__ */
#endif /* __x86_64__ */

/* Intrinsics vcvtph2pd.  */
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtph_pd (__m128h __A)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__A,
						 _mm512_setzero_pd (),
						 (__mmask8) -1,
						 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtph_pd (__m512d __A, __mmask8 __B, __m128h __C)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__C, __A, __B,
						 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtph_pd (__mmask8 __A, __m128h __B)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__B,
						 _mm512_setzero_pd (),
						 __A,
						 _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundph_pd (__m128h __A, int __B)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__A,
						 _mm512_setzero_pd (),
						 (__mmask8) -1,
						 __B);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundph_pd (__m512d __A, __mmask8 __B, __m128h __C, int __D)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__C, __A, __B, __D);
}

extern __inline __m512d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundph_pd (__mmask8 __A, __m128h __B, int __C)
{
  return __builtin_ia32_vcvtph2pd512_mask_round (__B,
						 _mm512_setzero_pd (),
						 __A,
						 __C);
}

#else
#define _mm512_cvt_roundph_pd(A, B)					\
  (__builtin_ia32_vcvtph2pd512_mask_round ((A),			\
					   _mm512_setzero_pd (),	\
					   (__mmask8)-1,		\
					   (B)))

#define _mm512_mask_cvt_roundph_pd(A, B, C, D)				\
  (__builtin_ia32_vcvtph2pd512_mask_round ((C), (A), (B), (D)))

#define _mm512_maskz_cvt_roundph_pd(A, B, C)				\
  (__builtin_ia32_vcvtph2pd512_mask_round ((B),			\
					   _mm512_setzero_pd (),	\
					   (A),			\
					   (C)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vcvtph2psx.  */
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtxph_ps (__m256h __A)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__A,
						  _mm512_setzero_ps (),
						  (__mmask16) -1,
						  _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtxph_ps (__m512 __A, __mmask16 __B, __m256h __C)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__C, __A, __B,
						  _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtxph_ps (__mmask16 __A, __m256h __B)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__B,
						  _mm512_setzero_ps (),
						  __A,
						  _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtx_roundph_ps (__m256h __A, int __B)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__A,
						  _mm512_setzero_ps (),
						  (__mmask16) -1,
						  __B);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtx_roundph_ps (__m512 __A, __mmask16 __B, __m256h __C, int __D)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__C, __A, __B, __D);
}

extern __inline __m512
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtx_roundph_ps (__mmask16 __A, __m256h __B, int __C)
{
  return __builtin_ia32_vcvtph2psx512_mask_round (__B,
						  _mm512_setzero_ps (),
						  __A,
						  __C);
}

#else
#define _mm512_cvtx_roundph_ps(A, B)					\
  (__builtin_ia32_vcvtph2psx512_mask_round ((A),			\
					    _mm512_setzero_ps (),	\
					    (__mmask16)-1,		\
					    (B)))

#define _mm512_mask_cvtx_roundph_ps(A, B, C, D)				\
  (__builtin_ia32_vcvtph2psx512_mask_round ((C), (A), (B), (D)))

#define _mm512_maskz_cvtx_roundph_ps(A, B, C)				\
  (__builtin_ia32_vcvtph2psx512_mask_round ((B),			\
					    _mm512_setzero_ps (),	\
					    (A),			\
					    (C)))
#endif /* __OPTIMIZE__ */

/* Intrinsics vcvtps2ph.  */
extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtxps_ph (__m512 __A)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __A,
						  _mm256_setzero_ph (),
						  (__mmask16) -1,
						  _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtxps_ph (__m256h __A, __mmask16 __B, __m512 __C)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __C,
						  __A, __B,
						  _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtxps_ph (__mmask16 __A, __m512 __B)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __B,
						  _mm256_setzero_ph (),
						  __A,
						  _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtx_roundps_ph (__m512 __A, int __B)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __A,
						  _mm256_setzero_ph (),
						  (__mmask16) -1,
						  __B);
}

extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtx_roundps_ph (__m256h __A, __mmask16 __B, __m512 __C, int __D)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __C,
						  __A, __B, __D);
}

extern __inline __m256h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtx_roundps_ph (__mmask16 __A, __m512 __B, int __C)
{
  return __builtin_ia32_vcvtps2phx512_mask_round ((__v16sf) __B,
						  _mm256_setzero_ph (),
						  __A, __C);
}

#else
#define _mm512_cvtx_roundps_ph(A, B)				\
  (__builtin_ia32_vcvtps2phx512_mask_round ((__v16sf)(A),	\
					    _mm256_setzero_ph (),\
					    (__mmask16)-1, (B)))

#define _mm512_mask_cvtx_roundps_ph(A, B, C, D)			\
  (__builtin_ia32_vcvtps2phx512_mask_round ((__v16sf)(C),	\
					    (A), (B), (D)))

#define _mm512_maskz_cvtx_roundps_ph(A, B, C)			\
  (__builtin_ia32_vcvtps2phx512_mask_round ((__v16sf)(B),	\
					    _mm256_setzero_ph (),\
					    (A), (C)))
#endif /* __OPTIMIZE__ */

/* Intrinsics vcvtpd2ph.  */
extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvtpd_ph (__m512d __A)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __A,
						 _mm_setzero_ph (),
						 (__mmask8) -1,
						 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvtpd_ph (__m128h __A, __mmask8 __B, __m512d __C)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __C,
						 __A, __B,
						 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvtpd_ph (__mmask8 __A, __m512d __B)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __B,
						 _mm_setzero_ph (),
						 __A,
						 _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_cvt_roundpd_ph (__m512d __A, int __B)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __A,
						 _mm_setzero_ph (),
						 (__mmask8) -1,
						 __B);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_cvt_roundpd_ph (__m128h __A, __mmask8 __B, __m512d __C, int __D)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __C,
						 __A, __B, __D);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_cvt_roundpd_ph (__mmask8 __A, __m512d __B, int __C)
{
  return __builtin_ia32_vcvtpd2ph512_mask_round ((__v8df) __B,
						 _mm_setzero_ph (),
						 __A, __C);
}

#else
#define _mm512_cvt_roundpd_ph(A, B)				\
  (__builtin_ia32_vcvtpd2ph512_mask_round ((__v8df)(A),		\
					   _mm_setzero_ph (),	\
					   (__mmask8)-1, (B)))

#define _mm512_mask_cvt_roundpd_ph(A, B, C, D)			\
  (__builtin_ia32_vcvtpd2ph512_mask_round ((__v8df)(C),		\
					   (A), (B), (D)))

#define _mm512_maskz_cvt_roundpd_ph(A, B, C)			\
  (__builtin_ia32_vcvtpd2ph512_mask_round ((__v8df)(B),		\
					   _mm_setzero_ph (),	\
					   (A), (C)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vcvtsh2ss, vcvtsh2sd.  */
extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsh_ss (__m128 __A, __m128h __B)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__B, __A,
					      _mm_setzero_ps (),
					      (__mmask8) -1,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsh_ss (__m128 __A, __mmask8 __B, __m128 __C,
			 __m128h __D)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__D, __C, __A, __B,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsh_ss (__mmask8 __A, __m128 __B,
			  __m128h __C)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__C, __B,
					      _mm_setzero_ps (),
					      __A, _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsh_sd (__m128d __A, __m128h __B)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__B, __A,
					      _mm_setzero_pd (),
					      (__mmask8) -1,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsh_sd (__m128d __A, __mmask8 __B, __m128d __C,
			 __m128h __D)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__D, __C, __A, __B,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsh_sd (__mmask8 __A, __m128d __B, __m128h __C)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__C, __B,
					      _mm_setzero_pd (),
					      __A, _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsh_ss (__m128 __A, __m128h __B, const int __R)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__B, __A,
					      _mm_setzero_ps (),
					      (__mmask8) -1, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvt_roundsh_ss (__m128 __A, __mmask8 __B, __m128 __C,
			 __m128h __D, const int __R)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__D, __C, __A, __B, __R);
}

extern __inline __m128
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvt_roundsh_ss (__mmask8 __A, __m128 __B,
			  __m128h __C, const int __R)
{
  return __builtin_ia32_vcvtsh2ss_mask_round (__C, __B,
					      _mm_setzero_ps (),
					      __A, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsh_sd (__m128d __A, __m128h __B, const int __R)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__B, __A,
					      _mm_setzero_pd (),
					      (__mmask8) -1, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvt_roundsh_sd (__m128d __A, __mmask8 __B, __m128d __C,
			 __m128h __D, const int __R)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__D, __C, __A, __B, __R);
}

extern __inline __m128d
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvt_roundsh_sd (__mmask8 __A, __m128d __B, __m128h __C, const int __R)
{
  return __builtin_ia32_vcvtsh2sd_mask_round (__C, __B,
					      _mm_setzero_pd (),
					      __A, __R);
}

#else
#define _mm_cvt_roundsh_ss(A, B, R)				\
  (__builtin_ia32_vcvtsh2ss_mask_round ((B), (A),		\
					_mm_setzero_ps (),	\
					(__mmask8) -1, (R)))

#define _mm_mask_cvt_roundsh_ss(A, B, C, D, R)				\
  (__builtin_ia32_vcvtsh2ss_mask_round ((D), (C), (A), (B), (R)))

#define _mm_maskz_cvt_roundsh_ss(A, B, C, R)			\
  (__builtin_ia32_vcvtsh2ss_mask_round ((C), (B),		\
					_mm_setzero_ps (),	\
					(A), (R)))

#define _mm_cvt_roundsh_sd(A, B, R)				\
  (__builtin_ia32_vcvtsh2sd_mask_round ((B), (A),		\
					_mm_setzero_pd (),	\
					(__mmask8) -1, (R)))

#define _mm_mask_cvt_roundsh_sd(A, B, C, D, R)				\
  (__builtin_ia32_vcvtsh2sd_mask_round ((D), (C), (A), (B), (R)))

#define _mm_maskz_cvt_roundsh_sd(A, B, C, R)			\
  (__builtin_ia32_vcvtsh2sd_mask_round ((C), (B),		\
					_mm_setzero_pd (),	\
					(A), (R)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vcvtss2sh, vcvtsd2sh.  */
extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtss_sh (__m128h __A, __m128 __B)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__B, __A,
					      _mm_setzero_ph (),
					      (__mmask8) -1,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtss_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128 __D)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__D, __C, __A, __B,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtss_sh (__mmask8 __A, __m128h __B, __m128 __C)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__C, __B,
					      _mm_setzero_ph (),
					      __A, _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvtsd_sh (__m128h __A, __m128d __B)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__B, __A,
					      _mm_setzero_ph (),
					      (__mmask8) -1,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvtsd_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128d __D)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__D, __C, __A, __B,
					      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvtsd_sh (__mmask8 __A, __m128h __B, __m128d __C)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__C, __B,
					      _mm_setzero_ph (),
					      __A, _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundss_sh (__m128h __A, __m128 __B, const int __R)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__B, __A,
					      _mm_setzero_ph (),
					      (__mmask8) -1, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvt_roundss_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128 __D,
			 const int __R)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__D, __C, __A, __B, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvt_roundss_sh (__mmask8 __A, __m128h __B, __m128 __C,
			  const int __R)
{
  return __builtin_ia32_vcvtss2sh_mask_round (__C, __B,
					      _mm_setzero_ph (),
					      __A, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_cvt_roundsd_sh (__m128h __A, __m128d __B, const int __R)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__B, __A,
					      _mm_setzero_ph (),
					      (__mmask8) -1, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_mask_cvt_roundsd_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128d __D,
			 const int __R)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__D, __C, __A, __B, __R);
}

extern __inline __m128h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm_maskz_cvt_roundsd_sh (__mmask8 __A, __m128h __B, __m128d __C,
			  const int __R)
{
  return __builtin_ia32_vcvtsd2sh_mask_round (__C, __B,
					      _mm_setzero_ph (),
					      __A, __R);
}

#else
#define _mm_cvt_roundss_sh(A, B, R)				\
  (__builtin_ia32_vcvtss2sh_mask_round ((B), (A),		\
					_mm_setzero_ph (),	\
					(__mmask8) -1, R))

#define _mm_mask_cvt_roundss_sh(A, B, C, D, R)				\
  (__builtin_ia32_vcvtss2sh_mask_round ((D), (C), (A), (B), (R)))

#define _mm_maskz_cvt_roundss_sh(A, B, C, R)			\
  (__builtin_ia32_vcvtss2sh_mask_round ((C), (B),		\
					_mm_setzero_ph (),	\
					A, R))

#define _mm_cvt_roundsd_sh(A, B, R)				\
  (__builtin_ia32_vcvtsd2sh_mask_round ((B), (A),		\
					_mm_setzero_ph (),	\
					(__mmask8) -1, R))

#define _mm_mask_cvt_roundsd_sh(A, B, C, D, R)				\
  (__builtin_ia32_vcvtsd2sh_mask_round ((D), (C), (A), (B), (R)))

#define _mm_maskz_cvt_roundsd_sh(A, B, C, R)			\
  (__builtin_ia32_vcvtsd2sh_mask_round ((C), (B),		\
					_mm_setzero_ph (),	\
					(A), (R)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vfmaddsub[132,213,231]ph.  */
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_ph (__m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) -1,
					_MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_ph (__m512h __A, __mmask32 __U, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) __U,
					_MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_ph (__m512h __A, __m512h __B, __m512h __C, __mmask32 __U)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask3 ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U,
					 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_ph (__mmask32 __U, __m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_maskz ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U,
					 _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmaddsub_round_ph (__m512h __A, __m512h __B, __m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) -1, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmaddsub_round_ph (__m512h __A, __mmask32 __U, __m512h __B,
			       __m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmaddsub_round_ph (__m512h __A, __m512h __B, __m512h __C,
				__mmask32 __U, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_mask3 ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmaddsub_round_ph (__mmask32 __U, __m512h __A, __m512h __B,
				__m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmaddsubph512_maskz ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U, __R);
}

#else
#define _mm512_fmaddsub_round_ph(A, B, C, R)				\
  ((__m512h)__builtin_ia32_vfmaddsubph512_mask ((A), (B), (C), -1, (R)))

#define _mm512_mask_fmaddsub_round_ph(A, U, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmaddsubph512_mask ((A), (B), (C), (U), (R)))

#define _mm512_mask3_fmaddsub_round_ph(A, B, C, U, R)			\
  ((__m512h)__builtin_ia32_vfmaddsubph512_mask3 ((A), (B), (C), (U), (R)))

#define _mm512_maskz_fmaddsub_round_ph(U, A, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmaddsubph512_maskz ((A), (B), (C), (U), (R)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vfmsubadd[132,213,231]ph.  */
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_fmsubadd_ph (__m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) -1,
					_MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_ph (__m512h __A, __mmask32 __U,
			 __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) __U,
					_MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_ph (__m512h __A, __m512h __B,
			  __m512h __C, __mmask32 __U)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask3 ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U,
					 _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_ph (__mmask32 __U, __m512h __A,
			  __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_maskz ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U,
					 _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmsubadd_round_ph (__m512h __A, __m512h __B,
			  __m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) -1, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmsubadd_round_ph (__m512h __A, __mmask32 __U, __m512h __B,
			       __m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask ((__v32hf) __A,
					(__v32hf) __B,
					(__v32hf) __C,
					(__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmsubadd_round_ph (__m512h __A, __m512h __B, __m512h __C,
				__mmask32 __U, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_mask3 ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmsubadd_round_ph (__mmask32 __U, __m512h __A, __m512h __B,
				__m512h __C, const int __R)
{
  return (__m512h)
    __builtin_ia32_vfmsubaddph512_maskz ((__v32hf) __A,
					 (__v32hf) __B,
					 (__v32hf) __C,
					 (__mmask32) __U, __R);
}

#else
#define _mm512_fmsubadd_round_ph(A, B, C, R)				\
  ((__m512h)__builtin_ia32_vfmsubaddph512_mask ((A), (B), (C), -1, (R)))

#define _mm512_mask_fmsubadd_round_ph(A, U, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmsubaddph512_mask ((A), (B), (C), (U), (R)))

#define _mm512_mask3_fmsubadd_round_ph(A, B, C, U, R)			\
  ((__m512h)__builtin_ia32_vfmsubaddph512_mask3 ((A), (B), (C), (U), (R)))

#define _mm512_maskz_fmsubadd_round_ph(U, A, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmsubaddph512_maskz ((A), (B), (C), (U), (R)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vfmadd[132,213,231]ph.  */
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_fmadd_ph (__m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddph512_mask ((__v32hf) __A,
				     (__v32hf) __B,
				     (__v32hf) __C,
				     (__mmask32) -1,
				     _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_ph (__m512h __A, __mmask32 __U, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddph512_mask ((__v32hf) __A,
				     (__v32hf) __B,
				     (__v32hf) __C,
				     (__mmask32) __U,
				     _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_ph (__m512h __A, __m512h __B, __m512h __C, __mmask32 __U)
{
  return (__m512h)
    __builtin_ia32_vfmaddph512_mask3 ((__v32hf) __A,
				      (__v32hf) __B,
				      (__v32hf) __C,
				      (__mmask32) __U,
				      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_ph (__mmask32 __U, __m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfmaddph512_maskz ((__v32hf) __A,
				      (__v32hf) __B,
				      (__v32hf) __C,
				      (__mmask32) __U,
				      _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fmadd_round_ph (__m512h __A, __m512h __B, __m512h __C, const int __R)
{
  return (__m512h) __builtin_ia32_vfmaddph512_mask ((__v32hf) __A,
						       (__v32hf) __B,
						       (__v32hf) __C,
						       (__mmask32) -1, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fmadd_round_ph (__m512h __A, __mmask32 __U, __m512h __B,
			       __m512h __C, const int __R)
{
  return (__m512h) __builtin_ia32_vfmaddph512_mask ((__v32hf) __A,
						       (__v32hf) __B,
						       (__v32hf) __C,
						       (__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fmadd_round_ph (__m512h __A, __m512h __B, __m512h __C,
				__mmask32 __U, const int __R)
{
  return (__m512h) __builtin_ia32_vfmaddph512_mask3 ((__v32hf) __A,
							(__v32hf) __B,
							(__v32hf) __C,
							(__mmask32) __U, __R);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fmadd_round_ph (__mmask32 __U, __m512h __A, __m512h __B,
				__m512h __C, const int __R)
{
  return (__m512h) __builtin_ia32_vfmaddph512_maskz ((__v32hf) __A,
							(__v32hf) __B,
							(__v32hf) __C,
							(__mmask32) __U, __R);
}

#else
#define _mm512_fmadd_round_ph(A, B, C, R)				\
  ((__m512h)__builtin_ia32_vfmaddph512_mask ((A), (B), (C), -1, (R)))

#define _mm512_mask_fmadd_round_ph(A, U, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmaddph512_mask ((A), (B), (C), (U), (R)))

#define _mm512_mask3_fmadd_round_ph(A, B, C, U, R)			\
  ((__m512h)__builtin_ia32_vfmaddph512_mask3 ((A), (B), (C), (U), (R)))

#define _mm512_maskz_fmadd_round_ph(U, A, B, C, R)			\
  ((__m512h)__builtin_ia32_vfmaddph512_maskz ((A), (B), (C), (U), (R)))

#endif /* __OPTIMIZE__ */

/* Intrinsics vfnmadd[132,213,231]ph.  */
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_ph (__m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfnmaddph512_mask ((__v32hf) __A,
				      (__v32hf) __B,
				      (__v32hf) __C,
				      (__mmask32) -1,
				      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask_fnmadd_ph (__m512h __A, __mmask32 __U, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfnmaddph512_mask ((__v32hf) __A,
				      (__v32hf) __B,
				      (__v32hf) __C,
				      (__mmask32) __U,
				      _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_mask3_fnmadd_ph (__m512h __A, __m512h __B, __m512h __C, __mmask32 __U)
{
  return (__m512h)
    __builtin_ia32_vfnmaddph512_mask3 ((__v32hf) __A,
				       (__v32hf) __B,
				       (__v32hf) __C,
				       (__mmask32) __U,
				       _MM_FROUND_CUR_DIRECTION);
}

extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_maskz_fnmadd_ph (__mmask32 __U, __m512h __A, __m512h __B, __m512h __C)
{
  return (__m512h)
    __builtin_ia32_vfnmaddph512_maskz ((__v32hf) __A,
				       (__v32hf) __B,
				       (__v32hf) __C,
				       (__mmask32) __U,
				       _MM_FROUND_CUR_DIRECTION);
}

#ifdef __OPTIMIZE__
extern __inline __m512h
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_mm512_fnmadd_round_ph (__m512h __A, __m512h __B, __m512h __C, const int __R)
{
  return (__m512h) __builtin_ia32_vfnmaddph512_mask ((__v32hf) __A,
