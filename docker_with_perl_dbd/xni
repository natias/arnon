            #
            # if minimal phase was 0 we don't need to retract anything
            phases.registernew(repo, tr, targetphase, [rev])
        return n


def _prepare_files(tr, ctx, error=False, origctx=None):
    repo = ctx.repo()
    p1 = ctx.p1()

    writechangesetcopy, writefilecopymeta = _write_copy_meta(repo)
    files = metadata.ChangingFiles()
    ms = mergestate.mergestate.read(repo)
    salvaged = _get_salvaged(repo, ms, ctx)
    for s in salvaged:
        files.mark_salvaged(s)

    narrow_files = {}
    if not ctx.repo().narrowmatch().always():
        for f, e in ms.allextras().items():
            action = e.get(b'outside-narrow-merge-action')
            if action is not None:
                narrow_files[f] = action
    if ctx.manifestnode() and not narrow_files:
        # reuse an existing manifest revision
        repo.ui.debug(b'reusing known manifest\n')
        mn = ctx.manifestnode()
        files.update_touched(ctx.files())
        if writechangesetcopy:
            files.update_added(ctx.filesadded())
            files.update_removed(ctx.filesremoved())
    elif not ctx.files() and not narrow_files:
        repo.ui.debug(b'reusing manifest from p1 (no file change)\n')
        mn = p1.manifestnode()
    else:
        mn = _process_files(tr, ctx, ms, files, narrow_files, error=error)

    if origctx and origctx.manifestnode() == mn:
        origfiles = origctx.files()
        assert files.touched.issubset(origfiles)
        files.update_touched(origfiles)

    if writechangesetcopy:
        files.update_copies_from_p1(ctx.p1copies())
        files.update_copies_from_p2(ctx.p2copies())

    return mn, files


def _get_salvaged(repo, ms, ctx):
    """returns a list of salvaged files

    returns empty list if config option which process salvaged files are
    not enabled"""
    salvaged = []
    copy_sd = repo.filecopiesmode == b'changeset-sidedata'
    if copy_sd and len(ctx.parents()) > 1:
        if ms.active():
            for fname in sorted(ms.allextras().keys()):
                might_removed = ms.extras(fname).get(b'merge-removal-candidate')
                if might_removed == b'yes':
                    if fname in ctx:
                        salvaged.append(fname)
    return salvaged


def _process_files(tr, ctx, ms, files, narrow_files=None, error=False):
    repo = ctx.repo()
    p1 = ctx.p1()
    p2 = ctx.p2()

    writechangesetcopy, writefilecopymeta = _write_copy_meta(repo)

    m1ctx = p1.manifestctx()
    m2ctx = p2.manifestctx()
    mctx = m1ctx.copy()

    m = mctx.read()
    m1 = m1ctx.read()
    m2 = m2ctx.read()

    # check in files
    added = []
    removed = list(ctx.removed())
    linkrev = len(repo)
    repo.ui.note(_(b"committing files:\n"))
    uipathfn = scmutil.getuipathfn(repo)
    all_files = ctx.modified() + ctx.added()
    all_files.extend(narrow_files.keys())
    all_files.sort()
    for f in all_files:
        repo.ui.note(uipathfn(f) + b"\n")
        if f in narrow_files:
            narrow_action = narrow_files.get(f)
            if narrow_action == mergestate.CHANGE_REMOVED:
                files.mark_removed(f)
                removed.append(f)
            elif narrow_action == mergestate.CHANGE_ADDED:
                files.mark_added(f)
                added.append(f)
                m[f] = m2[f]
                flags = m2ctx.find(f)[1] or b''
                m.setflag(f, flags)
            elif narrow_action == mergestate.CHANGE_MODIFIED:
                files.mark_touched(f)
                added.append(f)
                m[f] = m2[f]
                flags = m2ctx.find(f)[1] or b''
                m.setflag(f, flags)
            else:
                msg = _(b"corrupted mergestate, unknown narrow action: %b")
                hint = _(b"restart the merge")
                raise error.Abort(msg, hint=hint)
            continue
        try:
            fctx = ctx[f]
            if fctx is None:
                removed.append(f)
            else:
                added.append(f)
                m[f], is_touched = _filecommit(
                    repo, fctx, m1, m2, linkrev, tr, writefilecopymeta, ms
                )
                if is_touched:
                    if is_touched == 'added':
                        files.mark_added(f)
                    elif is_touched == 'merged':
                        files.mark_merged(f)
                    else:
                        files.mark_touched(f)
                m.setflag(f, fctx.flags())
        except OSError:
            repo.ui.warn(_(b"trouble committing %s!\n") % uipathfn(f))
            raise

    # update manifest
    removed = [f for f in removed if f in m1 or f in m2]
    drop = sorted([f for f in removed if f in m])
    for f in drop:
        del m[f]
    if p2.rev() == nullrev:
        files.update_removed(removed)
    else:
        rf = metadata.get_removal_filter(ctx, (p1, p2, m1, m2))
        for f in removed:
            if not rf(f):
                files.mark_removed(f)

    mn = _commit_manifest(
        tr,
        linkrev,
        ctx,
        mctx,
        m,
        files.touched,
        added,
        drop,
        bool(narrow_files),
    )

    return mn


def _filecommit(
    repo,
    fctx,
    manifest1,
    manifest2,
    linkrev,
    tr,
    includecopymeta,
    ms,
):
    """
    commit an individual file as part of a larger transaction

    input:

        fctx:       a file context with the content we are trying to commit
        manifest1:  manifest of changeset first parent
        manifest2:  manifest of changeset second parent
        linkrev:    revision number of the changeset being created
        tr:         current transation
        includecopymeta: boolean, set to False to skip storing the copy data
                    (only used by the Google specific feature of using
                    changeset extra as copy source of truth).
        ms:         mergestate object

    output: (filenode, touched)

        filenode: the filenode that should be used by this changeset
        touched:  one of: None (mean untouched), 'added' or 'modified'
    """

    fname = fctx.path()
    fparent1 = manifest1.get(fname, repo.nullid)
    fparent2 = manifest2.get(fname, repo.nullid)
    touched = None
    if fparent1 == fparent2 == repo.nullid:
        touched = 'added'

    if isinstance(fctx, context.filectx):
        # This block fast path most comparisons which are usually done. It
        # assumes that bare filectx is used and no merge happened, hence no
        # need to create a new file revision in this case.
        node = fctx.filenode()
        if node in [fparent1, fparent2]:
            repo.ui.debug(b'reusing %s filelog entry\n' % fname)
            if (
                fparent1 != repo.nullid
                and manifest1.flags(fname) != fctx.flags()
            ) or (
                fparent2 != repo.nullid
                and manifest2.flags(fname) != fctx.flags()
            ):
                touched = 'modified'
            return node, touched

    flog = repo.file(fname)
    meta = {}
    cfname = fctx.copysource()
    fnode = None

    if cfname and cfname != fname:
        # Mark the new revision of this file as a copy of another
        # file.  This copy data will effectively act as a parent
        # of this new revision.  If this is a merge, the first
        # parent will be the nullid (meaning "look up the copy data")
        # and the second one will be the other parent.  For example:
        #
        # 0 --- 1 --- 3   rev1 changes file foo
        #   \       /     rev2 renames foo to bar and changes it
        #    \- 2 -/      rev3 should have bar with all changes and
        #                      should record that bar descends from
        #                      bar in rev2 and foo in rev1
        #
        # this allows this merge to succeed:
        #
        # 0 --- 1 --- 3   rev4 reverts the content change from rev2
        #   \       /     merging rev3 and rev4 should use bar@rev2
        #    \- 2 --- 4        as the merge base
        #

        cnode = manifest1.get(cfname)
        newfparent = fparent2

        if manifest2:  # branch merge
            if (
                fparent2 == repo.nullid or cnode is None
            ):  # copied on remote side
                if cfname in manifest2:
                    cnode = manifest2[cfname]
                    newfparent = fparent1

        # Here, we used to search backwards through history to try to find
        # where the file copy came from if the source of a copy was not in
        # the parent directory. However, this doesn't actually make sense to
        # do (what does a copy from something not in your working copy even
        # mean?) and it causes bugs (eg, issue4476). Instead, we will warn
        # the user that copy information was dropped, so if they didn't
        # expect this outcome it can be fixed, but this is the correct
        # behavior in this circumstance.

        if cnode:
            repo.ui.debug(b" %s: copy %s:%s\n" % (fname, cfname, hex(cnode)))
            if includecopymeta:
                meta[b"copy"] = cfname
                meta[b"copyrev"] = hex(cnode)
            fparent1, fparent2 = repo.nullid, newfparent
        else:
            repo.ui.warn(
                _(
                    b"warning: can't find ancestor for '%s' "
                    b"copied from '%s'!\n"
                )
                % (fname, cfname)
            )

    elif fparent1 == repo.nullid:
        fparent1, fparent2 = fparent2, repo.nullid
    elif fparent2 != repo.nullid:
        if ms.active() and ms.extras(fname).get(b'filenode-source') == b'other':
            fparent1, fparent2 = fparent2, repo.nullid
        elif ms.active() and ms.extras(fname).get(b'merged') != b'yes':
            fparent1, fparent2 = fparent1, repo.nullid
        # is one parent an ancestor of the other?
        else:
            fparentancestors = flog.commonancestorsheads(fparent1, fparent2)
            if fparent1 in fparentancestors:
                fparent1, fparent2 = fparent2, repo.nullid
            elif fparent2 in fparentancestors:
                fparent2 = repo.nullid

    force_new_node = False
    # The file might have been deleted by merge code and user explicitly choose
    # to revert the file and keep it. The other case can be where there is
    # change-delete or delete-change conflict and user explicitly choose to keep
    # the file. The goal is to create a new filenode for users explicit choices
    if (
        repo.ui.configbool(b'experimental', b'merge-track-salvaged')
        and ms.active()
        and ms.extras(fname).get(b'merge-removal-candidate') == b'yes'
    ):
        force_new_node = True
    # is the file changed?
    text = fctx.data()
    if (
        fparent2 != repo.nullid
        or fparent1 == repo.nullid
        or meta
        or flog.cmp(fparent1, text)
        or force_new_node
    ):
        if touched is None:  # do not overwrite added
            if fparent2 == repo.nullid:
                touched = 'modified'
            else:
                touched = 'merged'
        fnode = flog.add(text, meta, tr, linkrev, fparent1, fparent2)
    # are just the flags changed during merge?
    elif fname in manifest1 and manifest1.flags(fname) != fctx.flags():
        touched = 'modified'
        fnode = fparent1
    else:
        fnode = fparent1
    return fnode, touched


def _commit_manifest(
    tr,
    linkrev,
    ctx,
    mctx,
    manifest,
    files,
    added,
    drop,
    has_some_narrow_action=False,
):
    """make a new manifest entry (or reuse a new one)

    given an initialised manifest context and precomputed list of
    - files: files affected by the commit
    - added: new entries in the manifest
    - drop:  entries present in parents but absent of this one

    Create a new manifest revision, reuse existing ones if possible.

    Return the nodeid of the manifest revision.
    """
    repo = ctx.repo()

    md = None

    # all this is cached, so it is find to get them all from the ctx.
    p1 = ctx.p1()
    p2 = ctx.p2()
    m1ctx = p1.manifestctx()

    m1 = m1ctx.read()

    if not files:
        # if no "files" actually changed in terms of the changelog,
        # try hard to detect unmodified manifest entry so that the
        # exact same commit can be reproduced later on convert.
        md = m1.diff(manifest, scmutil.matchfiles(repo, ctx.files()))
    if not files and md:
        repo.ui.debug(
            b'not reusing manifest (no file change in '
            b'changelog, but manifest differs)\n'
        )
    if files or md:
        repo.ui.note(_(b"committing manifest\n"))
        # we're using narrowmatch here since it's already applied at
        # other stages (such as dirstate.walk), so we're already
        # ignoring things outside of narrowspec in most cases. The
        # one case where we might have files outside the narrowspec
        # at this point is merges, and we already error out in the
        # case where the merge has files outside of the narrowspec,
        # so this is safe.
        if has_some_narrow_action:
            match = None
        else:
            match = repo.narrowmatch()
        mn = mctx.write(
            tr,
            linkrev,
            p1.manifestnode(),
            p2.manifestnode(),
            added,
            drop,
            match=match,
        )
    else:
        repo.ui.debug(
            b'reusing manifest from p1 (listed files ' b'actually unchanged)\n'
        )
        mn = p1.manifestnode()

    return mn


def _extra_with_copies(repo, extra, files):
    """encode copy information into a `extra` dictionnary"""
    p1copies = files.copied_from_p1
    p2copies = files.copied_from_p2
    filesadded = files.added
    filesremoved = files.removed
    files = sorted(files.touched)
    if not _write_copy_meta(repo)[1]:
        # If writing only to changeset extras, use None to indicate that
        # no entry should be written. If writing to both, write an empty
        # entry to prevent the reader from falling back to reading
        # filelogs.
        p1copies = p1copies or None
        p2copies = p2copies or None
        filesadded = filesadded or None
        filesremoved = filesremoved or None

    extrasentries = p1copies, p2copies, filesadded, filesremoved
    if extra is None and any(x is not None for x in extrasentries):
        extra = {}
    if p1copies is not None:
        p1copies = metadata.encodecopies(files, p1copies)
        extra[b'p1copies'] = p1copies
    if p2copies is not None:
        p2copies = metadata.encodecopies(files, p2copies)
        extra[b'p2copies'] = p2copies
    if filesadded is not None:
        filesadded = metadata.encodefileindices(files, filesadded)
        extra[b'filesadded'] = filesadded
    if filesremoved is not None:
        filesremoved = metadata.encodefileindices(files, filesremoved)
        extra[b'filesremoved'] = filesremoved
    return extra
                                                                                                                                                                                                                                                                                      usr/lib/python3/dist-packages/mercurial/config.py                                                   0000644 0000000 0000000 00000020720 14355257011 020554  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # config.py - configuration parsing for Mercurial
#
#  Copyright 2009 Olivia Mackall <olivia@selenic.com> and others
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.


import errno
import os

from .i18n import _
from .pycompat import getattr
from . import (
    encoding,
    error,
    util,
)


class config:
    def __init__(self, data=None):
        self._current_source_level = 0
        self._data = {}
        self._unset = []
        if data:
            for k in data._data:
                self._data[k] = data[k].copy()
            self._current_source_level = data._current_source_level + 1

    def new_source(self):
        """increment the source counter

        This is used to define source priority when reading"""
        self._current_source_level += 1

    def copy(self):
        return config(self)

    def __contains__(self, section):
        return section in self._data

    def hasitem(self, section, item):
        return item in self._data.get(section, {})

    def __getitem__(self, section):
        return self._data.get(section, {})

    def __iter__(self):
        for d in self.sections():
            yield d

    def update(self, src):
        current_level = self._current_source_level
        current_level += 1
        max_level = self._current_source_level
        for s, n in src._unset:
            ds = self._data.get(s, None)
            if ds is not None and n in ds:
                self._data[s] = ds.preparewrite()
                del self._data[s][n]
        for s in src:
            ds = self._data.get(s, None)
            if ds:
                self._data[s] = ds.preparewrite()
            else:
                self._data[s] = util.cowsortdict()
            for k, v in src._data[s].items():
                value, source, level = v
                level += current_level
                max_level = max(level, current_level)
                self._data[s][k] = (value, source, level)
        self._current_source_level = max_level

    def _get(self, section, item):
        return self._data.get(section, {}).get(item)

    def get(self, section, item, default=None):
        result = self._get(section, item)
        if result is None:
            return default
        return result[0]

    def backup(self, section, key):
        """return a tuple allowing restore to reinstall a previous value

        The main reason we need it is because it handles the "no data" case.
        """
        try:
            item = self._data[section][key]
        except KeyError:
            return (section, key)
        else:
            return (section, key) + item

    def source(self, section, item):
        result = self._get(section, item)
        if result is None:
            return b""
        return result[1]

    def level(self, section, item):
        result = self._get(section, item)
        if result is None:
            return None
        return result[2]

    def sections(self):
        return sorted(self._data.keys())

    def items(self, section):
        items = self._data.get(section, {}).items()
        return [(k, v[0]) for (k, v) in items]

    def set(self, section, item, value, source=b""):
        assert not isinstance(
            section, str
        ), b'config section may not be unicode strings on Python 3'
        assert not isinstance(
            item, str
        ), b'config item may not be unicode strings on Python 3'
        assert not isinstance(
            value, str
        ), b'config values may not be unicode strings on Python 3'
        if section not in self:
            self._data[section] = util.cowsortdict()
        else:
            self._data[section] = self._data[section].preparewrite()
        self._data[section][item] = (value, source, self._current_source_level)

    def alter(self, section, key, new_value):
        """alter a value without altering its source or level

        This method is meant to be used by `ui.fixconfig` only."""
        item = self._data[section][key]
        size = len(item)
        new_item = (new_value,) + item[1:]
        assert len(new_item) == size
        self._data[section][key] = new_item

    def restore(self, data):
        """restore data returned by self.backup"""
        if len(data) != 2:
            # restore old data
            section, key = data[:2]
            item = data[2:]
            self._data[section] = self._data[section].preparewrite()
            self._data[section][key] = item
        else:
            # no data before, remove everything
            section, item = data
            if section in self._data:
                self._data[section].pop(item, None)

    def parse(self, src, data, sections=None, remap=None, include=None):
        sectionre = util.re.compile(br'\[([^\[]+)\]')
        itemre = util.re.compile(br'([^=\s][^=]*?)\s*=\s*(.*\S|)')
        contre = util.re.compile(br'\s+(\S|\S.*\S)\s*$')
        emptyre = util.re.compile(br'(;|#|\s*$)')
        commentre = util.re.compile(br'(;|#)')
        unsetre = util.re.compile(br'%unset\s+(\S+)')
        includere = util.re.compile(br'%include\s+(\S|\S.*\S)\s*$')
        section = b""
        item = None
        line = 0
        cont = False

        if remap:
            section = remap.get(section, section)

        for l in data.splitlines(True):
            line += 1
            if line == 1 and l.startswith(b'\xef\xbb\xbf'):
                # Someone set us up the BOM
                l = l[3:]
            if cont:
                if commentre.match(l):
                    continue
                m = contre.match(l)
                if m:
                    if sections and section not in sections:
                        continue
                    v = self.get(section, item) + b"\n" + m.group(1)
                    self.set(section, item, v, b"%s:%d" % (src, line))
                    continue
                item = None
                cont = False
            m = includere.match(l)

            if m and include:
                expanded = util.expandpath(m.group(1))
                try:
                    include(expanded, remap=remap, sections=sections)
                except IOError as inst:
                    if inst.errno != errno.ENOENT:
                        raise error.ConfigError(
                            _(b"cannot include %s (%s)")
                            % (expanded, encoding.strtolocal(inst.strerror)),
                            b"%s:%d" % (src, line),
                        )
                continue
            if emptyre.match(l):
                continue
            m = sectionre.match(l)
            if m:
                section = m.group(1)
                if remap:
                    section = remap.get(section, section)
                if section not in self:
                    self._data[section] = util.cowsortdict()
                continue
            m = itemre.match(l)
            if m:
                item = m.group(1)
                cont = True
                if sections and section not in sections:
                    continue
                self.set(section, item, m.group(2), b"%s:%d" % (src, line))
                continue
            m = unsetre.match(l)
            if m:
                name = m.group(1)
                if sections and section not in sections:
                    continue
                if self.get(section, name) is not None:
                    self._data[section] = self._data[section].preparewrite()
                    del self._data[section][name]
                self._unset.append((section, name))
                continue

            message = l.rstrip()
            if l.startswith(b' '):
                message = b"unexpected leading whitespace: %s" % message
            raise error.ConfigError(message, (b"%s:%d" % (src, line)))

    def read(self, path, fp=None, sections=None, remap=None):
        self.new_source()
        if not fp:
            fp = util.posixfile(path, b'rb')
        assert (
            getattr(fp, 'mode', 'rb') == 'rb'
        ), b'config files must be opened in binary mode, got fp=%r mode=%r' % (
            fp,
            fp.mode,
        )

        dir = os.path.dirname(path)

        def include(rel, remap, sections):
            abs = os.path.normpath(os.path.join(dir, rel))
            self.read(abs, remap=remap, sections=sections)
            # anything after the include has a higher level
            self.new_source()

        self.parse(
            path, fp.read(), sections=sections, remap=remap, include=include
        )
                                                usr/lib/python3/dist-packages/mercurial/configitems.py                                              0000644 0000000 0000000 00000144674 14355257011 021635  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # configitems.py - centralized declaration of configuration option
#
#  Copyright 2017 Pierre-Yves David <pierre-yves.david@octobus.net>
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.


import functools
import re

from . import (
    encoding,
    error,
)


def loadconfigtable(ui, extname, configtable):
    """update config item known to the ui with the extension ones"""
    for section, items in sorted(configtable.items()):
        knownitems = ui._knownconfig.setdefault(section, itemregister())
        knownkeys = set(knownitems)
        newkeys = set(items)
        for key in sorted(knownkeys & newkeys):
            msg = b"extension '%s' overwrite config item '%s.%s'"
            msg %= (extname, section, key)
            ui.develwarn(msg, config=b'warn-config')

        knownitems.update(items)


class configitem:
    """represent a known config item

    :section: the official config section where to find this item,
       :name: the official name within the section,
    :default: default value for this item,
    :alias: optional list of tuples as alternatives,
    :generic: this is a generic definition, match name using regular expression.
    """

    def __init__(
        self,
        section,
        name,
        default=None,
        alias=(),
        generic=False,
        priority=0,
        experimental=False,
    ):
        self.section = section
        self.name = name
        self.default = default
        self.alias = list(alias)
        self.generic = generic
        self.priority = priority
        self.experimental = experimental
        self._re = None
        if generic:
            self._re = re.compile(self.name)


class itemregister(dict):
    """A specialized dictionary that can handle wild-card selection"""

    def __init__(self):
        super(itemregister, self).__init__()
        self._generics = set()

    def update(self, other):
        super(itemregister, self).update(other)
        self._generics.update(other._generics)

    def __setitem__(self, key, item):
        super(itemregister, self).__setitem__(key, item)
        if item.generic:
            self._generics.add(item)

    def get(self, key):
        baseitem = super(itemregister, self).get(key)
        if baseitem is not None and not baseitem.generic:
            return baseitem

        # search for a matching generic item
        generics = sorted(self._generics, key=(lambda x: (x.priority, x.name)))
        for item in generics:
            # we use 'match' instead of 'search' to make the matching simpler
            # for people unfamiliar with regular expression. Having the match
            # rooted to the start of the string will produce less surprising
            # result for user writing simple regex for sub-attribute.
            #
            # For example using "color\..*" match produces an unsurprising
            # result, while using search could suddenly match apparently
            # unrelated configuration that happens to contains "color."
            # anywhere. This is a tradeoff where we favor requiring ".*" on
            # some match to avoid the need to prefix most pattern with "^".
            # The "^" seems more error prone.
            if item._re.match(key):
                return item

        return None


coreitems = {}


def _register(configtable, *args, **kwargs):
    item = configitem(*args, **kwargs)
    section = configtable.setdefault(item.section, itemregister())
    if item.name in section:
        msg = b"duplicated config item registration for '%s.%s'"
        raise error.ProgrammingError(msg % (item.section, item.name))
    section[item.name] = item


# special value for case where the default is derived from other values
dynamicdefault = object()

# Registering actual config items


def getitemregister(configtable):
    f = functools.partial(_register, configtable)
    # export pseudo enum as configitem.*
    f.dynamicdefault = dynamicdefault
    return f


coreconfigitem = getitemregister(coreitems)


def _registerdiffopts(section, configprefix=b''):
    coreconfigitem(
        section,
        configprefix + b'nodates',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'showfunc',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'unified',
        default=None,
    )
    coreconfigitem(
        section,
        configprefix + b'git',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'ignorews',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'ignorewsamount',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'ignoreblanklines',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'ignorewseol',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'nobinary',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'noprefix',
        default=False,
    )
    coreconfigitem(
        section,
        configprefix + b'word-diff',
        default=False,
    )


coreconfigitem(
    b'alias',
    b'.*',
    default=dynamicdefault,
    generic=True,
)
coreconfigitem(
    b'auth',
    b'cookiefile',
    default=None,
)
_registerdiffopts(section=b'annotate')
# bookmarks.pushing: internal hack for discovery
coreconfigitem(
    b'bookmarks',
    b'pushing',
    default=list,
)
# bundle.mainreporoot: internal hack for bundlerepo
coreconfigitem(
    b'bundle',
    b'mainreporoot',
    default=b'',
)
coreconfigitem(
    b'censor',
    b'policy',
    default=b'abort',
    experimental=True,
)
coreconfigitem(
    b'chgserver',
    b'idletimeout',
    default=3600,
)
coreconfigitem(
    b'chgserver',
    b'skiphash',
    default=False,
)
coreconfigitem(
    b'cmdserver',
    b'log',
    default=None,
)
coreconfigitem(
    b'cmdserver',
    b'max-log-files',
    default=7,
)
coreconfigitem(
    b'cmdserver',
    b'max-log-size',
    default=b'1 MB',
)
coreconfigitem(
    b'cmdserver',
    b'max-repo-cache',
    default=0,
    experimental=True,
)
coreconfigitem(
    b'cmdserver',
    b'message-encodings',
    default=list,
)
coreconfigitem(
    b'cmdserver',
    b'track-log',
    default=lambda: [b'chgserver', b'cmdserver', b'repocache'],
)
coreconfigitem(
    b'cmdserver',
    b'shutdown-on-interrupt',
    default=True,
)
coreconfigitem(
    b'color',
    b'.*',
    default=None,
    generic=True,
)
coreconfigitem(
    b'color',
    b'mode',
    default=b'auto',
)
coreconfigitem(
    b'color',
    b'pagermode',
    default=dynamicdefault,
)
coreconfigitem(
    b'command-templates',
    b'graphnode',
    default=None,
    alias=[(b'ui', b'graphnodetemplate')],
)
coreconfigitem(
    b'command-templates',
    b'log',
    default=None,
    alias=[(b'ui', b'logtemplate')],
)
coreconfigitem(
    b'command-templates',
    b'mergemarker',
    default=(
        b'{node|short} '
        b'{ifeq(tags, "tip", "", '
        b'ifeq(tags, "", "", "{tags} "))}'
        b'{if(bookmarks, "{bookmarks} ")}'
        b'{ifeq(branch, "default", "", "{branch} ")}'
        b'- {author|user}: {desc|firstline}'
    ),
    alias=[(b'ui', b'mergemarkertemplate')],
)
coreconfigitem(
