
    for category in repo._wanted_sidedata:
        # Check that a computer is registered for that category for at least
        # one revlog kind.
        for kind, computers in repo._sidedata_computers.items():
            if computers.get(category):
                break
        else:
            # This should never happen since repos are supposed to be able to
            # generate the sidedata they require.
            raise error.ProgrammingError(
                _(
                    b'sidedata category requested by local side without local'
                    b"support: '%s'"
                )
                % pycompat.bytestr(category)
            )

    pullop.trmanager = transactionmanager(repo, b'pull', remote.url())
    wlock = util.nullcontextmanager()
    if not bookmod.bookmarksinstore(repo):
        wlock = repo.wlock()
    with wlock, repo.lock(), pullop.trmanager:
        if confirm or (
            repo.ui.configbool(b"pull", b"confirm") and not repo.ui.plain()
        ):
            add_confirm_callback(repo, pullop)

        # This should ideally be in _pullbundle2(). However, it needs to run
        # before discovery to avoid extra work.
        _maybeapplyclonebundle(pullop)
        streamclone.maybeperformlegacystreamclone(pullop)
        _pulldiscovery(pullop)
        if pullop.canusebundle2:
            _fullpullbundle2(repo, pullop)
        _pullchangeset(pullop)
        _pullphase(pullop)
        _pullbookmarks(pullop)
        _pullobsolete(pullop)

    # storing remotenames
    if repo.ui.configbool(b'experimental', b'remotenames'):
        logexchange.pullremotenames(repo, remote)

    return pullop


# list of steps to perform discovery before pull
pulldiscoveryorder = []

# Mapping between step name and function
#
# This exists to help extensions wrap steps if necessary
pulldiscoverymapping = {}


def pulldiscovery(stepname):
    """decorator for function performing discovery before pull

    The function is added to the step -> function mapping and appended to the
    list of steps.  Beware that decorated function will be added in order (this
    may matter).

    You can only use this decorator for a new step, if you want to wrap a step
    from an extension, change the pulldiscovery dictionary directly."""

    def dec(func):
        assert stepname not in pulldiscoverymapping
        pulldiscoverymapping[stepname] = func
        pulldiscoveryorder.append(stepname)
        return func

    return dec


def _pulldiscovery(pullop):
    """Run all discovery steps"""
    for stepname in pulldiscoveryorder:
        step = pulldiscoverymapping[stepname]
        step(pullop)


@pulldiscovery(b'b1:bookmarks')
def _pullbookmarkbundle1(pullop):
    """fetch bookmark data in bundle1 case

    If not using bundle2, we have to fetch bookmarks before changeset
    discovery to reduce the chance and impact of race conditions."""
    if pullop.remotebookmarks is not None:
        return
    if pullop.canusebundle2 and b'listkeys' in pullop.remotebundle2caps:
        # all known bundle2 servers now support listkeys, but lets be nice with
        # new implementation.
        return
    books = listkeys(pullop.remote, b'bookmarks')
    pullop.remotebookmarks = bookmod.unhexlifybookmarks(books)


@pulldiscovery(b'changegroup')
def _pulldiscoverychangegroup(pullop):
    """discovery phase for the pull

    Current handle changeset discovery only, will change handle all discovery
    at some point."""
    tmp = discovery.findcommonincoming(
        pullop.repo, pullop.remote, heads=pullop.heads, force=pullop.force
    )
    common, fetch, rheads = tmp
    has_node = pullop.repo.unfiltered().changelog.index.has_node
    if fetch and rheads:
        # If a remote heads is filtered locally, put in back in common.
        #
        # This is a hackish solution to catch most of "common but locally
        # hidden situation".  We do not performs discovery on unfiltered
        # repository because it end up doing a pathological amount of round
        # trip for w huge amount of changeset we do not care about.
        #
        # If a set of such "common but filtered" changeset exist on the server
        # but are not including a remote heads, we'll not be able to detect it,
        scommon = set(common)
        for n in rheads:
            if has_node(n):
                if n not in scommon:
                    common.append(n)
        if set(rheads).issubset(set(common)):
            fetch = []
    pullop.common = common
    pullop.fetch = fetch
    pullop.rheads = rheads


def _pullbundle2(pullop):
    """pull data using bundle2

    For now, the only supported data are changegroup."""
    kwargs = {b'bundlecaps': caps20to10(pullop.repo, role=b'client')}

    # make ui easier to access
    ui = pullop.repo.ui

    # At the moment we don't do stream clones over bundle2. If that is
    # implemented then here's where the check for that will go.
    streaming = streamclone.canperformstreamclone(pullop, bundle2=True)[0]

    # declare pull perimeters
    kwargs[b'common'] = pullop.common
    kwargs[b'heads'] = pullop.heads or pullop.rheads

    # check server supports narrow and then adding includepats and excludepats
    servernarrow = pullop.remote.capable(wireprototypes.NARROWCAP)
    if servernarrow and pullop.includepats:
        kwargs[b'includepats'] = pullop.includepats
    if servernarrow and pullop.excludepats:
        kwargs[b'excludepats'] = pullop.excludepats

    if streaming:
        kwargs[b'cg'] = False
        kwargs[b'stream'] = True
        pullop.stepsdone.add(b'changegroup')
        pullop.stepsdone.add(b'phases')

    else:
        # pulling changegroup
        pullop.stepsdone.add(b'changegroup')

        kwargs[b'cg'] = pullop.fetch

        legacyphase = b'phases' in ui.configlist(b'devel', b'legacy.exchange')
        hasbinaryphase = b'heads' in pullop.remotebundle2caps.get(b'phases', ())
        if not legacyphase and hasbinaryphase:
            kwargs[b'phases'] = True
            pullop.stepsdone.add(b'phases')

        if b'listkeys' in pullop.remotebundle2caps:
            if b'phases' not in pullop.stepsdone:
                kwargs[b'listkeys'] = [b'phases']

    bookmarksrequested = False
    legacybookmark = b'bookmarks' in ui.configlist(b'devel', b'legacy.exchange')
    hasbinarybook = b'bookmarks' in pullop.remotebundle2caps

    if pullop.remotebookmarks is not None:
        pullop.stepsdone.add(b'request-bookmarks')

    if (
        b'request-bookmarks' not in pullop.stepsdone
        and pullop.remotebookmarks is None
        and not legacybookmark
        and hasbinarybook
    ):
        kwargs[b'bookmarks'] = True
        bookmarksrequested = True

    if b'listkeys' in pullop.remotebundle2caps:
        if b'request-bookmarks' not in pullop.stepsdone:
            # make sure to always includes bookmark data when migrating
            # `hg incoming --bundle` to using this function.
            pullop.stepsdone.add(b'request-bookmarks')
            kwargs.setdefault(b'listkeys', []).append(b'bookmarks')

    # If this is a full pull / clone and the server supports the clone bundles
    # feature, tell the server whether we attempted a clone bundle. The
    # presence of this flag indicates the client supports clone bundles. This
    # will enable the server to treat clients that support clone bundles
    # differently from those that don't.
    if (
        pullop.remote.capable(b'clonebundles')
        and pullop.heads is None
        and list(pullop.common) == [pullop.repo.nullid]
    ):
        kwargs[b'cbattempted'] = pullop.clonebundleattempted

    if streaming:
        pullop.repo.ui.status(_(b'streaming all changes\n'))
    elif not pullop.fetch:
        pullop.repo.ui.status(_(b"no changes found\n"))
        pullop.cgresult = 0
    else:
        if pullop.heads is None and list(pullop.common) == [pullop.repo.nullid]:
            pullop.repo.ui.status(_(b"requesting all changes\n"))
    if obsolete.isenabled(pullop.repo, obsolete.exchangeopt):
        remoteversions = bundle2.obsmarkersversion(pullop.remotebundle2caps)
        if obsolete.commonversion(remoteversions) is not None:
            kwargs[b'obsmarkers'] = True
            pullop.stepsdone.add(b'obsmarkers')
    _pullbundle2extraprepare(pullop, kwargs)

    remote_sidedata = bundle2.read_remote_wanted_sidedata(pullop.remote)
    if remote_sidedata:
        kwargs[b'remote_sidedata'] = remote_sidedata

    with pullop.remote.commandexecutor() as e:
        args = dict(kwargs)
        args[b'source'] = b'pull'
        bundle = e.callcommand(b'getbundle', args).result()

        try:
            op = bundle2.bundleoperation(
                pullop.repo, pullop.gettransaction, source=b'pull'
            )
            op.modes[b'bookmarks'] = b'records'
            bundle2.processbundle(pullop.repo, bundle, op=op)
        except bundle2.AbortFromPart as exc:
            pullop.repo.ui.error(_(b'remote: abort: %s\n') % exc)
            raise error.RemoteError(_(b'pull failed on remote'), hint=exc.hint)
        except error.BundleValueError as exc:
            raise error.RemoteError(_(b'missing support for %s') % exc)

    if pullop.fetch:
        pullop.cgresult = bundle2.combinechangegroupresults(op)

    # processing phases change
    for namespace, value in op.records[b'listkeys']:
        if namespace == b'phases':
            _pullapplyphases(pullop, value)

    # processing bookmark update
    if bookmarksrequested:
        books = {}
        for record in op.records[b'bookmarks']:
            books[record[b'bookmark']] = record[b"node"]
        pullop.remotebookmarks = books
    else:
        for namespace, value in op.records[b'listkeys']:
            if namespace == b'bookmarks':
                pullop.remotebookmarks = bookmod.unhexlifybookmarks(value)

    # bookmark data were either already there or pulled in the bundle
    if pullop.remotebookmarks is not None:
        _pullbookmarks(pullop)


def _pullbundle2extraprepare(pullop, kwargs):
    """hook function so that extensions can extend the getbundle call"""


def _pullchangeset(pullop):
    """pull changeset from unbundle into the local repo"""
    # We delay the open of the transaction as late as possible so we
    # don't open transaction for nothing or you break future useful
    # rollback call
    if b'changegroup' in pullop.stepsdone:
        return
    pullop.stepsdone.add(b'changegroup')
    if not pullop.fetch:
        pullop.repo.ui.status(_(b"no changes found\n"))
        pullop.cgresult = 0
        return
    tr = pullop.gettransaction()
    if pullop.heads is None and list(pullop.common) == [pullop.repo.nullid]:
        pullop.repo.ui.status(_(b"requesting all changes\n"))
    elif pullop.heads is None and pullop.remote.capable(b'changegroupsubset'):
        # issue1320, avoid a race if remote changed after discovery
        pullop.heads = pullop.rheads

    if pullop.remote.capable(b'getbundle'):
        # TODO: get bundlecaps from remote
        cg = pullop.remote.getbundle(
            b'pull', common=pullop.common, heads=pullop.heads or pullop.rheads
        )
    elif pullop.heads is None:
        with pullop.remote.commandexecutor() as e:
            cg = e.callcommand(
                b'changegroup',
                {
                    b'nodes': pullop.fetch,
                    b'source': b'pull',
                },
            ).result()

    elif not pullop.remote.capable(b'changegroupsubset'):
        raise error.Abort(
            _(
                b"partial pull cannot be done because "
                b"other repository doesn't support "
                b"changegroupsubset."
            )
        )
    else:
        with pullop.remote.commandexecutor() as e:
            cg = e.callcommand(
                b'changegroupsubset',
                {
                    b'bases': pullop.fetch,
                    b'heads': pullop.heads,
                    b'source': b'pull',
                },
            ).result()

    bundleop = bundle2.applybundle(
        pullop.repo, cg, tr, b'pull', pullop.remote.url()
    )
    pullop.cgresult = bundle2.combinechangegroupresults(bundleop)


def _pullphase(pullop):
    # Get remote phases data from remote
    if b'phases' in pullop.stepsdone:
        return
    remotephases = listkeys(pullop.remote, b'phases')
    _pullapplyphases(pullop, remotephases)


def _pullapplyphases(pullop, remotephases):
    """apply phase movement from observed remote state"""
    if b'phases' in pullop.stepsdone:
        return
    pullop.stepsdone.add(b'phases')
    publishing = bool(remotephases.get(b'publishing', False))
    if remotephases and not publishing:
        # remote is new and non-publishing
        pheads, _dr = phases.analyzeremotephases(
            pullop.repo, pullop.pulledsubset, remotephases
        )
        dheads = pullop.pulledsubset
    else:
        # Remote is old or publishing all common changesets
        # should be seen as public
        pheads = pullop.pulledsubset
        dheads = []
    unfi = pullop.repo.unfiltered()
    phase = unfi._phasecache.phase
    rev = unfi.changelog.index.get_rev
    public = phases.public
    draft = phases.draft

    # exclude changesets already public locally and update the others
    pheads = [pn for pn in pheads if phase(unfi, rev(pn)) > public]
    if pheads:
        tr = pullop.gettransaction()
        phases.advanceboundary(pullop.repo, tr, public, pheads)

    # exclude changesets already draft locally and update the others
    dheads = [pn for pn in dheads if phase(unfi, rev(pn)) > draft]
    if dheads:
        tr = pullop.gettransaction()
        phases.advanceboundary(pullop.repo, tr, draft, dheads)


def _pullbookmarks(pullop):
    """process the remote bookmark information to update the local one"""
    if b'bookmarks' in pullop.stepsdone:
        return
    pullop.stepsdone.add(b'bookmarks')
    repo = pullop.repo
    remotebookmarks = pullop.remotebookmarks
    bookmarks_mode = None
    if pullop.remote_path is not None:
        bookmarks_mode = pullop.remote_path.bookmarks_mode
    bookmod.updatefromremote(
        repo.ui,
        repo,
        remotebookmarks,
        pullop.remote.url(),
        pullop.gettransaction,
        explicit=pullop.explicitbookmarks,
        mode=bookmarks_mode,
    )


def _pullobsolete(pullop):
    """utility function to pull obsolete markers from a remote

    The `gettransaction` is function that return the pull transaction, creating
    one if necessary. We return the transaction to inform the calling code that
    a new transaction have been created (when applicable).

    Exists mostly to allow overriding for experimentation purpose"""
    if b'obsmarkers' in pullop.stepsdone:
        return
    pullop.stepsdone.add(b'obsmarkers')
    tr = None
    if obsolete.isenabled(pullop.repo, obsolete.exchangeopt):
        pullop.repo.ui.debug(b'fetching remote obsolete markers\n')
        remoteobs = listkeys(pullop.remote, b'obsolete')
        if b'dump0' in remoteobs:
            tr = pullop.gettransaction()
            markers = []
            for key in sorted(remoteobs, reverse=True):
                if key.startswith(b'dump'):
                    data = util.b85decode(remoteobs[key])
                    version, newmarks = obsolete._readmarkers(data)
                    markers += newmarks
            if markers:
                pullop.repo.obsstore.add(tr, markers)
            pullop.repo.invalidatevolatilesets()
    return tr


def applynarrowacl(repo, kwargs):
    """Apply narrow fetch access control.

    This massages the named arguments for getbundle wire protocol commands
    so requested data is filtered through access control rules.
    """
    ui = repo.ui
    # TODO this assumes existence of HTTP and is a layering violation.
    username = ui.shortuser(ui.environ.get(b'REMOTE_USER') or ui.username())
    user_includes = ui.configlist(
        _NARROWACL_SECTION,
        username + b'.includes',
        ui.configlist(_NARROWACL_SECTION, b'default.includes'),
    )
    user_excludes = ui.configlist(
        _NARROWACL_SECTION,
        username + b'.excludes',
        ui.configlist(_NARROWACL_SECTION, b'default.excludes'),
    )
    if not user_includes:
        raise error.Abort(
            _(b"%s configuration for user %s is empty")
            % (_NARROWACL_SECTION, username)
        )

    user_includes = [
        b'path:.' if p == b'*' else b'path:' + p for p in user_includes
    ]
    user_excludes = [
        b'path:.' if p == b'*' else b'path:' + p for p in user_excludes
    ]

    req_includes = set(kwargs.get('includepats', []))
    req_excludes = set(kwargs.get('excludepats', []))

    req_includes, req_excludes, invalid_includes = narrowspec.restrictpatterns(
        req_includes, req_excludes, user_includes, user_excludes
    )

    if invalid_includes:
        raise error.Abort(
            _(b"The following includes are not accessible for %s: %s")
            % (username, stringutil.pprint(invalid_includes))
        )

    new_args = {}
    new_args.update(kwargs)
    new_args['narrow'] = True
    new_args['narrow_acl'] = True
    new_args['includepats'] = req_includes
    if req_excludes:
        new_args['excludepats'] = req_excludes

    return new_args


def _computeellipsis(repo, common, heads, known, match, depth=None):
    """Compute the shape of a narrowed DAG.

    Args:
      repo: The repository we're transferring.
      common: The roots of the DAG range we're transferring.
              May be just [nullid], which means all ancestors of heads.
      heads: The heads of the DAG range we're transferring.
      match: The narrowmatcher that allows us to identify relevant changes.
      depth: If not None, only consider nodes to be full nodes if they are at
             most depth changesets away from one of heads.

    Returns:
      A tuple of (visitnodes, relevant_nodes, ellipsisroots) where:

        visitnodes: The list of nodes (either full or ellipsis) which
                    need to be sent to the client.
        relevant_nodes: The set of changelog nodes which change a file inside
                 the narrowspec. The client needs these as non-ellipsis nodes.
        ellipsisroots: A dict of {rev: parents} that is used in
                       narrowchangegroup to produce ellipsis nodes with the
                       correct parents.
    """
    cl = repo.changelog
    mfl = repo.manifestlog

    clrev = cl.rev

    commonrevs = {clrev(n) for n in common} | {nullrev}
    headsrevs = {clrev(n) for n in heads}

    if depth:
        revdepth = {h: 0 for h in headsrevs}

    ellipsisheads = collections.defaultdict(set)
    ellipsisroots = collections.defaultdict(set)

    def addroot(head, curchange):
        """Add a root to an ellipsis head, splitting heads with 3 roots."""
        ellipsisroots[head].add(curchange)
        # Recursively split ellipsis heads with 3 roots by finding the
        # roots' youngest common descendant which is an elided merge commit.
        # That descendant takes 2 of the 3 roots as its own, and becomes a
        # root of the head.
        while len(ellipsisroots[head]) > 2:
            child, roots = splithead(head)
            splitroots(head, child, roots)
            head = child  # Recurse in case we just added a 3rd root

    def splitroots(head, child, roots):
        ellipsisroots[head].difference_update(roots)
        ellipsisroots[head].add(child)
        ellipsisroots[child].update(roots)
        ellipsisroots[child].discard(child)

    def splithead(head):
        r1, r2, r3 = sorted(ellipsisroots[head])
        for nr1, nr2 in ((r2, r3), (r1, r3), (r1, r2)):
            mid = repo.revs(
                b'sort(merge() & %d::%d & %d::%d, -rev)', nr1, head, nr2, head
            )
            for j in mid:
                if j == nr2:
                    return nr2, (nr1, nr2)
                if j not in ellipsisroots or len(ellipsisroots[j]) < 2:
                    return j, (nr1, nr2)
        raise error.Abort(
            _(
                b'Failed to split up ellipsis node! head: %d, '
                b'roots: %d %d %d'
            )
            % (head, r1, r2, r3)
        )

    missing = list(cl.findmissingrevs(common=commonrevs, heads=headsrevs))
    visit = reversed(missing)
    relevant_nodes = set()
    visitnodes = [cl.node(m) for m in missing]
    required = set(headsrevs) | known
    for rev in visit:
        clrev = cl.changelogrevision(rev)
        ps = [prev for prev in cl.parentrevs(rev) if prev != nullrev]
        if depth is not None:
            curdepth = revdepth[rev]
            for p in ps:
                revdepth[p] = min(curdepth + 1, revdepth.get(p, depth + 1))
        needed = False
        shallow_enough = depth is None or revdepth[rev] <= depth
        if shallow_enough:
            curmf = mfl[clrev.manifest].read()
            if ps:
                # We choose to not trust the changed files list in
                # changesets because it's not always correct. TODO: could
                # we trust it for the non-merge case?
                p1mf = mfl[cl.changelogrevision(ps[0]).manifest].read()
                needed = bool(curmf.diff(p1mf, match))
                if not needed and len(ps) > 1:
                    # For merge changes, the list of changed files is not
                    # helpful, since we need to emit the merge if a file
                    # in the narrow spec has changed on either side of the
                    # merge. As a result, we do a manifest diff to check.
                    p2mf = mfl[cl.changelogrevision(ps[1]).manifest].read()
                    needed = bool(curmf.diff(p2mf, match))
            else:
                # For a root node, we need to include the node if any
                # files in the node match the narrowspec.
                needed = any(curmf.walk(match))

        if needed:
            for head in ellipsisheads[rev]:
                addroot(head, rev)
            for p in ps:
                required.add(p)
            relevant_nodes.add(cl.node(rev))
        else:
            if not ps:
                ps = [nullrev]
            if rev in required:
                for head in ellipsisheads[rev]:
                    addroot(head, rev)
                for p in ps:
                    ellipsisheads[p].add(rev)
            else:
                for p in ps:
                    ellipsisheads[p] |= ellipsisheads[rev]

    # add common changesets as roots of their reachable ellipsis heads
    for c in commonrevs:
        for head in ellipsisheads[c]:
            addroot(head, c)
    return visitnodes, relevant_nodes, ellipsisroots


def caps20to10(repo, role):
    """return a set with appropriate options to use bundle20 during getbundle"""
    caps = {b'HG20'}
    capsblob = bundle2.encodecaps(bundle2.getrepocaps(repo, role=role))
    caps.add(b'bundle2=' + urlreq.quote(capsblob))
    return caps


# List of names of steps to perform for a bundle2 for getbundle, order matters.
getbundle2partsorder = []

# Mapping between step name and function
#
# This exists to help extensions wrap steps if necessary
getbundle2partsmapping = {}


def getbundle2partsgenerator(stepname, idx=None):
    """decorator for function generating bundle2 part for getbundle

    The function is added to the step -> function mapping and appended to the
    list of steps.  Beware that decorated functions will be added in order
    (this may matter).

    You can only use this decorator for new steps, if you want to wrap a step
    from an extension, attack the getbundle2partsmapping dictionary directly."""

    def dec(func):
        assert stepname not in getbundle2partsmapping
        getbundle2partsmapping[stepname] = func
        if idx is None:
            getbundle2partsorder.append(stepname)
        else:
            getbundle2partsorder.insert(idx, stepname)
        return func

    return dec


def bundle2requested(bundlecaps):
    if bundlecaps is not None:
        return any(cap.startswith(b'HG2') for cap in bundlecaps)
    return False


def getbundlechunks(
    repo,
    source,
    heads=None,
    common=None,
    bundlecaps=None,
    remote_sidedata=None,
    **kwargs
):
    """Return chunks constituting a bundle's raw data.

    Could be a bundle HG10 or a bundle HG20 depending on bundlecaps
    passed.

    Returns a 2-tuple of a dict with metadata about the generated bundle
    and an iterator over raw chunks (of varying sizes).
    """
    kwargs = pycompat.byteskwargs(kwargs)
    info = {}
    usebundle2 = bundle2requested(bundlecaps)
    # bundle10 case
    if not usebundle2:
        if bundlecaps and not kwargs.get(b'cg', True):
            raise ValueError(
                _(b'request for bundle10 must include changegroup')
            )

        if kwargs:
            raise ValueError(
                _(b'unsupported getbundle arguments: %s')
                % b', '.join(sorted(kwargs.keys()))
            )
        outgoing = _computeoutgoing(repo, heads, common)
        info[b'bundleversion'] = 1
        return (
            info,
            changegroup.makestream(
                repo,
                outgoing,
                b'01',
                source,
                bundlecaps=bundlecaps,
                remote_sidedata=remote_sidedata,
            ),
        )

    # bundle20 case
    info[b'bundleversion'] = 2
    b2caps = {}
    for bcaps in bundlecaps:
        if bcaps.startswith(b'bundle2='):
            blob = urlreq.unquote(bcaps[len(b'bundle2=') :])
            b2caps.update(bundle2.decodecaps(blob))
    bundler = bundle2.bundle20(repo.ui, b2caps)

    kwargs[b'heads'] = heads
    kwargs[b'common'] = common

    for name in getbundle2partsorder:
        func = getbundle2partsmapping[name]
        func(
            bundler,
            repo,
            source,
            bundlecaps=bundlecaps,
            b2caps=b2caps,
            remote_sidedata=remote_sidedata,
            **pycompat.strkwargs(kwargs)
        )

    info[b'prefercompressed'] = bundler.prefercompressed

    return info, bundler.getchunks()


@getbundle2partsgenerator(b'stream2')
def _getbundlestream2(bundler, repo, *args, **kwargs):
    return bundle2.addpartbundlestream2(bundler, repo, **kwargs)


@getbundle2partsgenerator(b'changegroup')
def _getbundlechangegrouppart(
    bundler,
    repo,
    source,
    bundlecaps=None,
    b2caps=None,
    heads=None,
    common=None,
    remote_sidedata=None,
    **kwargs
):
    """add a changegroup part to the requested bundle"""
    if not kwargs.get('cg', True) or not b2caps:
        return

    version = b'01'
    cgversions = b2caps.get(b'changegroup')
    if cgversions:  # 3.1 and 3.2 ship with an empty value
        cgversions = [
            v
            for v in cgversions
            if v in changegroup.supportedoutgoingversions(repo)
        ]
        if not cgversions:
            raise error.Abort(_(b'no common changegroup version'))
        version = max(cgversions)

    outgoing = _computeoutgoing(repo, heads, common)
    if not outgoing.missing:
        return

    if kwargs.get('narrow', False):
        include = sorted(filter(bool, kwargs.get('includepats', [])))
        exclude = sorted(filter(bool, kwargs.get('excludepats', [])))
        matcher = narrowspec.match(repo.root, include=include, exclude=exclude)
    else:
        matcher = None

    cgstream = changegroup.makestream(
        repo,
        outgoing,
        version,
        source,
        bundlecaps=bundlecaps,
        matcher=matcher,
        remote_sidedata=remote_sidedata,
    )

    part = bundler.newpart(b'changegroup', data=cgstream)
    if cgversions:
        part.addparam(b'version', version)

    part.addparam(b'nbchanges', b'%d' % len(outgoing.missing), mandatory=False)

    if scmutil.istreemanifest(repo):
        part.addparam(b'treemanifest', b'1')

    if repository.REPO_FEATURE_SIDE_DATA in repo.features:
        part.addparam(b'exp-sidedata', b'1')
        sidedata = bundle2.format_remote_wanted_sidedata(repo)
        part.addparam(b'exp-wanted-sidedata', sidedata)

    if (
        kwargs.get('narrow', False)
        and kwargs.get('narrow_acl', False)
        and (include or exclude)
    ):
        # this is mandatory because otherwise ACL clients won't work
        narrowspecpart = bundler.newpart(b'Narrow:responsespec')
        narrowspecpart.data = b'%s\0%s' % (
            b'\n'.join(include),
            b'\n'.join(exclude),
        )


@getbundle2partsgenerator(b'bookmarks')
def _getbundlebookmarkpart(
    bundler, repo, source, bundlecaps=None, b2caps=None, **kwargs
):
    """add a bookmark part to the requested bundle"""
    if not kwargs.get('bookmarks', False):
        return
    if not b2caps or b'bookmarks' not in b2caps:
        raise error.Abort(_(b'no common bookmarks exchange method'))
    books = bookmod.listbinbookmarks(repo)
    data = bookmod.binaryencode(repo, books)
    if data:
        bundler.newpart(b'bookmarks', data=data)


@getbundle2partsgenerator(b'listkeys')
def _getbundlelistkeysparts(
    bundler, repo, source, bundlecaps=None, b2caps=None, **kwargs
):
    """add parts containing listkeys namespaces to the requested bundle"""
    listkeys = kwargs.get('listkeys', ())
    for namespace in listkeys:
        part = bundler.newpart(b'listkeys')
        part.addparam(b'namespace', namespace)
        keys = repo.listkeys(namespace).items()
        part.data = pushkey.encodekeys(keys)


@getbundle2partsgenerator(b'obsmarkers')
def _getbundleobsmarkerpart(
    bundler, repo, source, bundlecaps=None, b2caps=None, heads=None, **kwargs
):
    """add an obsolescence markers part to the requested bundle"""
    if kwargs.get('obsmarkers', False):
        if heads is None:
            heads = repo.heads()
        subset = [c.node() for c in repo.set(b'::%ln', heads)]
        markers = repo.obsstore.relevantmarkers(subset)
        markers = obsutil.sortedmarkers(markers)
        bundle2.buildobsmarkerspart(bundler, markers)


@getbundle2partsgenerator(b'phases')
def _getbundlephasespart(
    bundler, repo, source, bundlecaps=None, b2caps=None, heads=None, **kwargs
):
    """add phase heads part to the requested bundle"""
    if kwargs.get('phases', False):
        if not b2caps or b'heads' not in b2caps.get(b'phases'):
            raise error.Abort(_(b'no common phases exchange method'))
        if heads is None:
            heads = repo.heads()

        headsbyphase = collections.defaultdict(set)
        if repo.publishing():
            headsbyphase[phases.public] = heads
        else:
            # find the appropriate heads to move

            phase = repo._phasecache.phase
            node = repo.changelog.node
            rev = repo.changelog.rev
            for h in heads:
                headsbyphase[phase(repo, rev(h))].add(h)
            seenphases = list(headsbyphase.keys())

            # We do not handle anything but public and draft phase for now)
            if seenphases:
                assert max(seenphases) <= phases.draft

            # if client is pulling non-public changesets, we need to find
            # intermediate public heads.
            draftheads = headsbyphase.get(phases.draft, set())
            if draftheads:
                publicheads = headsbyphase.get(phases.public, set())

                revset = b'heads(only(%ln, %ln) and public())'
                extraheads = repo.revs(revset, draftheads, publicheads)
                for r in extraheads:
                    headsbyphase[phases.public].add(node(r))

        # transform data in a format used by the encoding function
        phasemapping = {
            phase: sorted(headsbyphase[phase]) for phase in phases.allphases
        }

        # generate the actual part
        phasedata = phases.binaryencode(phasemapping)
        bundler.newpart(b'phase-heads', data=phasedata)


@getbundle2partsgenerator(b'hgtagsfnodes')
def _getbundletagsfnodes(
    bundler,
    repo,
    source,
    bundlecaps=None,
    b2caps=None,
    heads=None,
    common=None,
    **kwargs
):
    """Transfer the .hgtags filenodes mapping.

    Only values for heads in this bundle will be transferred.

    The part data consists of pairs of 20 byte changeset node and .hgtags
    filenodes raw values.
    """
    # Don't send unless:
    # - changeset are being exchanged,
    # - the client supports it.
    if not b2caps or not (kwargs.get('cg', True) and b'hgtagsfnodes' in b2caps):
        return

    outgoing = _computeoutgoing(repo, heads, common)
    bundle2.addparttagsfnodescache(repo, bundler, outgoing)


@getbundle2partsgenerator(b'cache:rev-branch-cache')
def _getbundlerevbranchcache(
    bundler,
    repo,
    source,
    bundlecaps=None,
    b2caps=None,
    heads=None,
    common=None,
    **kwargs
):
    """Transfer the rev-branch-cache mapping

    The payload is a series of data related to each branch

    1) branch name length
    2) number of open heads
    3) number of closed heads
    4) open heads nodes
    5) closed heads nodes
    """
    # Don't send unless:
    # - changeset are being exchanged,
    # - the client supports it.
    # - narrow bundle isn't in play (not currently compatible).
    if (
        not kwargs.get('cg', True)
        or not b2caps
        or b'rev-branch-cache' not in b2caps
        or kwargs.get('narrow', False)
        or repo.ui.has_section(_NARROWACL_SECTION)
    ):
        return

    outgoing = _computeoutgoing(repo, heads, common)
    bundle2.addpartrevbranchcache(repo, bundler, outgoing)


def check_heads(repo, their_heads, context):
    """check if the heads of a repo have been modified

    Used by peer for unbundling.
    """
    heads = repo.heads()
    heads_hash = hashutil.sha1(b''.join(sorted(heads))).digest()
    if not (
        their_heads == [b'force']
        or their_heads == heads
        or their_heads == [b'hashed', heads_hash]
    ):
        # someone else committed/pushed/unbundled while we
        # were transferring data
        raise error.PushRaced(
            b'repository changed while %s - please try again' % context
        )


def unbundle(repo, cg, heads, source, url):
    """Apply a bundle to a repo.

    this function makes sure the repo is locked during the application and have
    mechanism to check that no push race occurred between the creation of the
