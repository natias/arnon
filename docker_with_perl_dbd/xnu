                            repo, cx, path, filecontent[path]
                        )
                    return None

                if len(ps) == 0 or ps[0] < 0:
                    pars = [None, None]
                elif len(ps) == 1:
                    pars = [nodeids[ps[0]], None]
                else:
                    pars = [nodeids[p] for p in ps]
                cx = context.memctx(
                    repo,
                    pars,
                    b"r%i" % id,
                    files,
                    fctxfn,
                    date=(id, 0),
                    user=b"debugbuilddag",
                    extra={b'branch': atbranch},
                )
                nodeid = repo.commitctx(cx)
                nodeids.append(nodeid)
                at = id
            elif type == b'l':
                id, name = data
                ui.note((b'tag %s\n' % name))
                tags.append(b"%s %s\n" % (hex(repo.changelog.node(id)), name))
            elif type == b'a':
                ui.note((b'branch %s\n' % data))
                atbranch = data
            progress.update(id)

        if tags:
            repo.vfs.write(b"localtags", b"".join(tags))


def _debugchangegroup(ui, gen, all=None, indent=0, **opts):
    indent_string = b' ' * indent
    if all:
        ui.writenoi18n(
            b"%sformat: id, p1, p2, cset, delta base, len(delta)\n"
            % indent_string
        )

        def showchunks(named):
            ui.write(b"\n%s%s\n" % (indent_string, named))
            for deltadata in gen.deltaiter():
                node, p1, p2, cs, deltabase, delta, flags, sidedata = deltadata
                ui.write(
                    b"%s%s %s %s %s %s %d\n"
                    % (
                        indent_string,
                        hex(node),
                        hex(p1),
                        hex(p2),
                        hex(cs),
                        hex(deltabase),
                        len(delta),
                    )
                )

        gen.changelogheader()
        showchunks(b"changelog")
        gen.manifestheader()
        showchunks(b"manifest")
        for chunkdata in iter(gen.filelogheader, {}):
            fname = chunkdata[b'filename']
            showchunks(fname)
    else:
        if isinstance(gen, bundle2.unbundle20):
            raise error.Abort(_(b'use debugbundle2 for this file'))
        gen.changelogheader()
        for deltadata in gen.deltaiter():
            node, p1, p2, cs, deltabase, delta, flags, sidedata = deltadata
            ui.write(b"%s%s\n" % (indent_string, hex(node)))


def _debugobsmarkers(ui, part, indent=0, **opts):
    """display version and markers contained in 'data'"""
    opts = pycompat.byteskwargs(opts)
    data = part.read()
    indent_string = b' ' * indent
    try:
        version, markers = obsolete._readmarkers(data)
    except error.UnknownVersion as exc:
        msg = b"%sunsupported version: %s (%d bytes)\n"
        msg %= indent_string, exc.version, len(data)
        ui.write(msg)
    else:
        msg = b"%sversion: %d (%d bytes)\n"
        msg %= indent_string, version, len(data)
        ui.write(msg)
        fm = ui.formatter(b'debugobsolete', opts)
        for rawmarker in sorted(markers):
            m = obsutil.marker(None, rawmarker)
            fm.startitem()
            fm.plain(indent_string)
            cmdutil.showmarker(fm, m)
        fm.end()


def _debugphaseheads(ui, data, indent=0):
    """display version and markers contained in 'data'"""
    indent_string = b' ' * indent
    headsbyphase = phases.binarydecode(data)
    for phase in phases.allphases:
        for head in headsbyphase[phase]:
            ui.write(indent_string)
            ui.write(b'%s %s\n' % (hex(head), phases.phasenames[phase]))


def _quasirepr(thing):
    if isinstance(thing, (dict, util.sortdict, collections.OrderedDict)):
        return b'{%s}' % (
            b', '.join(b'%s: %s' % (k, thing[k]) for k in sorted(thing))
        )
    return pycompat.bytestr(repr(thing))


def _debugbundle2(ui, gen, all=None, **opts):
    """lists the contents of a bundle2"""
    if not isinstance(gen, bundle2.unbundle20):
        raise error.Abort(_(b'not a bundle2 file'))
    ui.write((b'Stream params: %s\n' % _quasirepr(gen.params)))
    parttypes = opts.get('part_type', [])
    for part in gen.iterparts():
        if parttypes and part.type not in parttypes:
            continue
        msg = b'%s -- %s (mandatory: %r)\n'
        ui.write((msg % (part.type, _quasirepr(part.params), part.mandatory)))
        if part.type == b'changegroup':
            version = part.params.get(b'version', b'01')
            cg = changegroup.getunbundler(version, part, b'UN')
            if not ui.quiet:
                _debugchangegroup(ui, cg, all=all, indent=4, **opts)
        if part.type == b'obsmarkers':
            if not ui.quiet:
                _debugobsmarkers(ui, part, indent=4, **opts)
        if part.type == b'phase-heads':
            if not ui.quiet:
                _debugphaseheads(ui, part, indent=4)


@command(
    b'debugbundle',
    [
        (b'a', b'all', None, _(b'show all details')),
        (b'', b'part-type', [], _(b'show only the named part type')),
        (b'', b'spec', None, _(b'print the bundlespec of the bundle')),
    ],
    _(b'FILE'),
    norepo=True,
)
def debugbundle(ui, bundlepath, all=None, spec=None, **opts):
    """lists the contents of a bundle"""
    with hg.openpath(ui, bundlepath) as f:
        if spec:
            spec = exchange.getbundlespec(ui, f)
            ui.write(b'%s\n' % spec)
            return

        gen = exchange.readbundle(ui, f, bundlepath)
        if isinstance(gen, bundle2.unbundle20):
            return _debugbundle2(ui, gen, all=all, **opts)
        _debugchangegroup(ui, gen, all=all, **opts)


@command(b'debugcapabilities', [], _(b'PATH'), norepo=True)
def debugcapabilities(ui, path, **opts):
    """lists the capabilities of a remote peer"""
    opts = pycompat.byteskwargs(opts)
    peer = hg.peer(ui, opts, path)
    try:
        caps = peer.capabilities()
        ui.writenoi18n(b'Main capabilities:\n')
        for c in sorted(caps):
            ui.write(b'  %s\n' % c)
        b2caps = bundle2.bundle2caps(peer)
        if b2caps:
            ui.writenoi18n(b'Bundle2 capabilities:\n')
            for key, values in sorted(b2caps.items()):
                ui.write(b'  %s\n' % key)
                for v in values:
                    ui.write(b'    %s\n' % v)
    finally:
        peer.close()


@command(
    b'debugchangedfiles',
    [
        (
            b'',
            b'compute',
            False,
            b"compute information instead of reading it from storage",
        ),
    ],
    b'REV',
)
def debugchangedfiles(ui, repo, rev, **opts):
    """list the stored files changes for a revision"""
    ctx = logcmdutil.revsingle(repo, rev, None)
    files = None

    if opts['compute']:
        files = metadata.compute_all_files_changes(ctx)
    else:
        sd = repo.changelog.sidedata(ctx.rev())
        files_block = sd.get(sidedata.SD_FILES)
        if files_block is not None:
            files = metadata.decode_files_sidedata(sd)
    if files is not None:
        for f in sorted(files.touched):
            if f in files.added:
                action = b"added"
            elif f in files.removed:
                action = b"removed"
            elif f in files.merged:
                action = b"merged"
            elif f in files.salvaged:
                action = b"salvaged"
            else:
                action = b"touched"

            copy_parent = b""
            copy_source = b""
            if f in files.copied_from_p1:
                copy_parent = b"p1"
                copy_source = files.copied_from_p1[f]
            elif f in files.copied_from_p2:
                copy_parent = b"p2"
                copy_source = files.copied_from_p2[f]

            data = (action, copy_parent, f, copy_source)
            template = b"%-8s %2s: %s, %s;\n"
            ui.write(template % data)


@command(b'debugcheckstate', [], b'')
def debugcheckstate(ui, repo):
    """validate the correctness of the current dirstate"""
    parent1, parent2 = repo.dirstate.parents()
    m1 = repo[parent1].manifest()
    m2 = repo[parent2].manifest()
    errors = 0
    for err in repo.dirstate.verify(m1, m2):
        ui.warn(err[0] % err[1:])
        errors += 1
    if errors:
        errstr = _(b".hg/dirstate inconsistent with current parent's manifest")
        raise error.Abort(errstr)


@command(
    b'debugcolor',
    [(b'', b'style', None, _(b'show all configured styles'))],
    b'hg debugcolor',
)
def debugcolor(ui, repo, **opts):
    """show available color, effects or style"""
    ui.writenoi18n(b'color mode: %s\n' % stringutil.pprint(ui._colormode))
    if opts.get('style'):
        return _debugdisplaystyle(ui)
    else:
        return _debugdisplaycolor(ui)


def _debugdisplaycolor(ui):
    ui = ui.copy()
    ui._styles.clear()
    for effect in color._activeeffects(ui).keys():
        ui._styles[effect] = effect
    if ui._terminfoparams:
        for k, v in ui.configitems(b'color'):
            if k.startswith(b'color.'):
                ui._styles[k] = k[6:]
            elif k.startswith(b'terminfo.'):
                ui._styles[k] = k[9:]
    ui.write(_(b'available colors:\n'))
    # sort label with a '_' after the other to group '_background' entry.
    items = sorted(ui._styles.items(), key=lambda i: (b'_' in i[0], i[0], i[1]))
    for colorname, label in items:
        ui.write(b'%s\n' % colorname, label=label)


def _debugdisplaystyle(ui):
    ui.write(_(b'available style:\n'))
    if not ui._styles:
        return
    width = max(len(s) for s in ui._styles)
    for label, effects in sorted(ui._styles.items()):
        ui.write(b'%s' % label, label=label)
        if effects:
            # 50
            ui.write(b': ')
            ui.write(b' ' * (max(0, width - len(label))))
            ui.write(b', '.join(ui.label(e, e) for e in effects.split()))
        ui.write(b'\n')


@command(b'debugcreatestreamclonebundle', [], b'FILE')
def debugcreatestreamclonebundle(ui, repo, fname):
    """create a stream clone bundle file

    Stream bundles are special bundles that are essentially archives of
    revlog files. They are commonly used for cloning very quickly.
    """
    # TODO we may want to turn this into an abort when this functionality
    # is moved into `hg bundle`.
    if phases.hassecret(repo):
        ui.warn(
            _(
                b'(warning: stream clone bundle will contain secret '
                b'revisions)\n'
            )
        )

    requirements, gen = streamclone.generatebundlev1(repo)
    changegroup.writechunks(ui, gen, fname)

    ui.write(_(b'bundle requirements: %s\n') % b', '.join(sorted(requirements)))


@command(
    b'debugdag',
    [
        (b't', b'tags', None, _(b'use tags as labels')),
        (b'b', b'branches', None, _(b'annotate with branch names')),
        (b'', b'dots', None, _(b'use dots for runs')),
        (b's', b'spaces', None, _(b'separate elements by spaces')),
    ],
    _(b'[OPTION]... [FILE [REV]...]'),
    optionalrepo=True,
)
def debugdag(ui, repo, file_=None, *revs, **opts):
    """format the changelog or an index DAG as a concise textual description

    If you pass a revlog index, the revlog's DAG is emitted. If you list
    revision numbers, they get labeled in the output as rN.

    Otherwise, the changelog DAG of the current repo is emitted.
    """
    spaces = opts.get('spaces')
    dots = opts.get('dots')
    if file_:
        rlog = revlog.revlog(vfsmod.vfs(encoding.getcwd(), audit=False), file_)
        revs = {int(r) for r in revs}

        def events():
            for r in rlog:
                yield b'n', (r, list(p for p in rlog.parentrevs(r) if p != -1))
                if r in revs:
                    yield b'l', (r, b"r%i" % r)

    elif repo:
        cl = repo.changelog
        tags = opts.get('tags')
        branches = opts.get('branches')
        if tags:
            labels = {}
            for l, n in repo.tags().items():
                labels.setdefault(cl.rev(n), []).append(l)

        def events():
            b = b"default"
            for r in cl:
                if branches:
                    newb = cl.read(cl.node(r))[5][b'branch']
                    if newb != b:
                        yield b'a', newb
                        b = newb
                yield b'n', (r, list(p for p in cl.parentrevs(r) if p != -1))
                if tags:
                    ls = labels.get(r)
                    if ls:
                        for l in ls:
                            yield b'l', (r, l)

    else:
        raise error.Abort(_(b'need repo for changelog dag'))

    for line in dagparser.dagtextlines(
        events(),
        addspaces=spaces,
        wraplabels=True,
        wrapannotations=True,
        wrapnonlinear=dots,
        usedots=dots,
        maxlinewidth=70,
    ):
        ui.write(line)
        ui.write(b"\n")


@command(b'debugdata', cmdutil.debugrevlogopts, _(b'-c|-m|FILE REV'))
def debugdata(ui, repo, file_, rev=None, **opts):
    """dump the contents of a data file revision"""
    opts = pycompat.byteskwargs(opts)
    if opts.get(b'changelog') or opts.get(b'manifest') or opts.get(b'dir'):
        if rev is not None:
            raise error.CommandError(b'debugdata', _(b'invalid arguments'))
        file_, rev = None, file_
    elif rev is None:
        raise error.CommandError(b'debugdata', _(b'invalid arguments'))
    r = cmdutil.openstorage(repo, b'debugdata', file_, opts)
    try:
        ui.write(r.rawdata(r.lookup(rev)))
    except KeyError:
        raise error.Abort(_(b'invalid revision identifier %s') % rev)


@command(
    b'debugdate',
    [(b'e', b'extended', None, _(b'try extended date formats'))],
    _(b'[-e] DATE [RANGE]'),
    norepo=True,
    optionalrepo=True,
)
def debugdate(ui, date, range=None, **opts):
    """parse and display a date"""
    if opts["extended"]:
        d = dateutil.parsedate(date, dateutil.extendeddateformats)
    else:
        d = dateutil.parsedate(date)
    ui.writenoi18n(b"internal: %d %d\n" % d)
    ui.writenoi18n(b"standard: %s\n" % dateutil.datestr(d))
    if range:
        m = dateutil.matchdate(range)
        ui.writenoi18n(b"match: %s\n" % m(d[0]))


@command(
    b'debugdeltachain',
    cmdutil.debugrevlogopts + cmdutil.formatteropts,
    _(b'-c|-m|FILE'),
    optionalrepo=True,
)
def debugdeltachain(ui, repo, file_=None, **opts):
    """dump information about delta chains in a revlog

    Output can be templatized. Available template keywords are:

    :``rev``:       revision number
    :``p1``:        parent 1 revision number (for reference)
    :``p2``:        parent 2 revision number (for reference)
    :``chainid``:   delta chain identifier (numbered by unique base)
    :``chainlen``:  delta chain length to this revision
    :``prevrev``:   previous revision in delta chain
    :``deltatype``: role of delta / how it was computed
                    - base:  a full snapshot
                    - snap:  an intermediate snapshot
                    - p1:    a delta against the first parent
                    - p2:    a delta against the second parent
                    - skip1: a delta against the same base as p1
                              (when p1 has empty delta
                    - skip2: a delta against the same base as p2
                              (when p2 has empty delta
                    - prev:  a delta against the previous revision
                    - other: a delta against an arbitrary revision
    :``compsize``:  compressed size of revision
    :``uncompsize``: uncompressed size of revision
    :``chainsize``: total size of compressed revisions in chain
    :``chainratio``: total chain size divided by uncompressed revision size
                    (new delta chains typically start at ratio 2.00)
    :``lindist``:   linear distance from base revision in delta chain to end
                    of this revision
    :``extradist``: total size of revisions not part of this delta chain from
                    base of delta chain to end of this revision; a measurement
                    of how much extra data we need to read/seek across to read
                    the delta chain for this revision
    :``extraratio``: extradist divided by chainsize; another representation of
                    how much unrelated data is needed to load this delta chain

    If the repository is configured to use the sparse read, additional keywords
    are available:

    :``readsize``:     total size of data read from the disk for a revision
                       (sum of the sizes of all the blocks)
    :``largestblock``: size of the largest block of data read from the disk
    :``readdensity``:  density of useful bytes in the data read from the disk
    :``srchunks``:  in how many data hunks the whole revision would be read

    The sparse read can be enabled with experimental.sparse-read = True
    """
    opts = pycompat.byteskwargs(opts)
    r = cmdutil.openrevlog(repo, b'debugdeltachain', file_, opts)
    index = r.index
    start = r.start
    length = r.length
    generaldelta = r._generaldelta
    withsparseread = getattr(r, '_withsparseread', False)

    # security to avoid crash on corrupted revlogs
    total_revs = len(index)

    def revinfo(rev):
        e = index[rev]
        compsize = e[revlog_constants.ENTRY_DATA_COMPRESSED_LENGTH]
        uncompsize = e[revlog_constants.ENTRY_DATA_UNCOMPRESSED_LENGTH]
        chainsize = 0

        base = e[revlog_constants.ENTRY_DELTA_BASE]
        p1 = e[revlog_constants.ENTRY_PARENT_1]
        p2 = e[revlog_constants.ENTRY_PARENT_2]

        # If the parents of a revision has an empty delta, we never try to delta
        # against that parent, but directly against the delta base of that
        # parent (recursively). It avoids adding a useless entry in the chain.
        #
        # However we need to detect that as a special case for delta-type, that
        # is not simply "other".
        p1_base = p1
        if p1 != nullrev and p1 < total_revs:
            e1 = index[p1]
            while e1[revlog_constants.ENTRY_DATA_COMPRESSED_LENGTH] == 0:
                new_base = e1[revlog_constants.ENTRY_DELTA_BASE]
                if (
                    new_base == p1_base
                    or new_base == nullrev
                    or new_base >= total_revs
                ):
                    break
                p1_base = new_base
                e1 = index[p1_base]
        p2_base = p2
        if p2 != nullrev and p2 < total_revs:
            e2 = index[p2]
            while e2[revlog_constants.ENTRY_DATA_COMPRESSED_LENGTH] == 0:
                new_base = e2[revlog_constants.ENTRY_DELTA_BASE]
                if (
                    new_base == p2_base
                    or new_base == nullrev
                    or new_base >= total_revs
                ):
                    break
                p2_base = new_base
                e2 = index[p2_base]

        if generaldelta:
            if base == p1:
                deltatype = b'p1'
            elif base == p2:
                deltatype = b'p2'
            elif base == rev:
                deltatype = b'base'
            elif base == p1_base:
                deltatype = b'skip1'
            elif base == p2_base:
                deltatype = b'skip2'
            elif r.issnapshot(rev):
                deltatype = b'snap'
            elif base == rev - 1:
                deltatype = b'prev'
            else:
                deltatype = b'other'
        else:
            if base == rev:
                deltatype = b'base'
            else:
                deltatype = b'prev'

        chain = r._deltachain(rev)[0]
        for iterrev in chain:
            e = index[iterrev]
            chainsize += e[revlog_constants.ENTRY_DATA_COMPRESSED_LENGTH]

        return p1, p2, compsize, uncompsize, deltatype, chain, chainsize

    fm = ui.formatter(b'debugdeltachain', opts)

    fm.plain(
        b'    rev      p1      p2  chain# chainlen     prev   delta       '
        b'size    rawsize  chainsize     ratio   lindist extradist '
        b'extraratio'
    )
    if withsparseread:
        fm.plain(b'   readsize largestblk rddensity srchunks')
    fm.plain(b'\n')

    chainbases = {}
    for rev in r:
        p1, p2, comp, uncomp, deltatype, chain, chainsize = revinfo(rev)
        chainbase = chain[0]
        chainid = chainbases.setdefault(chainbase, len(chainbases) + 1)
        basestart = start(chainbase)
        revstart = start(rev)
        lineardist = revstart + comp - basestart
        extradist = lineardist - chainsize
        try:
            prevrev = chain[-2]
        except IndexError:
            prevrev = -1

        if uncomp != 0:
            chainratio = float(chainsize) / float(uncomp)
        else:
            chainratio = chainsize

        if chainsize != 0:
            extraratio = float(extradist) / float(chainsize)
        else:
            extraratio = extradist

        fm.startitem()
        fm.write(
            b'rev p1 p2 chainid chainlen prevrev deltatype compsize '
            b'uncompsize chainsize chainratio lindist extradist '
            b'extraratio',
            b'%7d %7d %7d %7d %8d %8d %7s %10d %10d %10d %9.5f %9d %9d %10.5f',
            rev,
            p1,
            p2,
            chainid,
            len(chain),
            prevrev,
            deltatype,
            comp,
            uncomp,
            chainsize,
            chainratio,
            lineardist,
            extradist,
            extraratio,
            rev=rev,
            chainid=chainid,
            chainlen=len(chain),
            prevrev=prevrev,
            deltatype=deltatype,
            compsize=comp,
            uncompsize=uncomp,
            chainsize=chainsize,
            chainratio=chainratio,
            lindist=lineardist,
            extradist=extradist,
            extraratio=extraratio,
        )
        if withsparseread:
            readsize = 0
            largestblock = 0
            srchunks = 0

            for revschunk in deltautil.slicechunk(r, chain):
                srchunks += 1
                blkend = start(revschunk[-1]) + length(revschunk[-1])
                blksize = blkend - start(revschunk[0])

                readsize += blksize
                if largestblock < blksize:
                    largestblock = blksize

            if readsize:
                readdensity = float(chainsize) / float(readsize)
            else:
                readdensity = 1

            fm.write(
                b'readsize largestblock readdensity srchunks',
                b' %10d %10d %9.5f %8d',
                readsize,
                largestblock,
                readdensity,
                srchunks,
                readsize=readsize,
                largestblock=largestblock,
                readdensity=readdensity,
                srchunks=srchunks,
            )

        fm.plain(b'\n')

    fm.end()


@command(
    b'debug-delta-find',
    cmdutil.debugrevlogopts + cmdutil.formatteropts,
    _(b'-c|-m|FILE REV'),
    optionalrepo=True,
)
def debugdeltafind(ui, repo, arg_1, arg_2=None, **opts):
    """display the computation to get to a valid delta for storing REV

    This command will replay the process used to find the "best" delta to store
    a revision and display information about all the steps used to get to that
    result.

    The revision use the revision number of the target storage (not changelog
    revision number).

    note: the process is initiated from a full text of the revision to store.
    """
    opts = pycompat.byteskwargs(opts)
    if arg_2 is None:
        file_ = None
        rev = arg_1
    else:
        file_ = arg_1
        rev = arg_2

    rev = int(rev)

    revlog = cmdutil.openrevlog(repo, b'debugdeltachain', file_, opts)

    deltacomputer = deltautil.deltacomputer(
        revlog,
        write_debug=ui.write,
        debug_search=not ui.quiet,
    )

    node = revlog.node(rev)
    p1r, p2r = revlog.parentrevs(rev)
    p1 = revlog.node(p1r)
    p2 = revlog.node(p2r)
    btext = [revlog.revision(rev)]
    textlen = len(btext[0])
    cachedelta = None
    flags = revlog.flags(rev)

    revinfo = revlogutils.revisioninfo(
        node,
        p1,
        p2,
        btext,
        textlen,
        cachedelta,
        flags,
    )

    fh = revlog._datafp()
    deltacomputer.finddeltainfo(revinfo, fh, target_rev=rev)


@command(
    b'debugdirstate|debugstate',
    [
        (
            b'',
            b'nodates',
            None,
            _(b'do not display the saved mtime (DEPRECATED)'),
        ),
        (b'', b'dates', True, _(b'display the saved mtime')),
        (b'', b'datesort', None, _(b'sort by saved mtime')),
        (
            b'',
            b'docket',
            False,
            _(b'display the docket (metadata file) instead'),
        ),
        (
            b'',
            b'all',
            False,
            _(b'display dirstate-v2 tree nodes that would not exist in v1'),
        ),
    ],
    _(b'[OPTION]...'),
)
def debugstate(ui, repo, **opts):
    """show the contents of the current dirstate"""

    if opts.get("docket"):
        if not repo.dirstate._use_dirstate_v2:
            raise error.Abort(_(b'dirstate v1 does not have a docket'))

        docket = repo.dirstate._map.docket
        (
            start_offset,
            root_nodes,
            nodes_with_entry,
            nodes_with_copy,
            unused_bytes,
            _unused,
            ignore_pattern,
        ) = dirstateutils.v2.TREE_METADATA.unpack(docket.tree_metadata)

        ui.write(_(b"size of dirstate data: %d\n") % docket.data_size)
        ui.write(_(b"data file uuid: %s\n") % docket.uuid)
        ui.write(_(b"start offset of root nodes: %d\n") % start_offset)
        ui.write(_(b"number of root nodes: %d\n") % root_nodes)
        ui.write(_(b"nodes with entries: %d\n") % nodes_with_entry)
        ui.write(_(b"nodes with copies: %d\n") % nodes_with_copy)
        ui.write(_(b"number of unused bytes: %d\n") % unused_bytes)
        ui.write(
            _(b"ignore pattern hash: %s\n") % binascii.hexlify(ignore_pattern)
        )
        return

    nodates = not opts['dates']
    if opts.get('nodates') is not None:
        nodates = True
    datesort = opts.get('datesort')

    if datesort:

        def keyfunc(entry):
            filename, _state, _mode, _size, mtime = entry
            return (mtime, filename)

    else:
        keyfunc = None  # sort by filename
    entries = list(repo.dirstate._map.debug_iter(all=opts['all']))
    entries.sort(key=keyfunc)
    for entry in entries:
        filename, state, mode, size, mtime = entry
        if mtime == -1:
            timestr = b'unset               '
        elif nodates:
            timestr = b'set                 '
        else:
            timestr = time.strftime("%Y-%m-%d %H:%M:%S ", time.localtime(mtime))
            timestr = encoding.strtolocal(timestr)
        if mode & 0o20000:
            mode = b'lnk'
        else:
            mode = b'%3o' % (mode & 0o777 & ~util.umask)
        ui.write(b"%c %s %10d %s%s\n" % (state, mode, size, timestr, filename))
    for f in repo.dirstate.copies():
        ui.write(_(b"copy: %s -> %s\n") % (repo.dirstate.copied(f), f))


@command(
    b'debugdirstateignorepatternshash',
    [],
    _(b''),
)
def debugdirstateignorepatternshash(ui, repo, **opts):
    """show the hash of ignore patterns stored in dirstate if v2,
    or nothing for dirstate-v2
    """
    if repo.dirstate._use_dirstate_v2:
        docket = repo.dirstate._map.docket
        hash_len = 20  # 160 bits for SHA-1
        hash_bytes = docket.tree_metadata[-hash_len:]
        ui.write(binascii.hexlify(hash_bytes) + b'\n')


@command(
    b'debugdiscovery',
    [
        (b'', b'old', None, _(b'use old-style discovery')),
        (
            b'',
            b'nonheads',
            None,
            _(b'use old-style discovery with non-heads included'),
        ),
        (b'', b'rev', [], b'restrict discovery to this set of revs'),
        (b'', b'seed', b'12323', b'specify the random seed use for discovery'),
        (
            b'',
            b'local-as-revs',
            b"",
            b'treat local has having these revisions only',
        ),
        (
            b'',
            b'remote-as-revs',
            b"",
            b'use local as remote, with only these revisions',
        ),
    ]
    + cmdutil.remoteopts
    + cmdutil.formatteropts,
    _(b'[--rev REV] [OTHER]'),
)
def debugdiscovery(ui, repo, remoteurl=b"default", **opts):
    """runs the changeset discovery protocol in isolation

    The local peer can be "replaced" by a subset of the local repository by
    using the `--local-as-revs` flag. In the same way, the usual `remote` peer
    can be "replaced" by a subset of the local repository using the
    `--remote-as-revs` flag. This is useful to efficiently debug pathological
    discovery situations.

    The following developer oriented config are relevant for people playing with this command:

    * devel.discovery.exchange-heads=True

      If False, the discovery will not start with
      remote head fetching and local head querying.

    * devel.discovery.grow-sample=True

      If False, the sample size used in set discovery will not be increased
      through the process

    * devel.discovery.grow-sample.dynamic=True

      When discovery.grow-sample.dynamic is True, the default, the sample size is
      adapted to the shape of the undecided set (it is set to the max of:
      <target-size>, len(roots(undecided)), len(heads(undecided)

    * devel.discovery.grow-sample.rate=1.05

      the rate at which the sample grow

    * devel.discovery.randomize=True

      If andom sampling during discovery are deterministic. It is meant for
      integration tests.

    * devel.discovery.sample-size=200

      Control the initial size of the discovery sample

    * devel.discovery.sample-size.initial=100

      Control the initial size of the discovery for initial change
    """
    opts = pycompat.byteskwargs(opts)
    unfi = repo.unfiltered()

    # setup potential extra filtering
    local_revs = opts[b"local_as_revs"]
    remote_revs = opts[b"remote_as_revs"]

    # make sure tests are repeatable
    random.seed(int(opts[b'seed']))

    if not remote_revs:

        remoteurl, branches = urlutil.get_unique_pull_path(
            b'debugdiscovery', repo, ui, remoteurl
        )
        remote = hg.peer(repo, opts, remoteurl)
        ui.status(_(b'comparing with %s\n') % urlutil.hidepassword(remoteurl))
    else:
        branches = (None, [])
        remote_filtered_revs = logcmdutil.revrange(
            unfi, [b"not (::(%s))" % remote_revs]
        )
        remote_filtered_revs = frozenset(remote_filtered_revs)

        def remote_func(x):
            return remote_filtered_revs

        repoview.filtertable[b'debug-discovery-remote-filter'] = remote_func

        remote = repo.peer()
        remote._repo = remote._repo.filtered(b'debug-discovery-remote-filter')

    if local_revs:
        local_filtered_revs = logcmdutil.revrange(
            unfi, [b"not (::(%s))" % local_revs]
        )
        local_filtered_revs = frozenset(local_filtered_revs)

        def local_func(x):
            return local_filtered_revs

        repoview.filtertable[b'debug-discovery-local-filter'] = local_func
        repo = repo.filtered(b'debug-discovery-local-filter')

    data = {}
    if opts.get(b'old'):

        def doit(pushedrevs, remoteheads, remote=remote):
            if not util.safehasattr(remote, b'branches'):
                # enable in-client legacy support
                remote = localrepo.locallegacypeer(remote.local())
                if remote_revs:
                    r = remote._repo.filtered(b'debug-discovery-remote-filter')
                    remote._repo = r
            common, _in, hds = treediscovery.findcommonincoming(
                repo, remote, force=True, audit=data
            )
            common = set(common)
            if not opts.get(b'nonheads'):
                ui.writenoi18n(
                    b"unpruned common: %s\n"
                    % b" ".join(sorted(short(n) for n in common))
                )

                clnode = repo.changelog.node
                common = repo.revs(b'heads(::%ln)', common)
                common = {clnode(r) for r in common}
            return common, hds

    else:

        def doit(pushedrevs, remoteheads, remote=remote):
            nodes = None
            if pushedrevs:
                revs = logcmdutil.revrange(repo, pushedrevs)
                nodes = [repo[r].node() for r in revs]
            common, any, hds = setdiscovery.findcommonheads(
                ui,
                repo,
                remote,
                ancestorsof=nodes,
                audit=data,
                abortwhenunrelated=False,
            )
            return common, hds

    remoterevs, _checkout = hg.addbranchrevs(repo, remote, branches, revs=None)
    localrevs = opts[b'rev']

