    def isexact(self):
        return self._matcher.isexact()

    def prefix(self):
        return self._matcher.prefix()

    @encoding.strmethod
    def __repr__(self):
        return b'<prefixdirmatcher path=%r, matcher=%r>' % (
            pycompat.bytestr(self._path),
            self._matcher,
        )


class unionmatcher(basematcher):
    """A matcher that is the union of several matchers.

    The non-matching-attributes (bad, traversedir) are taken from the first
    matcher.
    """

    def __init__(self, matchers):
        m1 = matchers[0]
        super(unionmatcher, self).__init__()
        self.traversedir = m1.traversedir
        self._matchers = matchers

    def matchfn(self, f):
        for match in self._matchers:
            if match(f):
                return True
        return False

    def visitdir(self, dir):
        r = False
        for m in self._matchers:
            v = m.visitdir(dir)
            if v == b'all':
                return v
            r |= v
        return r

    def visitchildrenset(self, dir):
        r = set()
        this = False
        for m in self._matchers:
            v = m.visitchildrenset(dir)
            if not v:
                continue
            if v == b'all':
                return v
            if this or v == b'this':
                this = True
                # don't break, we might have an 'all' in here.
                continue
            assert isinstance(v, set)
            r = r.union(v)
        if this:
            return b'this'
        return r

    @encoding.strmethod
    def __repr__(self):
        return b'<unionmatcher matchers=%r>' % self._matchers


def patkind(pattern, default=None):
    r"""If pattern is 'kind:pat' with a known kind, return kind.

    >>> patkind(br're:.*\.c$')
    're'
    >>> patkind(b'glob:*.c')
    'glob'
    >>> patkind(b'relpath:test.py')
    'relpath'
    >>> patkind(b'main.py')
    >>> patkind(b'main.py', default=b're')
    're'
    """
    return _patsplit(pattern, default)[0]


def _patsplit(pattern, default):
    """Split a string into the optional pattern kind prefix and the actual
    pattern."""
    if b':' in pattern:
        kind, pat = pattern.split(b':', 1)
        if kind in allpatternkinds:
            return kind, pat
    return default, pattern


def _globre(pat):
    r"""Convert an extended glob string to a regexp string.

    >>> from . import pycompat
    >>> def bprint(s):
    ...     print(pycompat.sysstr(s))
    >>> bprint(_globre(br'?'))
    .
    >>> bprint(_globre(br'*'))
    [^/]*
    >>> bprint(_globre(br'**'))
    .*
    >>> bprint(_globre(br'**/a'))
    (?:.*/)?a
    >>> bprint(_globre(br'a/**/b'))
    a/(?:.*/)?b
    >>> bprint(_globre(br'[a*?!^][^b][!c]'))
    [a*?!^][\^b][^c]
    >>> bprint(_globre(br'{a,b}'))
    (?:a|b)
    >>> bprint(_globre(br'.\*\?'))
    \.\*\?
    """
    i, n = 0, len(pat)
    res = b''
    group = 0
    escape = util.stringutil.regexbytesescapemap.get

    def peek():
        return i < n and pat[i : i + 1]

    while i < n:
        c = pat[i : i + 1]
        i += 1
        if c not in b'*?[{},\\':
            res += escape(c, c)
        elif c == b'*':
            if peek() == b'*':
                i += 1
                if peek() == b'/':
                    i += 1
                    res += b'(?:.*/)?'
                else:
                    res += b'.*'
            else:
                res += b'[^/]*'
        elif c == b'?':
            res += b'.'
        elif c == b'[':
            j = i
            if j < n and pat[j : j + 1] in b'!]':
                j += 1
            while j < n and pat[j : j + 1] != b']':
                j += 1
            if j >= n:
                res += b'\\['
            else:
                stuff = pat[i:j].replace(b'\\', b'\\\\')
                i = j + 1
                if stuff[0:1] == b'!':
                    stuff = b'^' + stuff[1:]
                elif stuff[0:1] == b'^':
                    stuff = b'\\' + stuff
                res = b'%s[%s]' % (res, stuff)
        elif c == b'{':
            group += 1
            res += b'(?:'
        elif c == b'}' and group:
            res += b')'
            group -= 1
        elif c == b',' and group:
            res += b'|'
        elif c == b'\\':
            p = peek()
            if p:
                i += 1
                res += escape(p, p)
            else:
                res += escape(c, c)
        else:
            res += escape(c, c)
    return res


FLAG_RE = util.re.compile(br'^\(\?([aiLmsux]+)\)(.*)')


def _regex(kind, pat, globsuffix):
    """Convert a (normalized) pattern of any kind into a
    regular expression.
    globsuffix is appended to the regexp of globs."""
    if not pat and kind in (b'glob', b'relpath'):
        return b''
    if kind == b're':
        return pat
    if kind in (b'path', b'relpath'):
        if pat == b'.':
            return b''
        return util.stringutil.reescape(pat) + b'(?:/|$)'
    if kind == b'rootfilesin':
        if pat == b'.':
            escaped = b''
        else:
            # Pattern is a directory name.
            escaped = util.stringutil.reescape(pat) + b'/'
        # Anything after the pattern must be a non-directory.
        return escaped + b'[^/]+$'
    if kind == b'relglob':
        globre = _globre(pat)
        if globre.startswith(b'[^/]*'):
            # When pat has the form *XYZ (common), make the returned regex more
            # legible by returning the regex for **XYZ instead of **/*XYZ.
            return b'.*' + globre[len(b'[^/]*') :] + globsuffix
        return b'(?:|.*/)' + globre + globsuffix
    if kind == b'relre':
        flag = None
        m = FLAG_RE.match(pat)
        if m:
            flag, pat = m.groups()
        if not pat.startswith(b'^'):
            pat = b'.*' + pat
        if flag is not None:
            pat = br'(?%s:%s)' % (flag, pat)
        return pat
    if kind in (b'glob', b'rootglob'):
        return _globre(pat) + globsuffix
    raise error.ProgrammingError(b'not a regex pattern: %s:%s' % (kind, pat))


def _buildmatch(kindpats, globsuffix, root):
    """Return regexp string and a matcher function for kindpats.
    globsuffix is appended to the regexp of globs."""
    matchfuncs = []

    subincludes, kindpats = _expandsubinclude(kindpats, root)
    if subincludes:
        submatchers = {}

        def matchsubinclude(f):
            for prefix, matcherargs in subincludes:
                if f.startswith(prefix):
                    mf = submatchers.get(prefix)
                    if mf is None:
                        mf = match(*matcherargs)
                        submatchers[prefix] = mf

                    if mf(f[len(prefix) :]):
                        return True
            return False

        matchfuncs.append(matchsubinclude)

    regex = b''
    if kindpats:
        if all(k == b'rootfilesin' for k, p, s in kindpats):
            dirs = {p for k, p, s in kindpats}

            def mf(f):
                i = f.rfind(b'/')
                if i >= 0:
                    dir = f[:i]
                else:
                    dir = b'.'
                return dir in dirs

            regex = b'rootfilesin: %s' % stringutil.pprint(list(sorted(dirs)))
            matchfuncs.append(mf)
        else:
            regex, mf = _buildregexmatch(kindpats, globsuffix)
            matchfuncs.append(mf)

    if len(matchfuncs) == 1:
        return regex, matchfuncs[0]
    else:
        return regex, lambda f: any(mf(f) for mf in matchfuncs)


MAX_RE_SIZE = 20000


def _joinregexes(regexps):
    """gather multiple regular expressions into a single one"""
    return b'|'.join(regexps)


def _buildregexmatch(kindpats, globsuffix):
    """Build a match function from a list of kinds and kindpats,
    return regexp string and a matcher function.

    Test too large input
    >>> _buildregexmatch([
    ...     (b'relglob', b'?' * MAX_RE_SIZE, b'')
    ... ], b'$')
    Traceback (most recent call last):
    ...
    Abort: matcher pattern is too long (20009 bytes)
    """
    try:
        allgroups = []
        regexps = [_regex(k, p, globsuffix) for (k, p, s) in kindpats]
        fullregexp = _joinregexes(regexps)

        startidx = 0
        groupsize = 0
        for idx, r in enumerate(regexps):
            piecesize = len(r)
            if piecesize > MAX_RE_SIZE:
                msg = _(b"matcher pattern is too long (%d bytes)") % piecesize
                raise error.Abort(msg)
            elif (groupsize + piecesize) > MAX_RE_SIZE:
                group = regexps[startidx:idx]
                allgroups.append(_joinregexes(group))
                startidx = idx
                groupsize = 0
            groupsize += piecesize + 1

        if startidx == 0:
            matcher = _rematcher(fullregexp)
            func = lambda s: bool(matcher(s))
        else:
            group = regexps[startidx:]
            allgroups.append(_joinregexes(group))
            allmatchers = [_rematcher(g) for g in allgroups]
            func = lambda s: any(m(s) for m in allmatchers)
        return fullregexp, func
    except re.error:
        for k, p, s in kindpats:
            try:
                _rematcher(_regex(k, p, globsuffix))
            except re.error:
                if s:
                    raise error.Abort(
                        _(b"%s: invalid pattern (%s): %s") % (s, k, p)
                    )
                else:
                    raise error.Abort(_(b"invalid pattern (%s): %s") % (k, p))
        raise error.Abort(_(b"invalid pattern"))


def _patternrootsanddirs(kindpats):
    """Returns roots and directories corresponding to each pattern.

    This calculates the roots and directories exactly matching the patterns and
    returns a tuple of (roots, dirs) for each. It does not return other
    directories which may also need to be considered, like the parent
    directories.
    """
    r = []
    d = []
    for kind, pat, source in kindpats:
        if kind in (b'glob', b'rootglob'):  # find the non-glob prefix
            root = []
            for p in pat.split(b'/'):
                if b'[' in p or b'{' in p or b'*' in p or b'?' in p:
                    break
                root.append(p)
            r.append(b'/'.join(root))
        elif kind in (b'relpath', b'path'):
            if pat == b'.':
                pat = b''
            r.append(pat)
        elif kind in (b'rootfilesin',):
            if pat == b'.':
                pat = b''
            d.append(pat)
        else:  # relglob, re, relre
            r.append(b'')
    return r, d


def _roots(kindpats):
    '''Returns root directories to match recursively from the given patterns.'''
    roots, dirs = _patternrootsanddirs(kindpats)
    return roots


def _rootsdirsandparents(kindpats):
    """Returns roots and exact directories from patterns.

    `roots` are directories to match recursively, `dirs` should
    be matched non-recursively, and `parents` are the implicitly required
    directories to walk to items in either roots or dirs.

    Returns a tuple of (roots, dirs, parents).

    >>> r = _rootsdirsandparents(
    ...     [(b'glob', b'g/h/*', b''), (b'glob', b'g/h', b''),
    ...      (b'glob', b'g*', b'')])
    >>> print(r[0:2], sorted(r[2])) # the set has an unstable output
    (['g/h', 'g/h', ''], []) ['', 'g']
    >>> r = _rootsdirsandparents(
    ...     [(b'rootfilesin', b'g/h', b''), (b'rootfilesin', b'', b'')])
    >>> print(r[0:2], sorted(r[2])) # the set has an unstable output
    ([], ['g/h', '']) ['', 'g']
    >>> r = _rootsdirsandparents(
    ...     [(b'relpath', b'r', b''), (b'path', b'p/p', b''),
    ...      (b'path', b'', b'')])
    >>> print(r[0:2], sorted(r[2])) # the set has an unstable output
    (['r', 'p/p', ''], []) ['', 'p']
    >>> r = _rootsdirsandparents(
    ...     [(b'relglob', b'rg*', b''), (b're', b're/', b''),
    ...      (b'relre', b'rr', b'')])
    >>> print(r[0:2], sorted(r[2])) # the set has an unstable output
    (['', '', ''], []) ['']
    """
    r, d = _patternrootsanddirs(kindpats)

    p = set()
    # Add the parents as non-recursive/exact directories, since they must be
    # scanned to get to either the roots or the other exact directories.
    p.update(pathutil.dirs(d))
    p.update(pathutil.dirs(r))

    # FIXME: all uses of this function convert these to sets, do so before
    # returning.
    # FIXME: all uses of this function do not need anything in 'roots' and
    # 'dirs' to also be in 'parents', consider removing them before returning.
    return r, d, p


def _explicitfiles(kindpats):
    """Returns the potential explicit filenames from the patterns.

    >>> _explicitfiles([(b'path', b'foo/bar', b'')])
    ['foo/bar']
    >>> _explicitfiles([(b'rootfilesin', b'foo/bar', b'')])
    []
    """
    # Keep only the pattern kinds where one can specify filenames (vs only
    # directory names).
    filable = [kp for kp in kindpats if kp[0] not in (b'rootfilesin',)]
    return _roots(filable)


def _prefix(kindpats):
    '''Whether all the patterns match a prefix (i.e. recursively)'''
    for kind, pat, source in kindpats:
        if kind not in (b'path', b'relpath'):
            return False
    return True


_commentre = None


def readpatternfile(filepath, warn, sourceinfo=False):
    """parse a pattern file, returning a list of
    patterns. These patterns should be given to compile()
    to be validated and converted into a match function.

    trailing white space is dropped.
    the escape character is backslash.
    comments start with #.
    empty lines are skipped.

    lines can be of the following formats:

    syntax: regexp # defaults following lines to non-rooted regexps
    syntax: glob   # defaults following lines to non-rooted globs
    re:pattern     # non-rooted regular expression
    glob:pattern   # non-rooted glob
    rootglob:pat   # rooted glob (same root as ^ in regexps)
    pattern        # pattern of the current default type

    if sourceinfo is set, returns a list of tuples:
    (pattern, lineno, originalline).
    This is useful to debug ignore patterns.
    """

    syntaxes = {
        b're': b'relre:',
        b'regexp': b'relre:',
        b'glob': b'relglob:',
        b'rootglob': b'rootglob:',
        b'include': b'include',
        b'subinclude': b'subinclude',
    }
    syntax = b'relre:'
    patterns = []

    fp = open(filepath, b'rb')
    for lineno, line in enumerate(fp, start=1):
        if b"#" in line:
            global _commentre
            if not _commentre:
                _commentre = util.re.compile(br'((?:^|[^\\])(?:\\\\)*)#.*')
            # remove comments prefixed by an even number of escapes
            m = _commentre.search(line)
            if m:
                line = line[: m.end(1)]
            # fixup properly escaped comments that survived the above
            line = line.replace(b"\\#", b"#")
        line = line.rstrip()
        if not line:
            continue

        if line.startswith(b'syntax:'):
            s = line[7:].strip()
            try:
                syntax = syntaxes[s]
            except KeyError:
                if warn:
                    warn(
                        _(b"%s: ignoring invalid syntax '%s'\n") % (filepath, s)
                    )
            continue

        linesyntax = syntax
        for s, rels in syntaxes.items():
            if line.startswith(rels):
                linesyntax = rels
                line = line[len(rels) :]
                break
            elif line.startswith(s + b':'):
                linesyntax = rels
                line = line[len(s) + 1 :]
                break
        if sourceinfo:
            patterns.append((linesyntax + line, lineno, line))
        else:
            patterns.append(linesyntax + line)
    fp.close()
    return patterns
                                                                                                                                                                                                                                                                                                                                                                             usr/lib/python3/dist-packages/mercurial/mdiff.py                                                    0000644 0000000 0000000 00000042450 14355257011 020400  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        # mdiff.py - diff and patch routines for mercurial
#
# Copyright 2005, 2006 Olivia Mackall <olivia@selenic.com>
#
# This software may be used and distributed according to the terms of the
# GNU General Public License version 2 or any later version.


import re
import struct
import zlib

from .i18n import _
from .pycompat import (
    getattr,
    setattr,
)
from . import (
    diffhelper,
    encoding,
    error,
    policy,
    pycompat,
    util,
)
from .utils import dateutil

bdiff = policy.importmod('bdiff')
mpatch = policy.importmod('mpatch')

blocks = bdiff.blocks
fixws = bdiff.fixws
patches = mpatch.patches
patchedsize = mpatch.patchedsize
textdiff = bdiff.bdiff
splitnewlines = bdiff.splitnewlines


# TODO: this looks like it could be an attrs, which might help pytype
class diffopts:
    """context is the number of context lines
    text treats all files as text
    showfunc enables diff -p output
    git enables the git extended patch format
    nodates removes dates from diff headers
    nobinary ignores binary files
    noprefix disables the 'a/' and 'b/' prefixes (ignored in plain mode)
    ignorews ignores all whitespace changes in the diff
    ignorewsamount ignores changes in the amount of whitespace
    ignoreblanklines ignores changes whose lines are all blank
    upgrade generates git diffs to avoid data loss
    """

    _HAS_DYNAMIC_ATTRIBUTES = True

    defaults = {
        b'context': 3,
        b'text': False,
        b'showfunc': False,
        b'git': False,
        b'nodates': False,
        b'nobinary': False,
        b'noprefix': False,
        b'index': 0,
        b'ignorews': False,
        b'ignorewsamount': False,
        b'ignorewseol': False,
        b'ignoreblanklines': False,
        b'upgrade': False,
        b'showsimilarity': False,
        b'worddiff': False,
        b'xdiff': False,
    }

    def __init__(self, **opts):
        opts = pycompat.byteskwargs(opts)
        for k in self.defaults.keys():
            v = opts.get(k)
            if v is None:
                v = self.defaults[k]
            setattr(self, k, v)

        try:
            self.context = int(self.context)
        except ValueError:
            raise error.InputError(
                _(b'diff context lines count must be an integer, not %r')
                % pycompat.bytestr(self.context)
            )

    def copy(self, **kwargs):
        opts = {k: getattr(self, k) for k in self.defaults}
        opts = pycompat.strkwargs(opts)
        opts.update(kwargs)
        return diffopts(**opts)


defaultopts = diffopts()


def wsclean(opts, text, blank=True):
    if opts.ignorews:
        text = bdiff.fixws(text, 1)
    elif opts.ignorewsamount:
        text = bdiff.fixws(text, 0)
    if blank and opts.ignoreblanklines:
        text = re.sub(b'\n+', b'\n', text).strip(b'\n')
    if opts.ignorewseol:
        text = re.sub(br'[ \t\r\f]+\n', br'\n', text)
    return text


def splitblock(base1, lines1, base2, lines2, opts):
    # The input lines matches except for interwoven blank lines. We
    # transform it into a sequence of matching blocks and blank blocks.
    lines1 = [(wsclean(opts, l) and 1 or 0) for l in lines1]
    lines2 = [(wsclean(opts, l) and 1 or 0) for l in lines2]
    s1, e1 = 0, len(lines1)
    s2, e2 = 0, len(lines2)
    while s1 < e1 or s2 < e2:
        i1, i2, btype = s1, s2, b'='
        if i1 >= e1 or lines1[i1] == 0 or i2 >= e2 or lines2[i2] == 0:
            # Consume the block of blank lines
            btype = b'~'
            while i1 < e1 and lines1[i1] == 0:
                i1 += 1
            while i2 < e2 and lines2[i2] == 0:
                i2 += 1
        else:
            # Consume the matching lines
            while i1 < e1 and lines1[i1] == 1 and lines2[i2] == 1:
                i1 += 1
                i2 += 1
        yield [base1 + s1, base1 + i1, base2 + s2, base2 + i2], btype
        s1 = i1
        s2 = i2


def hunkinrange(hunk, linerange):
    """Return True if `hunk` defined as (start, length) is in `linerange`
    defined as (lowerbound, upperbound).

    >>> hunkinrange((5, 10), (2, 7))
    True
    >>> hunkinrange((5, 10), (6, 12))
    True
    >>> hunkinrange((5, 10), (13, 17))
    True
    >>> hunkinrange((5, 10), (3, 17))
    True
    >>> hunkinrange((5, 10), (1, 3))
    False
    >>> hunkinrange((5, 10), (18, 20))
    False
    >>> hunkinrange((5, 10), (1, 5))
    False
    >>> hunkinrange((5, 10), (15, 27))
    False
    """
    start, length = hunk
    lowerbound, upperbound = linerange
    return lowerbound < start + length and start < upperbound


def blocksinrange(blocks, rangeb):
    """filter `blocks` like (a1, a2, b1, b2) from items outside line range
    `rangeb` from ``(b1, b2)`` point of view.

    Return `filteredblocks, rangea` where:

    * `filteredblocks` is list of ``block = (a1, a2, b1, b2), stype`` items of
      `blocks` that are inside `rangeb` from ``(b1, b2)`` point of view; a
      block ``(b1, b2)`` being inside `rangeb` if
      ``rangeb[0] < b2 and b1 < rangeb[1]``;
    * `rangea` is the line range w.r.t. to ``(a1, a2)`` parts of `blocks`.
    """
    lbb, ubb = rangeb
    lba, uba = None, None
    filteredblocks = []
    for block in blocks:
        (a1, a2, b1, b2), stype = block
        if lbb >= b1 and ubb <= b2 and stype == b'=':
            # rangeb is within a single "=" hunk, restrict back linerange1
            # by offsetting rangeb
            lba = lbb - b1 + a1
            uba = ubb - b1 + a1
        else:
            if b1 <= lbb < b2:
                if stype == b'=':
                    lba = a2 - (b2 - lbb)
                else:
                    lba = a1
            if b1 < ubb <= b2:
                if stype == b'=':
                    uba = a1 + (ubb - b1)
                else:
                    uba = a2
        if hunkinrange((b1, (b2 - b1)), rangeb):
            filteredblocks.append(block)
    if lba is None or uba is None or uba < lba:
        raise error.InputError(_(b'line range exceeds file size'))
    return filteredblocks, (lba, uba)


def chooseblocksfunc(opts=None):
    if (
        opts is None
        or not opts.xdiff
        or not util.safehasattr(bdiff, b'xdiffblocks')
    ):
        return bdiff.blocks
    else:
        return bdiff.xdiffblocks


def allblocks(text1, text2, opts=None, lines1=None, lines2=None):
    """Return (block, type) tuples, where block is an mdiff.blocks
    line entry. type is '=' for blocks matching exactly one another
    (bdiff blocks), '!' for non-matching blocks and '~' for blocks
    matching only after having filtered blank lines.
    line1 and line2 are text1 and text2 split with splitnewlines() if
    they are already available.
    """
    if opts is None:
        opts = defaultopts
    if opts.ignorews or opts.ignorewsamount or opts.ignorewseol:
        text1 = wsclean(opts, text1, False)
        text2 = wsclean(opts, text2, False)
    diff = chooseblocksfunc(opts)(text1, text2)
    for i, s1 in enumerate(diff):
        # The first match is special.
        # we've either found a match starting at line 0 or a match later
        # in the file.  If it starts later, old and new below will both be
        # empty and we'll continue to the next match.
        if i > 0:
            s = diff[i - 1]
        else:
            s = [0, 0, 0, 0]
        s = [s[1], s1[0], s[3], s1[2]]

        # bdiff sometimes gives huge matches past eof, this check eats them,
        # and deals with the special first match case described above
        if s[0] != s[1] or s[2] != s[3]:
            type = b'!'
            if opts.ignoreblanklines:
                if lines1 is None:
                    lines1 = splitnewlines(text1)
                if lines2 is None:
                    lines2 = splitnewlines(text2)
                old = wsclean(opts, b"".join(lines1[s[0] : s[1]]))
                new = wsclean(opts, b"".join(lines2[s[2] : s[3]]))
                if old == new:
                    type = b'~'
            yield s, type
        yield s1, b'='


def unidiff(a, ad, b, bd, fn1, fn2, binary, opts=defaultopts):
    """Return a unified diff as a (headers, hunks) tuple.

    If the diff is not null, `headers` is a list with unified diff header
    lines "--- <original>" and "+++ <new>" and `hunks` is a generator yielding
    (hunkrange, hunklines) coming from _unidiff().
    Otherwise, `headers` and `hunks` are empty.

    Set binary=True if either a or b should be taken as a binary file.
    """

    def datetag(date, fn=None):
        if not opts.git and not opts.nodates:
            return b'\t%s' % date
        if fn and b' ' in fn:
            return b'\t'
        return b''

    sentinel = [], ()
    if not a and not b:
        return sentinel

    if opts.noprefix:
        aprefix = bprefix = b''
    else:
        aprefix = b'a/'
        bprefix = b'b/'

    epoch = dateutil.datestr((0, 0))

    fn1 = util.pconvert(fn1)
    fn2 = util.pconvert(fn2)

    if binary:
        if a and b and len(a) == len(b) and a == b:
            return sentinel
        headerlines = []
        hunks = ((None, [b'Binary file %s has changed\n' % fn1]),)
    elif not a:
        without_newline = not b.endswith(b'\n')
        b = splitnewlines(b)
        if a is None:
            l1 = b'--- /dev/null%s' % datetag(epoch)
        else:
            l1 = b"--- %s%s%s" % (aprefix, fn1, datetag(ad, fn1))
        l2 = b"+++ %s%s" % (bprefix + fn2, datetag(bd, fn2))
        headerlines = [l1, l2]
        size = len(b)
        hunkrange = (0, 0, 1, size)
        hunklines = [b"@@ -0,0 +1,%d @@\n" % size] + [b"+" + e for e in b]
        if without_newline:
            hunklines[-1] += b'\n'
            hunklines.append(diffhelper.MISSING_NEWLINE_MARKER)
        hunks = ((hunkrange, hunklines),)
    elif not b:
        without_newline = not a.endswith(b'\n')
        a = splitnewlines(a)
        l1 = b"--- %s%s%s" % (aprefix, fn1, datetag(ad, fn1))
        if b is None:
            l2 = b'+++ /dev/null%s' % datetag(epoch)
        else:
            l2 = b"+++ %s%s%s" % (bprefix, fn2, datetag(bd, fn2))
        headerlines = [l1, l2]
        size = len(a)
        hunkrange = (1, size, 0, 0)
        hunklines = [b"@@ -1,%d +0,0 @@\n" % size] + [b"-" + e for e in a]
        if without_newline:
            hunklines[-1] += b'\n'
            hunklines.append(diffhelper.MISSING_NEWLINE_MARKER)
        hunks = ((hunkrange, hunklines),)
    else:
        hunks = _unidiff(a, b, opts=opts)
        if not next(hunks):
            return sentinel

        headerlines = [
            b"--- %s%s%s" % (aprefix, fn1, datetag(ad, fn1)),
            b"+++ %s%s%s" % (bprefix, fn2, datetag(bd, fn2)),
        ]

    return headerlines, hunks


def _unidiff(t1, t2, opts=defaultopts):
    """Yield hunks of a headerless unified diff from t1 and t2 texts.

    Each hunk consists of a (hunkrange, hunklines) tuple where `hunkrange` is a
    tuple (s1, l1, s2, l2) representing the range information of the hunk to
    form the '@@ -s1,l1 +s2,l2 @@' header and `hunklines` is a list of lines
    of the hunk combining said header followed by line additions and
    deletions.

    The hunks are prefixed with a bool.
    """
    l1 = splitnewlines(t1)
    l2 = splitnewlines(t2)

    def contextend(l, len):
        ret = l + opts.context
        if ret > len:
            ret = len
        return ret

    def contextstart(l):
        ret = l - opts.context
        if ret < 0:
            return 0
        return ret

    lastfunc = [0, b'']

    def yieldhunk(hunk):
        (astart, a2, bstart, b2, delta) = hunk
        aend = contextend(a2, len(l1))
        alen = aend - astart
        blen = b2 - bstart + aend - a2

        func = b""
        if opts.showfunc:
            lastpos, func = lastfunc
            # walk backwards from the start of the context up to the start of
            # the previous hunk context until we find a line starting with an
            # alphanumeric char.
            for i in range(astart - 1, lastpos - 1, -1):
                if l1[i][0:1].isalnum():
                    func = b' ' + l1[i].rstrip()
                    # split long function name if ASCII. otherwise we have no
                    # idea where the multi-byte boundary is, so just leave it.
                    if encoding.isasciistr(func):
                        func = func[:41]
                    lastfunc[1] = func
                    break
            # by recording this hunk's starting point as the next place to
            # start looking for function lines, we avoid reading any line in
            # the file more than once.
            lastfunc[0] = astart

        # zero-length hunk ranges report their start line as one less
        if alen:
            astart += 1
        if blen:
            bstart += 1

        hunkrange = astart, alen, bstart, blen
        hunklines = (
            [b"@@ -%d,%d +%d,%d @@%s\n" % (hunkrange + (func,))]
            + delta
            + [b' ' + l1[x] for x in range(a2, aend)]
        )
        # If either file ends without a newline and the last line of
        # that file is part of a hunk, a marker is printed. If the
        # last line of both files is identical and neither ends in
        # a newline, print only one marker. That's the only case in
        # which the hunk can end in a shared line without a newline.
        skip = False
        if not t1.endswith(b'\n') and astart + alen == len(l1) + 1:
            for i in range(len(hunklines) - 1, -1, -1):
                if hunklines[i].startswith((b'-', b' ')):
                    if hunklines[i].startswith(b' '):
                        skip = True
                    hunklines[i] += b'\n'
                    hunklines.insert(i + 1, diffhelper.MISSING_NEWLINE_MARKER)
                    break
        if not skip and not t2.endswith(b'\n') and bstart + blen == len(l2) + 1:
            for i in range(len(hunklines) - 1, -1, -1):
                if hunklines[i].startswith(b'+'):
                    hunklines[i] += b'\n'
                    hunklines.insert(i + 1, diffhelper.MISSING_NEWLINE_MARKER)
                    break
        yield hunkrange, hunklines

    # bdiff.blocks gives us the matching sequences in the files.  The loop
    # below finds the spaces between those matching sequences and translates
    # them into diff output.
    #
    hunk = None
    ignoredlines = 0
    has_hunks = False
    for s, stype in allblocks(t1, t2, opts, l1, l2):
        a1, a2, b1, b2 = s
        if stype != b'!':
            if stype == b'~':
                # The diff context lines are based on t1 content. When
                # blank lines are ignored, the new lines offsets must
                # be adjusted as if equivalent blocks ('~') had the
                # same sizes on both sides.
                ignoredlines += (b2 - b1) - (a2 - a1)
            continue
        delta = []
        old = l1[a1:a2]
        new = l2[b1:b2]

        b1 -= ignoredlines
        b2 -= ignoredlines
        astart = contextstart(a1)
        bstart = contextstart(b1)
        prev = None
        if hunk:
            # join with the previous hunk if it falls inside the context
            if astart < hunk[1] + opts.context + 1:
                prev = hunk
                astart = hunk[1]
                bstart = hunk[3]
            else:
                if not has_hunks:
                    has_hunks = True
                    yield True
                for x in yieldhunk(hunk):
                    yield x
        if prev:
            # we've joined the previous hunk, record the new ending points.
            hunk[1] = a2
            hunk[3] = b2
            delta = hunk[4]
        else:
            # create a new hunk
            hunk = [astart, a2, bstart, b2, delta]

        delta[len(delta) :] = [b' ' + x for x in l1[astart:a1]]
        delta[len(delta) :] = [b'-' + x for x in old]
        delta[len(delta) :] = [b'+' + x for x in new]

    if hunk:
        if not has_hunks:
            has_hunks = True
            yield True
        for x in yieldhunk(hunk):
            yield x
